{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+","social":[{"link":"https://github.com/mindsdb/mindsdb","type":"github"},{"link":"https://twitter.com/mindsdb","type":"twitter"},{"link":"https://www.mindsdb.com","type":"link"}]},"docs":[{"location":"","text":"","title":"Official MindsDB Docs"},{"location":"Config/","text":"MindsDB has config variables that can be set by as environment variables or on a script. Here are some of the variables of general interest: MINDSDB_STORAGE_PATH Where MindsDB stores its data, by default it is the path where pip installs packages + mindsdb/mindsdb_storage DEFAULT_MARGIN_OF_ERROR The reverse of how much of the data you feed in mindsdb will sample in order to generate insights about the data. For example, if this is set to 0.4, mindsdb will sample 0.6 of the data. DEFAULT_LOG_LEVEL What logs mindsdb will display (By default this is set to: CONST.INFO_LOG_LEVEL ) How to set config variables? You can either set it as environment variables or you can do it in your script. import os os . environ [ '<varname>' ] = < value > For example, if you want to specify a different storage directory: import os os . environ [ 'MINDSDB_STORAGE_PATH' ] = '/home/my_wonderful_username/place_where_i_store_big_files/' # now we import mindsdb from mindsdb import MindsDB Alternatively, we can se the variable after importing mindsdb: mindsdb . CONFIG . MINDSDB_STORAGE_PATH = '/home/my_wonderful_username/place_where_i_store_big_files/' You can see all the config variables available here","title":"Configuration Settings"},{"location":"Config/#how-to-set-config-variables","text":"You can either set it as environment variables or you can do it in your script. import os os . environ [ '<varname>' ] = < value > For example, if you want to specify a different storage directory: import os os . environ [ 'MINDSDB_STORAGE_PATH' ] = '/home/my_wonderful_username/place_where_i_store_big_files/' # now we import mindsdb from mindsdb import MindsDB Alternatively, we can se the variable after importing mindsdb: mindsdb . CONFIG . MINDSDB_STORAGE_PATH = '/home/my_wonderful_username/place_where_i_store_big_files/' You can see all the config variables available here","title":"How to set config variables?"},{"location":"FAQ/","text":"Why MindsDB? We are building MindsDB, because we want to Democratize Machine Learning . If you want to learn more about why this is important and how we aim to do this, you can check out our presentation here . Who are we building MindsDB for? We are building MindsDB for all of those that can get their hands in data. and can type a few lines of code. What are MindsDB's current Goals? MindsDB has 3 simple goals. Provide a way to learn and predict from data with a line of code. Explainable, by answering the following questions: When learning: What is interesting in my data and why? When can I trust this model and why? When I should not trust this model and why? How can I improve this model? When predicting: Why this prediction? Why not something else? Remain state of the art. Since MindsDB users are delegating the ML machinery, MindsDB should try to always generate state of the art models for the users. What is the roadmap? MindsDB roadmap is aimed to be aligned with our goals: Success versions 1.0 MindsDB GUI where you can visualize explainability goals Support for images and complex text MindsDB REST API's Success versions 2.0 Run Machine Learning Models as Tables Important versions 3.0 Auto ETL and Data Preparation What type of data can MindsDB learn and predict from? We support tabular data, this is CSV, Excel, JSON, text files, pandas data frame, URLs, s3 files or MySQL, Mongo, ClickHouse, PostgreSQL data stores. For more information please see the data sources documentation . How does it work? In very simple terms, MindsDB follows the following steps: to learn : break data source intro train, test, validate transform data source into tensors build and train encoders (if necessary) produce a neural network based model that can take in the input tensor and produce the target tensor break train data into batches and try learning a model that can fit the target test and validate until model convergence store metadata about the most fit model to predict : transform question data into input tensor load most fit model run input tensor into model transform output tensor into readable output You can learn more about the internals of mindsdb here . How can I help? You can help in the following ways: Trying MindsDB and reporting issues . If you know python, you can also help us debug open issues. Issues labels with the good first issue tag should be the easiest to start with . You can help us with documentation and examples. Tell your friends, write a blog post about MindsDB. Join our team, we are growing fast so we should have a few open positions . Why is it called MindsDB? Well, as most names, we needed one, we like science fiction and the culture series , where there are these AI super smart entities called Minds. How about the DB part?. Although in the future we will support all kinds of data, currently our objective is to add intelligence to existing data stores/databases, hence the term DB. As to becoming a Mind to your DB . Why the bear? Who doesn't like bears! Anyway, a bear for UC Berkeley where this all was initially coded. What is the difference between AI and Machine Learning? What is XAI?","title":"FAQ"},{"location":"FAQ/#why-mindsdb","text":"We are building MindsDB, because we want to Democratize Machine Learning . If you want to learn more about why this is important and how we aim to do this, you can check out our presentation here .","title":"Why MindsDB?"},{"location":"FAQ/#who-are-we-building-mindsdb-for","text":"We are building MindsDB for all of those that can get their hands in data. and can type a few lines of code.","title":"Who are we building MindsDB for?"},{"location":"FAQ/#what-are-mindsdbs-current-goals","text":"MindsDB has 3 simple goals. Provide a way to learn and predict from data with a line of code. Explainable, by answering the following questions: When learning: What is interesting in my data and why? When can I trust this model and why? When I should not trust this model and why? How can I improve this model? When predicting: Why this prediction? Why not something else? Remain state of the art. Since MindsDB users are delegating the ML machinery, MindsDB should try to always generate state of the art models for the users.","title":"What are MindsDB's current Goals?"},{"location":"FAQ/#what-is-the-roadmap","text":"MindsDB roadmap is aimed to be aligned with our goals: Success versions 1.0 MindsDB GUI where you can visualize explainability goals Support for images and complex text MindsDB REST API's Success versions 2.0 Run Machine Learning Models as Tables Important versions 3.0 Auto ETL and Data Preparation","title":"What is the roadmap?"},{"location":"FAQ/#what-type-of-data-can-mindsdb-learn-and-predict-from","text":"We support tabular data, this is CSV, Excel, JSON, text files, pandas data frame, URLs, s3 files or MySQL, Mongo, ClickHouse, PostgreSQL data stores. For more information please see the data sources documentation .","title":"What type of data can MindsDB learn and predict from?"},{"location":"FAQ/#how-does-it-work","text":"In very simple terms, MindsDB follows the following steps: to learn : break data source intro train, test, validate transform data source into tensors build and train encoders (if necessary) produce a neural network based model that can take in the input tensor and produce the target tensor break train data into batches and try learning a model that can fit the target test and validate until model convergence store metadata about the most fit model to predict : transform question data into input tensor load most fit model run input tensor into model transform output tensor into readable output You can learn more about the internals of mindsdb here .","title":"How does it work?"},{"location":"FAQ/#how-can-i-help","text":"You can help in the following ways: Trying MindsDB and reporting issues . If you know python, you can also help us debug open issues. Issues labels with the good first issue tag should be the easiest to start with . You can help us with documentation and examples. Tell your friends, write a blog post about MindsDB. Join our team, we are growing fast so we should have a few open positions .","title":"How can I help?"},{"location":"FAQ/#why-is-it-called-mindsdb","text":"Well, as most names, we needed one, we like science fiction and the culture series , where there are these AI super smart entities called Minds. How about the DB part?. Although in the future we will support all kinds of data, currently our objective is to add intelligence to existing data stores/databases, hence the term DB. As to becoming a Mind to your DB . Why the bear? Who doesn't like bears! Anyway, a bear for UC Berkeley where this all was initially coded.","title":"Why is it called MindsDB?"},{"location":"FAQ/#what-is-the-difference-between-ai-and-machine-learning","text":"","title":"What is the difference between AI and Machine Learning?"},{"location":"FAQ/#what-is-xai","text":"","title":"What is XAI?"},{"location":"FunctionalInterface/","text":"This section goes into detail about each of the methods exposed by Functional and each of the arguments they work with. All of the Functional methods act as utilities for Predictors. Get Models F.get_models() Takes no argument, returns a list with all the models and some information about them. Note: this is akin to a static method, it acts the same way no matter what predictor object you call it on, but due to various consideration it hasn't been swtiched to a static method yet Get Model Data F.get_model_data(model_name='model_name') Returns all the data we have about a given model. This is a rather complex python dictionary meant to be interpreted by the Scout GUI. We recommend looking at the training logs mindsdb_native gives to see some of these insights in an easy to read format. model_name -- Required argument, the name of the model to return data about. Note: this is akin to a static method, it acts the same way no matter what predictor object you call it on, but due to various consideration it hasn't been swtiched to a static method yet Export Model F.export_predictor(model_name='model_name') Exports this predictor's data (or the data of another predictor) to a zip file inside the CONFIG.MINDSDB_STORAGE_PATH directory. model_name -- The name of the model to export (defaults to the name of the current Predictor). Rename Model F.rename_model(old_model_name='old_name', new_model_name='new_name') Renames the created model. old_model_name: the name of the model you want to rename new_model_name: the new name of the model Export Storage F.export_storage(mindsdb_storage_dir='mindsdb_storage') Exports mindsdb's storage directory to a zip file. mindsdb_storage_dir -- The location where you want to save the mindsdb storage directory. Load Model F.import_model(model_archive_path='path/to/predictor.zip') Loads a predictor that was previously exported into the current mindsdb_native storage path so you can use it later. model_archive_path -- full_path that contains your mindsdb_native predictor zip file. Delete Model F.delete_model(model_name='blah') Deletes a given predictor from the storage path mindsdb_native is currently operating with. model_name -- The name of the model to delete (defaults to the name of the current Predictor). Analyse dataset F.analyse_dataset(from_data=the_data_source, sample_settings={}) Analyse the dataset inside the data source, file, ulr or pandas dataframe. This runs all the steps prior to actually training a predictive model. from_data -- the data that you want to analyse, this can be either a file, a pandas data frame, a url or a mindsdb data source. sample_settings -- A dictionary of options for sampling from the dataset. Includes sample_for_analysis . sample_for_training , sample_margin_of_error , sample_confidence_level , sample_percentage , sample_function .","title":"Functional Interface"},{"location":"FunctionalInterface/#get-models","text":"F.get_models() Takes no argument, returns a list with all the models and some information about them. Note: this is akin to a static method, it acts the same way no matter what predictor object you call it on, but due to various consideration it hasn't been swtiched to a static method yet","title":"Get Models"},{"location":"FunctionalInterface/#get-model-data","text":"F.get_model_data(model_name='model_name') Returns all the data we have about a given model. This is a rather complex python dictionary meant to be interpreted by the Scout GUI. We recommend looking at the training logs mindsdb_native gives to see some of these insights in an easy to read format. model_name -- Required argument, the name of the model to return data about. Note: this is akin to a static method, it acts the same way no matter what predictor object you call it on, but due to various consideration it hasn't been swtiched to a static method yet","title":"Get Model Data"},{"location":"FunctionalInterface/#export-model","text":"F.export_predictor(model_name='model_name') Exports this predictor's data (or the data of another predictor) to a zip file inside the CONFIG.MINDSDB_STORAGE_PATH directory. model_name -- The name of the model to export (defaults to the name of the current Predictor).","title":"Export Model"},{"location":"FunctionalInterface/#rename-model","text":"F.rename_model(old_model_name='old_name', new_model_name='new_name') Renames the created model. old_model_name: the name of the model you want to rename new_model_name: the new name of the model","title":"Rename Model"},{"location":"FunctionalInterface/#export-storage","text":"F.export_storage(mindsdb_storage_dir='mindsdb_storage') Exports mindsdb's storage directory to a zip file. mindsdb_storage_dir -- The location where you want to save the mindsdb storage directory.","title":"Export Storage"},{"location":"FunctionalInterface/#load-model","text":"F.import_model(model_archive_path='path/to/predictor.zip') Loads a predictor that was previously exported into the current mindsdb_native storage path so you can use it later. model_archive_path -- full_path that contains your mindsdb_native predictor zip file.","title":"Load Model"},{"location":"FunctionalInterface/#delete-model","text":"F.delete_model(model_name='blah') Deletes a given predictor from the storage path mindsdb_native is currently operating with. model_name -- The name of the model to delete (defaults to the name of the current Predictor).","title":"Delete Model"},{"location":"FunctionalInterface/#analyse-dataset","text":"F.analyse_dataset(from_data=the_data_source, sample_settings={}) Analyse the dataset inside the data source, file, ulr or pandas dataframe. This runs all the steps prior to actually training a predictive model. from_data -- the data that you want to analyse, this can be either a file, a pandas data frame, a url or a mindsdb data source. sample_settings -- A dictionary of options for sampling from the dataset. Includes sample_for_analysis . sample_for_training , sample_margin_of_error , sample_confidence_level , sample_percentage , sample_function .","title":"Analyse dataset"},{"location":"GoogleColab/","text":"Using MindsDB with Google Colab Google Colab is a free cloud service that supports free GPU! You can use MindsDB there. Fortunately, this is really easy. Inside Google Colab, start a new python 3 notebook and in a cell, insert the following: !pip install mindsdb Let's Build an Example First we'll import mindsdb from mindsdb import Predictor This is where it gets interesting. It's now up to you to install any dataset you want, so long as its a Excel or CSV file (or some other from of separator, doesn't necessarily have to be a \",\"). We'll be linking it to colab next. In this example we'll be using a students dataset from kaggle. You can get it here if you want to follow along. Once you have your CSV dataset, download it and put it in a new folder on your Google Drive. We'll call ours Datasets . We'll import it into colab using the following lines from google.colab import drive drive . mount ( '/content/drive' ) Now just follow the instructions and enter your authorization code. Here, we'll create a file variable that stores the path of our dataset. file = \"./drive/My Drive/Datasets/StudentsPerformance.csv\" Training Now let's create a MindsDB object and initialize it with our data from the file. We'll be prediciting the reading_score and we'll call our model 'reading_predictor'. Remember that depending on your dataset, these variables might change. Just remember that predict is the column you want to make your prediction on and that mindsdb will automatically rename all your columns to snake case. mdb = Predictor ( name = 'reading_score_predictor' ) mdb . learn ( from_data = file , # call file from google drive to_predict = 'reading_score' ) Testing mdb.predict needs 1 of 2 arguments to run a prediction: when_data is a file with one or more values or a dictionary of values for the columns we want to use for the prediction. The following example uses a dictionary via the when_data argument: # Load the `Predictor` we just trained via calling `learn` mdb = Predictor ( name = 'reading_score_predictor' ) # Make a prediction using a dictionary of input values predictions = mdb . predict ( when_data = { 'writing_score' : 80 , 'math_score' : 40 , 'lunch' : 'standard' } ) Finally we print out the result: # The dictionary containing the prediction print ( predictions ) # The confidence we have in the prediction (`0` being the lowest confidence and `1` being 100% confident) # Note, the confidence is not equal to the model's overall accuracy print ( predictions [ 'reading_score_confidence' ]) # The actual value predicted for `reading_score` print ( predictions [ 'reading_score' ]) You can find our Google Colab Example here.","title":"Running on Google Colab"},{"location":"GoogleColab/#using-mindsdb-with-google-colab","text":"Google Colab is a free cloud service that supports free GPU! You can use MindsDB there. Fortunately, this is really easy. Inside Google Colab, start a new python 3 notebook and in a cell, insert the following: !pip install mindsdb","title":"Using MindsDB with Google Colab"},{"location":"GoogleColab/#lets-build-an-example","text":"First we'll import mindsdb from mindsdb import Predictor This is where it gets interesting. It's now up to you to install any dataset you want, so long as its a Excel or CSV file (or some other from of separator, doesn't necessarily have to be a \",\"). We'll be linking it to colab next. In this example we'll be using a students dataset from kaggle. You can get it here if you want to follow along. Once you have your CSV dataset, download it and put it in a new folder on your Google Drive. We'll call ours Datasets . We'll import it into colab using the following lines from google.colab import drive drive . mount ( '/content/drive' ) Now just follow the instructions and enter your authorization code. Here, we'll create a file variable that stores the path of our dataset. file = \"./drive/My Drive/Datasets/StudentsPerformance.csv\"","title":"Let's Build an Example"},{"location":"GoogleColab/#training","text":"Now let's create a MindsDB object and initialize it with our data from the file. We'll be prediciting the reading_score and we'll call our model 'reading_predictor'. Remember that depending on your dataset, these variables might change. Just remember that predict is the column you want to make your prediction on and that mindsdb will automatically rename all your columns to snake case. mdb = Predictor ( name = 'reading_score_predictor' ) mdb . learn ( from_data = file , # call file from google drive to_predict = 'reading_score' )","title":"Training"},{"location":"GoogleColab/#testing","text":"mdb.predict needs 1 of 2 arguments to run a prediction: when_data is a file with one or more values or a dictionary of values for the columns we want to use for the prediction. The following example uses a dictionary via the when_data argument: # Load the `Predictor` we just trained via calling `learn` mdb = Predictor ( name = 'reading_score_predictor' ) # Make a prediction using a dictionary of input values predictions = mdb . predict ( when_data = { 'writing_score' : 80 , 'math_score' : 40 , 'lunch' : 'standard' } ) Finally we print out the result: # The dictionary containing the prediction print ( predictions ) # The confidence we have in the prediction (`0` being the lowest confidence and `1` being 100% confident) # Note, the confidence is not equal to the model's overall accuracy print ( predictions [ 'reading_score_confidence' ]) # The actual value predicted for `reading_score` print ( predictions [ 'reading_score' ]) You can find our Google Colab Example here.","title":"Testing"},{"location":"InsideMindsDB/","text":"Different transactions PREDICT, CREATE MODEL etc, require different steps/phases, however they may share some of these phases, in order to make this process modular we keep the variables in the Transaction controller (the data bus) as the communication interface, as such, the implementation of a given phase can change, so long as the expected variables in the bus prevail. (We will describe in more detail some of the Phase Modules in the next section). The MindsDB Stack DataExtractor It deals with extracting inputs from various data-sources such as files, directories and SQL compatible databases. If input is a query, it builds the joins with all implied tables (if any). StatsLoader : There is some transaction such as PREDICT where it's assumed that the statistical information is already known, all we have to do is make sure we load the right statistics to the transaction BUS. At the moment we don't support loading database from {char}svs that don't have headers or have incomplete headers. NOTE : That as of now mindsDB requires that the full dataset can be loaded into memory, in the future we might look into supporting very large datasets using something like apache drill to query a FS or db for the chunks of data we need in order to train and generate our statistical analysis . StatsGenerator Once the data is pulled and aggregated from the various data sources, MindsDB runs an analysis of each of the columns of the corpus. The purpose of the stats generator is two fold: To provide various data quality scores in order to determine the overall quality of a column (e.g. variance, some correlation metrics between columns, amount of duplicates). To provide properties about the columns which have to be used in the following steps and in order to rain the model. (e.g. histogram, data type) After all stats are computed, we warn the user of any interesting insights we found about his data and (if web logs are enabled), use the generated values to plot some interesting information about the data (e.g. data type distribution, outliers, histogram). Finally, the various stats are passed on as part of the metadata, so that further phases and the model itself can use them. Model Interface Train mode : When calling learn ,the model interface will feed the data to a machine learning framework which does the training in order to build a model. Predict mode : When calling predict , the model interface will feed the data to the model built by learn in order to generate a prediction. Data adaption : The ModelInterface phase is simply a lightweight wrapper over the model backends which handle adapting the data frame used by mindsdb into a format they can work with. During this process additional metadata for the machine learning libraries/frameworks is generated based on the results of the Stats Generator phase. Learning backend : The learning backends as the ensemble learning libraries used by mindsdb to train the model that will generate the predictions. Currently the learning backend we are working on supporting is Lightwood (created by us, based on the pre 1.0 version of mindsdb, work in progress). ModelAnalyzer The model analyzer phase runs after training is done in order to gather insights about the model and gather insights about the data that we can only get post-training. At the moment, it contains the fitting for a probabilistic model which is used to determine the accuracy of future prediction, based on the number of missing features and the bucket in which the predicted value falls.","title":"Inside MindsDB"},{"location":"InsideMindsDB/#the-mindsdb-stack","text":"","title":"The MindsDB Stack"},{"location":"InsideMindsDB/#dataextractor","text":"It deals with extracting inputs from various data-sources such as files, directories and SQL compatible databases. If input is a query, it builds the joins with all implied tables (if any). StatsLoader : There is some transaction such as PREDICT where it's assumed that the statistical information is already known, all we have to do is make sure we load the right statistics to the transaction BUS. At the moment we don't support loading database from {char}svs that don't have headers or have incomplete headers. NOTE : That as of now mindsDB requires that the full dataset can be loaded into memory, in the future we might look into supporting very large datasets using something like apache drill to query a FS or db for the chunks of data we need in order to train and generate our statistical analysis .","title":"DataExtractor"},{"location":"InsideMindsDB/#statsgenerator","text":"Once the data is pulled and aggregated from the various data sources, MindsDB runs an analysis of each of the columns of the corpus. The purpose of the stats generator is two fold: To provide various data quality scores in order to determine the overall quality of a column (e.g. variance, some correlation metrics between columns, amount of duplicates). To provide properties about the columns which have to be used in the following steps and in order to rain the model. (e.g. histogram, data type) After all stats are computed, we warn the user of any interesting insights we found about his data and (if web logs are enabled), use the generated values to plot some interesting information about the data (e.g. data type distribution, outliers, histogram). Finally, the various stats are passed on as part of the metadata, so that further phases and the model itself can use them.","title":"StatsGenerator"},{"location":"InsideMindsDB/#model-interface","text":"Train mode : When calling learn ,the model interface will feed the data to a machine learning framework which does the training in order to build a model. Predict mode : When calling predict , the model interface will feed the data to the model built by learn in order to generate a prediction. Data adaption : The ModelInterface phase is simply a lightweight wrapper over the model backends which handle adapting the data frame used by mindsdb into a format they can work with. During this process additional metadata for the machine learning libraries/frameworks is generated based on the results of the Stats Generator phase. Learning backend : The learning backends as the ensemble learning libraries used by mindsdb to train the model that will generate the predictions. Currently the learning backend we are working on supporting is Lightwood (created by us, based on the pre 1.0 version of mindsdb, work in progress).","title":"Model Interface"},{"location":"InsideMindsDB/#modelanalyzer","text":"The model analyzer phase runs after training is done in order to gather insights about the model and gather insights about the data that we can only get post-training. At the moment, it contains the fitting for a probabilistic model which is used to determine the accuracy of future prediction, based on the number of missing features and the bucket in which the predicted value falls.","title":"ModelAnalyzer"},{"location":"Installing/","text":"Installation Linux Installation Follow the Linux installation instructions. Windows Installation Follow the Windows installation instructions. macOS Installation Follow the macOS installation instructions. Install with Docker If none of the above specific OS installation options doesn't work for you, alternatively, you can also run MindsDB in a docker container assuming that you have docker installed in your computer. Run following commands to pull and run our latest image: docker pull mindsdb/mindsdb docker run -p 47334:47334 -p 47335:47335 -p 47336:47336 mindsdb/mindsdb Try out using GoogleCollab Checkout this example on GoogleCollab. Hardware Due to the fact that pytorch only supports certain instruction sets, mindsdb can only use certain types of GPUs. Currently, on AWS, g3 and p3 instance types should be fine, but p2 and g2 instances are not supported. VPS on DigitalOcean with 3 GB Memory and above should work. MindsDB Server After succesfull installation MindsDB Server can be started by running: python -m mindsdb You should see a similar message to: GUI should be available by http://127.0.0.1:47334/index.html Start on 127.0.0.1:47334 Serving on http://127.0.0.1:47334 To access MindsDB APIs visit http://127.0.0.1:47334 . To access MindsDB Scout visit http://127.0.0.1:47334/index.html","title":"Install MindsDB"},{"location":"Installing/#installation","text":"Linux Installation Follow the Linux installation instructions. Windows Installation Follow the Windows installation instructions. macOS Installation Follow the macOS installation instructions.","title":"Installation"},{"location":"Installing/#install-with-docker","text":"If none of the above specific OS installation options doesn't work for you, alternatively, you can also run MindsDB in a docker container assuming that you have docker installed in your computer. Run following commands to pull and run our latest image: docker pull mindsdb/mindsdb docker run -p 47334:47334 -p 47335:47335 -p 47336:47336 mindsdb/mindsdb","title":"Install with Docker"},{"location":"Installing/#try-out-using-googlecollab","text":"Checkout this example on GoogleCollab.","title":"Try out using GoogleCollab"},{"location":"Installing/#hardware","text":"Due to the fact that pytorch only supports certain instruction sets, mindsdb can only use certain types of GPUs. Currently, on AWS, g3 and p3 instance types should be fine, but p2 and g2 instances are not supported. VPS on DigitalOcean with 3 GB Memory and above should work.","title":"Hardware"},{"location":"Installing/#mindsdb-server","text":"After succesfull installation MindsDB Server can be started by running: python -m mindsdb You should see a similar message to: GUI should be available by http://127.0.0.1:47334/index.html Start on 127.0.0.1:47334 Serving on http://127.0.0.1:47334 To access MindsDB APIs visit http://127.0.0.1:47334 . To access MindsDB Scout visit http://127.0.0.1:47334/index.html","title":"MindsDB Server"},{"location":"JupyterNotebook/","text":"Create a notebook Click on the Link to Try it in your browser with Classic Notebook : You\u2019ll see a screen load with \u2018Binder\u2019 at the top. This should resolve to a screen, with a file menu near the top. On the far left to the file menu, select file, then drag down \u2018New Notebook\u2019 and from there select \u2018Python 3\u2019. You will then see Python command line Installing mindsdb and running In the command line type: !pip install git+https://github.com/mindsdb/mindsdb.git@master --user --no-cache-dir --upgrade --force-reinstall; then press the Run button in the top bar and wait for the install to finish. Now we can run one of our mindsdb examples, first by training a model: import mindsdb # Instantiate a mindsdb Predictor mdb = mindsdb . Predictor ( name = 'real_estate_model' ) # We tell the Predictor what column or key we want to learn and from what data mdb . learn ( from_data = \"https://s3.eu-west-2.amazonaws.com/mindsdb-example-data/home_rentals.csv\" , # the path to the file where we can learn from, (note: can be url) to_predict = 'rental_price' , # the column we want to learn to predict given all the data in the file ) Then generating some predictions: mdb = mindsdb . Predictor ( name = 'real_estate_model' ) # use the model to make predictions result = mdb . predict ( when_data = { 'number_of_rooms' : 2 , 'number_of_bathrooms' : 1 , 'sqft' : 1190 }) # The result will be an array containing predictions for each data point (in this case only one), a confidence for said prediction and a few other extra informations print ( 'The predicted price is $ {price} with {conf} confidence' . format ( price = result [ 0 ][ 'rental_price' ], conf = result [ 0 ][ 'rental_price_confidence' ]))","title":"Running on Jupyter Notebook"},{"location":"JupyterNotebook/#create-a-notebook","text":"Click on the Link to Try it in your browser with Classic Notebook : You\u2019ll see a screen load with \u2018Binder\u2019 at the top. This should resolve to a screen, with a file menu near the top. On the far left to the file menu, select file, then drag down \u2018New Notebook\u2019 and from there select \u2018Python 3\u2019. You will then see Python command line","title":"Create a notebook"},{"location":"JupyterNotebook/#installing-mindsdb-and-running","text":"In the command line type: !pip install git+https://github.com/mindsdb/mindsdb.git@master --user --no-cache-dir --upgrade --force-reinstall; then press the Run button in the top bar and wait for the install to finish. Now we can run one of our mindsdb examples, first by training a model: import mindsdb # Instantiate a mindsdb Predictor mdb = mindsdb . Predictor ( name = 'real_estate_model' ) # We tell the Predictor what column or key we want to learn and from what data mdb . learn ( from_data = \"https://s3.eu-west-2.amazonaws.com/mindsdb-example-data/home_rentals.csv\" , # the path to the file where we can learn from, (note: can be url) to_predict = 'rental_price' , # the column we want to learn to predict given all the data in the file ) Then generating some predictions: mdb = mindsdb . Predictor ( name = 'real_estate_model' ) # use the model to make predictions result = mdb . predict ( when_data = { 'number_of_rooms' : 2 , 'number_of_bathrooms' : 1 , 'sqft' : 1190 }) # The result will be an array containing predictions for each data point (in this case only one), a confidence for said prediction and a few other extra informations print ( 'The predicted price is $ {price} with {conf} confidence' . format ( price = result [ 0 ][ 'rental_price' ], conf = result [ 0 ][ 'rental_price_confidence' ]))","title":"Installing mindsdb and running"},{"location":"PredictorInterface/","text":"This section goes into detail about each of the methods exposed by Predictor and each of the arguments they work with. Predictor Note: The Predictor in MindsDB's words means Machine Learning Model. Constructor predictor = Predictor(name='weather_forecast') Constructs a new mindsdb predictor name -- Required argument, the name of the predictor, used for saving the predictor, creating a new predictor with the same name as a previous one loads the data from the old predictor root_folder -- The directory (also known as folder) where the predictor information should be saved log_level -- The log level that the predictor should use, number from 0 to 50, with 0 meaning log everything and 50 meaning log only fatal errors: DEBUG_LOG_LEVEL = 10 INFO_LOG_LEVEL = 20 WARNING_LOG_LEVEL = 30 ERROR_LOG_LEVEL = 40 NO_LOGS_LOG_LEVEL = 50 Learn predictor.learn(from_data=a_data_source, to_predict='a_column') Teach the predictor to make predictions on a given dataset, extract information about the dataset and extract information about the resulting machine learning model making the predictions. This is the \"main\" functionality of mindsdb_native together with the \"predict\" method. from_data -- the data that you want to use for training, this can be either a file, a pandas data frame, a url or a mindsdb data source. to_predict -- The columns/keys to be predicted (aka the targets, output columns, target variables), can be either a string (when specifying a single column) or a list of strings (when specifying multiple columns). test_from_data -- Specify a different data source on which mindsdb should test the machine learning model, by default mindsdb_native takes testing and validation samples from your dataset to use during training and analysis, and only trains the predictive model on ~80% of the data. This might seem sub-optimal if you're not used to machine learning, but trust us, it allows us to have much better confidence in the model we give you. timeseries specific parameters : group_by, window_size, order_by -- For more information on how to use these, please see the advanced examples section dealing with timeseries . Please note, these are currently subject to change, though they will remain backwards compatible until v2.0. ignore_columns -- Ignore certain columns from your data entirely. stop_training_in_x_seconds -- Stop training the model after this amount of seconds, note, the full amount it takes for mindsdb_native to run might be up to twice the amount you specify in this value. Thought, usually, the model training constitutes ~70-90% of the total mindsdb_native runtime. stop_training_in_accuracy -- Deprecated argument, left for backwards compatibility, to be removed or revamped in v2.0, refrain from using, it has no effects. backend -- The machine learning backend to use in order to train the model. This can be a string equal to lightwood (default) or ludwig , this can also be a custom model object. rebuild_model -- Defaults to True , if this is set to False the model will be loaded and the model analysis and data analysis will be re-run, but a new model won't be trained. use_gpu -- Defaults to None (autodetect), set to True if you have a GPU and want to make sure it's used or to False if you want to train the model on the CPU, this will speed up model training a lot in most situations. Note, that the default learning backend (lightwood) only work with relatively new (2016+) GPUs. equal_accuracy_for_all_output_categories -- When you have unbalanced target variable values, this will treat all of them as equally important when training the model. To see more information about this and an example, please see advanced section . output_categories_importance_dictionary -- A dictionary containing a number representing the importance for each (or some) values from the column to be predicted. An example of how his can be used (assume the column we are predicting is called is_true and takes two falues): {'is_true': {'True':0.6, 'False':1}} . The bigger the number (maximum value is one), the more important will it be for the model to predict that specific value correctly (usually at the cost of predicting other values correctly and getting more false positives for that value). advanced_args -- A dictionary of advanced arguments. Includes force_disable_cache , force_categorical_encoding , handle_foreign_keys , use_selfaware_model , deduplicate_data . sample_settings -- A dictionary of options for sampling from the dataset. Includes sample_for_analysis . sample_for_training , sample_margin_of_error , sample_confidence_level , sample_percentage , sample_function . If you are interested in using advanced_args or sample_settings but you are unsure of how they work, please shot us an email or create a github issue and we will help you. Predict predict(self, when_data = None, update_cached_model = False, use_gpu=False, advanced_args={}, backend=None, run_confidence_variation_analysis=False): predictor.predict(from_data=the_data_source) Make a prediction about a given dataset. when_data -- the data that you want to make the predictions for, this can be either a file, a pandas data frame, a url, a dictionary used for single prediction(column name: value) or a mindsdb data source. update_cached_model -- Deprecated argument, left for backwards compatibility, to be removed or revamped in v2.0, refrain from using, it has no effects. use_gpu -- Defaults to None (autodetect), set to True if you have a GPU and want to make sure it's used or to False if you want to train the model on the CPU, this will speed up model training a lot in most situations. Note, that the default learning backend (lightwood) only work with relatively new (2016+) GPUs. advanced_args -- A dictionary of advanced arguments. Includes force_disable_cache . backend -- The machine learning backend to use in order to train the model. This can be a string equal to lightwood (default) or ludwig , this can also be a custom model object. Note, you shouldn't use a different backend than the one you used to train the model, this will result in undefined behavior in the worst case scenario and most likely lead to a weired error. This defaults to whatever backend was last used when calling learn on this predictor. run_confidence_variation_analysis -- Run a confidence variation analysis on each of the given input column, currently only works when making single predictions via when_data . It provides some more in-depth analysis of a given prediction, by specifying how the confidence would increase/decrease based on which of the columns in the prediction were not present (had null, None or empty values). Test predictor.test(when_data=data, accuracy_score_functions='r2_score', score_using='predicted_value', predict_args={'use_gpu': True}): Test the overall confidence of the predictor e.g {'rental_price_accuracy': 0.95}. when_data -- Use this when you have data in either a file, a pandas data frame, or url to a file that you want to predict from. accuracy_score_functions -- A single function or a dictionary for the form {f'{target_name}': acc_func} when multiple targets are used. score_using -- What values from the explanation of the target to be used in the score function. predict_args -- Dictionary of arguments to be passed to predict (same arguments that predict accepts), e.g: predict_args={'use_gpu': True} . Predictor Quality Prediction Quality DataSources Mindsdb exposes a number of data sources that can be used with the predictor, you can find more details in the datasources section . Constants and Configuration For the constants and configuration options exposed by mindsdb_native at large please refer to this section .","title":"Predictor Interface"},{"location":"PredictorInterface/#predictor","text":"Note: The Predictor in MindsDB's words means Machine Learning Model.","title":"Predictor"},{"location":"PredictorInterface/#constructor","text":"predictor = Predictor(name='weather_forecast') Constructs a new mindsdb predictor name -- Required argument, the name of the predictor, used for saving the predictor, creating a new predictor with the same name as a previous one loads the data from the old predictor root_folder -- The directory (also known as folder) where the predictor information should be saved log_level -- The log level that the predictor should use, number from 0 to 50, with 0 meaning log everything and 50 meaning log only fatal errors: DEBUG_LOG_LEVEL = 10 INFO_LOG_LEVEL = 20 WARNING_LOG_LEVEL = 30 ERROR_LOG_LEVEL = 40 NO_LOGS_LOG_LEVEL = 50","title":"Constructor"},{"location":"PredictorInterface/#learn","text":"predictor.learn(from_data=a_data_source, to_predict='a_column') Teach the predictor to make predictions on a given dataset, extract information about the dataset and extract information about the resulting machine learning model making the predictions. This is the \"main\" functionality of mindsdb_native together with the \"predict\" method. from_data -- the data that you want to use for training, this can be either a file, a pandas data frame, a url or a mindsdb data source. to_predict -- The columns/keys to be predicted (aka the targets, output columns, target variables), can be either a string (when specifying a single column) or a list of strings (when specifying multiple columns). test_from_data -- Specify a different data source on which mindsdb should test the machine learning model, by default mindsdb_native takes testing and validation samples from your dataset to use during training and analysis, and only trains the predictive model on ~80% of the data. This might seem sub-optimal if you're not used to machine learning, but trust us, it allows us to have much better confidence in the model we give you. timeseries specific parameters : group_by, window_size, order_by -- For more information on how to use these, please see the advanced examples section dealing with timeseries . Please note, these are currently subject to change, though they will remain backwards compatible until v2.0. ignore_columns -- Ignore certain columns from your data entirely. stop_training_in_x_seconds -- Stop training the model after this amount of seconds, note, the full amount it takes for mindsdb_native to run might be up to twice the amount you specify in this value. Thought, usually, the model training constitutes ~70-90% of the total mindsdb_native runtime. stop_training_in_accuracy -- Deprecated argument, left for backwards compatibility, to be removed or revamped in v2.0, refrain from using, it has no effects. backend -- The machine learning backend to use in order to train the model. This can be a string equal to lightwood (default) or ludwig , this can also be a custom model object. rebuild_model -- Defaults to True , if this is set to False the model will be loaded and the model analysis and data analysis will be re-run, but a new model won't be trained. use_gpu -- Defaults to None (autodetect), set to True if you have a GPU and want to make sure it's used or to False if you want to train the model on the CPU, this will speed up model training a lot in most situations. Note, that the default learning backend (lightwood) only work with relatively new (2016+) GPUs. equal_accuracy_for_all_output_categories -- When you have unbalanced target variable values, this will treat all of them as equally important when training the model. To see more information about this and an example, please see advanced section . output_categories_importance_dictionary -- A dictionary containing a number representing the importance for each (or some) values from the column to be predicted. An example of how his can be used (assume the column we are predicting is called is_true and takes two falues): {'is_true': {'True':0.6, 'False':1}} . The bigger the number (maximum value is one), the more important will it be for the model to predict that specific value correctly (usually at the cost of predicting other values correctly and getting more false positives for that value). advanced_args -- A dictionary of advanced arguments. Includes force_disable_cache , force_categorical_encoding , handle_foreign_keys , use_selfaware_model , deduplicate_data . sample_settings -- A dictionary of options for sampling from the dataset. Includes sample_for_analysis . sample_for_training , sample_margin_of_error , sample_confidence_level , sample_percentage , sample_function . If you are interested in using advanced_args or sample_settings but you are unsure of how they work, please shot us an email or create a github issue and we will help you.","title":"Learn"},{"location":"PredictorInterface/#predict","text":"predict(self, when_data = None, update_cached_model = False, use_gpu=False, advanced_args={}, backend=None, run_confidence_variation_analysis=False): predictor.predict(from_data=the_data_source) Make a prediction about a given dataset. when_data -- the data that you want to make the predictions for, this can be either a file, a pandas data frame, a url, a dictionary used for single prediction(column name: value) or a mindsdb data source. update_cached_model -- Deprecated argument, left for backwards compatibility, to be removed or revamped in v2.0, refrain from using, it has no effects. use_gpu -- Defaults to None (autodetect), set to True if you have a GPU and want to make sure it's used or to False if you want to train the model on the CPU, this will speed up model training a lot in most situations. Note, that the default learning backend (lightwood) only work with relatively new (2016+) GPUs. advanced_args -- A dictionary of advanced arguments. Includes force_disable_cache . backend -- The machine learning backend to use in order to train the model. This can be a string equal to lightwood (default) or ludwig , this can also be a custom model object. Note, you shouldn't use a different backend than the one you used to train the model, this will result in undefined behavior in the worst case scenario and most likely lead to a weired error. This defaults to whatever backend was last used when calling learn on this predictor. run_confidence_variation_analysis -- Run a confidence variation analysis on each of the given input column, currently only works when making single predictions via when_data . It provides some more in-depth analysis of a given prediction, by specifying how the confidence would increase/decrease based on which of the columns in the prediction were not present (had null, None or empty values).","title":"Predict"},{"location":"PredictorInterface/#test","text":"predictor.test(when_data=data, accuracy_score_functions='r2_score', score_using='predicted_value', predict_args={'use_gpu': True}): Test the overall confidence of the predictor e.g {'rental_price_accuracy': 0.95}. when_data -- Use this when you have data in either a file, a pandas data frame, or url to a file that you want to predict from. accuracy_score_functions -- A single function or a dictionary for the form {f'{target_name}': acc_func} when multiple targets are used. score_using -- What values from the explanation of the target to be used in the score function. predict_args -- Dictionary of arguments to be passed to predict (same arguments that predict accepts), e.g: predict_args={'use_gpu': True} .","title":"Test"},{"location":"PredictorInterface/#predictor-quality","text":"","title":"Predictor Quality"},{"location":"PredictorInterface/#prediction-quality","text":"","title":"Prediction Quality"},{"location":"PredictorInterface/#datasources","text":"Mindsdb exposes a number of data sources that can be used with the predictor, you can find more details in the datasources section .","title":"DataSources"},{"location":"PredictorInterface/#constants-and-configuration","text":"For the constants and configuration options exposed by mindsdb_native at large please refer to this section .","title":"Constants and Configuration"},{"location":"comparisons/ComparsionWithOtherLibraries/","text":"Let's compared Mindsdb with some popular deep learning and machine learning libraries to show what makes it different. Models With libraries such as Tensorflow, Sklearn, Pytorch you must have the expertise to build models from scratch. Your models are also black boxes, you can't be sure how or why they work and you have to pre-process your data in a format that's suitable for the model and look for any errors in the data yourself. With Mindsdb anyone can build state of the art models without any machine learning knowledge. Mindsdb also provides data extraction, analyses your input data and analyses the resulting model to try and understand what makes it work and what types of situations it works best in. Data Preprocessing Building models require time for cleaning the data, normalizing the data, converting it into the format your library uses, determining the type of data in each column and a proper encoding for it Mindsdb can read data from csv, json, excel, file urls, s3 objects , dataframes and relational database tables or queries (currently there's native support for maraiadb, mysql and postgres), you just need to tell it which colum(s) it should predict. It will automatically process the data for you and give insights about the data. Code Samples We are going to use home_rentals.csv dataset for comparison purpose. Inside the dataset dir, you can find the dataset split into train and test data. Our goal is to predict the rental_price of the house given the information we have in home_rental.csv . We will look at doing this with Sklearn, Tensorflow, Ludwig and Mindsdb. Sklearn is a generic easy-to-use machine learning library. Tensorflow is the state of the art deep learning model building library from google. Ludwig is a library from Uber that aims to help people build machine learning models without knowledge of machine learning (similar to mindsdb) Building the model Now we will build the actual models to train on the training dataset and run some predictions on the testing dataset. For the purpose of this example, we'll build a simple linear regression with both Tensorflow and Sklearn, in order to keep the code to a minimum. Tensorflow import tensorflow as tf import pandas as pd import numpy as np from tensorflow import keras from tensorflow.keras import layers from sklearn.preprocessing import MinMaxScaler # process data df = pd . read_csv ( \"home_rentals.csv\" ) labels = df . pop ( 'rental_price' ) . values . reshape ( - 1 , 1 ) features = df . _get_numeric_data () . values xscaler = MinMaxScaler () features = xscaler . fit_transform ( features ) yscaler = MinMaxScaler () labels = yscaler . fit_transform ( labels ) input_dim = 4 # only numerical features for this example output_dim = 1 # predict rental_price # neural network definition inputs = keras . Input ( shape = ( 4 )) x = layers . Dense ( 100 )( inputs ) outputs = layers . Dense ( 1 )( x ) model = keras . Model ( inputs = inputs , outputs = outputs ) optimizer = keras . optimizers . SGD ( learning_rate = 1e-4 ) # transform data to TensorFlow format dataset = tf . data . Dataset . from_tensor_slices (( features . astype ( np . float32 ), labels . astype ( np . float32 ))) dataset = dataset . shuffle ( buffer_size = 64 ) . batch ( 32 ) def compute_loss ( labels , predictions ): return tf . reduce_mean ( tf . square ( labels - predictions )) # mean squared error def train_on_batch ( x , y ): with tf . GradientTape () as tape : predictions = model ( x ) loss = compute_loss ( y , predictions ) gradients = tape . gradient ( loss , model . trainable_weights ) optimizer . apply_gradients ( zip ( gradients , model . trainable_weights )) return loss epochs = 50 for epoch in range ( epochs ): for step , ( x , y ) in enumerate ( dataset ): loss = train_on_batch ( x , y ) if epoch % 10 == 0 : print ( f 'Epoch { epoch } : last batch loss = { float ( loss ) } ' ) # predict for test sample feat = [[ 2 , # rooms 1 , # bathrooms 1190 , # square feet 2000 ]] # initial price print ( \"The predicted price is %f \" % yscaler . inverse_transform ( model . predict ( xscaler . transform ( feat )))) Sklearn from sklearn import datasets , linear_model from sklearn.metrics import mean_squared_error , r2_score from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder import pandas as pd #load data data = pd . read_csv ( \"home_rentals.csv\" ) #target value labels = data [ 'rental_price' ] train1 = data . drop ([ 'rental_price' ], axis = 1 ) #train test split x_train , x_test , y_train , y_test = train_test_split ( train1 , labels ) # label encode values le = LabelEncoder () le . fit ( x_train [ 'location' ] . astype ( str )) x_train [ 'location' ] = le . transform ( x_train [ 'location' ] . astype ( str )) x_test [ 'location' ] = le . transform ( x_test [ 'location' ] . astype ( str )) le . fit ( x_train [ 'neighborhood' ] . astype ( str )) x_train [ 'neighborhood' ] = le . transform ( x_train [ 'neighborhood' ] . astype ( str )) x_test [ 'neighborhood' ] = le . transform ( x_test [ 'neighborhood' ] . astype ( str )) # Create linear regression object regr = linear_model . LinearRegression () # Train the model using the training sets regr . fit ( x_train , y_train ) # Make predictions using the testing set prediction = regr . predict ( x_test ) # The coefficients print ( 'Prediction ' , prediction ) print ( 'Coefficients: \\n ' , regr . coef_ ) # The mean squared error print ( 'Mean squared error: %.2f ' % mean_squared_error ( y_test , prediction )) # The coefficient of determination: 1 is perfect prediction print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , prediction )) Ludwig from ludwig.api import LudwigModel import pandas as pd # read data train_dataf = pd . read_csv ( \"train.csv\" ) # defining the data model_definition = { 'input_features' :[ { 'name' : 'number_of_rooms' , 'type' : 'numerical' }, { 'name' : 'number_of_bathrooms' , 'type' : 'numerical' }, { 'name' : 'sqft' , 'type' : 'numerical' }, { 'name' : 'location' , 'type' : 'text' }, { 'name' : 'days_on_market' , 'type' : 'numerical' }, { 'name' : 'initial_price' , 'type' : 'numerical' }, { 'name' : 'neighborhood' , 'type' : 'text' }, ], 'output_features' : [ { 'name' : 'rental_price' , 'type' : 'numerical' } ] } # creating and training the model model = LudwigModel ( model_definition ) train_stats = model . train ( data_df = train_dataf ) # read test data test_dataf = pd . read_csv ( \"test.csv\" ) #predict data predictions = model . predict ( data_df = test_dataf ) print ( predictions ) model . close () Note: If the data is inconsistent or of erroneous, null value it may throw an error. So you may want to preprocess/clean the data first. Mindsdb from mindsdb import Predictor # tell mindsDB what we want to learn and from what data Predictor ( name = 'home_rentals_price' ) . learn ( to_predict = 'rental_price' , # the column we want to learn to predict given all the data in the file from_data = 'train.csv' # the path to the file where we can learn from, (note: can be url) ) # use the model to make predictions result = Predictor ( name = 'home_rentals_price' ) . predict ( when = { 'number_of_rooms' : 2 , 'initial_price' : 2000 , 'number_of_bathrooms' : 1 , 'sqft' : 1190 }) # now print the results print ( 'The predicted price is between $ {price} with {conf} confidence' . format ( price = result [ 0 ] . explanation [ 'rental_price' ][ 'confidence_interval' ], conf = result [ 0 ] . explanation [ 'rental_price' ][ 'confidence' ])) Generally speaking, Mindsdb differentiates itself from other libraries by its simplicity . Lastly, Mindsdb Scout provides you with an easy way to visiualize more insights about the model. This can also be done by calling mdb.get_model_data('model_name') , but it's easier to use Mindsdb Scout to visualize the data, rather than looking at the raw json. Comparing Mindsdb accuracy with state-of-the-art models We have a few example datasets where we try to compare the accuracy obtained by Mindsdb with that of the best models we could find. It should be noted, Mindsdb accuracy doesn't always beat or match stat-of-the-art models, but the main goal of Mindsdb is to learn quickly, be very easy to use and be adaptable on any dataset. If you have the time and know-how to build a model that performs better than Mindsdb, but you still want the insights into the model and the data, as well as the pre-processing that Mindsdb provides, you can always plug in a custom machine learning model into Mindsdb. We are currently creating a new Benchmarks repository where you can find detailed list with up to date examples. Untill we release that you can check the old list of accuracy comparisons on our examples repo . Each directory containes different examples, datasets and README.md . To see the accuracies and the models, simply run mindsdb_acc.py to run mindsdb on the dataset. At some point we might keep a more easy to read list of these comparisons, but for now the results change to often and there are too many models to make this practical to maintain. We invite anyone with an interesting dataset and a well performing models to send it to us, or contribute to this repository, so that we can see how mindsdb stands up to it (or try it themselves and tell us the results they got).","title":"Comparison with other Libraries"},{"location":"comparisons/ComparsionWithOtherLibraries/#models","text":"With libraries such as Tensorflow, Sklearn, Pytorch you must have the expertise to build models from scratch. Your models are also black boxes, you can't be sure how or why they work and you have to pre-process your data in a format that's suitable for the model and look for any errors in the data yourself. With Mindsdb anyone can build state of the art models without any machine learning knowledge. Mindsdb also provides data extraction, analyses your input data and analyses the resulting model to try and understand what makes it work and what types of situations it works best in.","title":"Models"},{"location":"comparisons/ComparsionWithOtherLibraries/#data-preprocessing","text":"Building models require time for cleaning the data, normalizing the data, converting it into the format your library uses, determining the type of data in each column and a proper encoding for it Mindsdb can read data from csv, json, excel, file urls, s3 objects , dataframes and relational database tables or queries (currently there's native support for maraiadb, mysql and postgres), you just need to tell it which colum(s) it should predict. It will automatically process the data for you and give insights about the data.","title":"Data Preprocessing"},{"location":"comparisons/ComparsionWithOtherLibraries/#code-samples","text":"We are going to use home_rentals.csv dataset for comparison purpose. Inside the dataset dir, you can find the dataset split into train and test data. Our goal is to predict the rental_price of the house given the information we have in home_rental.csv . We will look at doing this with Sklearn, Tensorflow, Ludwig and Mindsdb. Sklearn is a generic easy-to-use machine learning library. Tensorflow is the state of the art deep learning model building library from google. Ludwig is a library from Uber that aims to help people build machine learning models without knowledge of machine learning (similar to mindsdb)","title":"Code Samples"},{"location":"comparisons/ComparsionWithOtherLibraries/#building-the-model","text":"Now we will build the actual models to train on the training dataset and run some predictions on the testing dataset. For the purpose of this example, we'll build a simple linear regression with both Tensorflow and Sklearn, in order to keep the code to a minimum.","title":"Building the model"},{"location":"comparisons/ComparsionWithOtherLibraries/#tensorflow","text":"import tensorflow as tf import pandas as pd import numpy as np from tensorflow import keras from tensorflow.keras import layers from sklearn.preprocessing import MinMaxScaler # process data df = pd . read_csv ( \"home_rentals.csv\" ) labels = df . pop ( 'rental_price' ) . values . reshape ( - 1 , 1 ) features = df . _get_numeric_data () . values xscaler = MinMaxScaler () features = xscaler . fit_transform ( features ) yscaler = MinMaxScaler () labels = yscaler . fit_transform ( labels ) input_dim = 4 # only numerical features for this example output_dim = 1 # predict rental_price # neural network definition inputs = keras . Input ( shape = ( 4 )) x = layers . Dense ( 100 )( inputs ) outputs = layers . Dense ( 1 )( x ) model = keras . Model ( inputs = inputs , outputs = outputs ) optimizer = keras . optimizers . SGD ( learning_rate = 1e-4 ) # transform data to TensorFlow format dataset = tf . data . Dataset . from_tensor_slices (( features . astype ( np . float32 ), labels . astype ( np . float32 ))) dataset = dataset . shuffle ( buffer_size = 64 ) . batch ( 32 ) def compute_loss ( labels , predictions ): return tf . reduce_mean ( tf . square ( labels - predictions )) # mean squared error def train_on_batch ( x , y ): with tf . GradientTape () as tape : predictions = model ( x ) loss = compute_loss ( y , predictions ) gradients = tape . gradient ( loss , model . trainable_weights ) optimizer . apply_gradients ( zip ( gradients , model . trainable_weights )) return loss epochs = 50 for epoch in range ( epochs ): for step , ( x , y ) in enumerate ( dataset ): loss = train_on_batch ( x , y ) if epoch % 10 == 0 : print ( f 'Epoch { epoch } : last batch loss = { float ( loss ) } ' ) # predict for test sample feat = [[ 2 , # rooms 1 , # bathrooms 1190 , # square feet 2000 ]] # initial price print ( \"The predicted price is %f \" % yscaler . inverse_transform ( model . predict ( xscaler . transform ( feat ))))","title":"Tensorflow"},{"location":"comparisons/ComparsionWithOtherLibraries/#sklearn","text":"from sklearn import datasets , linear_model from sklearn.metrics import mean_squared_error , r2_score from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder import pandas as pd #load data data = pd . read_csv ( \"home_rentals.csv\" ) #target value labels = data [ 'rental_price' ] train1 = data . drop ([ 'rental_price' ], axis = 1 ) #train test split x_train , x_test , y_train , y_test = train_test_split ( train1 , labels ) # label encode values le = LabelEncoder () le . fit ( x_train [ 'location' ] . astype ( str )) x_train [ 'location' ] = le . transform ( x_train [ 'location' ] . astype ( str )) x_test [ 'location' ] = le . transform ( x_test [ 'location' ] . astype ( str )) le . fit ( x_train [ 'neighborhood' ] . astype ( str )) x_train [ 'neighborhood' ] = le . transform ( x_train [ 'neighborhood' ] . astype ( str )) x_test [ 'neighborhood' ] = le . transform ( x_test [ 'neighborhood' ] . astype ( str )) # Create linear regression object regr = linear_model . LinearRegression () # Train the model using the training sets regr . fit ( x_train , y_train ) # Make predictions using the testing set prediction = regr . predict ( x_test ) # The coefficients print ( 'Prediction ' , prediction ) print ( 'Coefficients: \\n ' , regr . coef_ ) # The mean squared error print ( 'Mean squared error: %.2f ' % mean_squared_error ( y_test , prediction )) # The coefficient of determination: 1 is perfect prediction print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , prediction ))","title":"Sklearn"},{"location":"comparisons/ComparsionWithOtherLibraries/#ludwig","text":"from ludwig.api import LudwigModel import pandas as pd # read data train_dataf = pd . read_csv ( \"train.csv\" ) # defining the data model_definition = { 'input_features' :[ { 'name' : 'number_of_rooms' , 'type' : 'numerical' }, { 'name' : 'number_of_bathrooms' , 'type' : 'numerical' }, { 'name' : 'sqft' , 'type' : 'numerical' }, { 'name' : 'location' , 'type' : 'text' }, { 'name' : 'days_on_market' , 'type' : 'numerical' }, { 'name' : 'initial_price' , 'type' : 'numerical' }, { 'name' : 'neighborhood' , 'type' : 'text' }, ], 'output_features' : [ { 'name' : 'rental_price' , 'type' : 'numerical' } ] } # creating and training the model model = LudwigModel ( model_definition ) train_stats = model . train ( data_df = train_dataf ) # read test data test_dataf = pd . read_csv ( \"test.csv\" ) #predict data predictions = model . predict ( data_df = test_dataf ) print ( predictions ) model . close () Note: If the data is inconsistent or of erroneous, null value it may throw an error. So you may want to preprocess/clean the data first.","title":"Ludwig"},{"location":"comparisons/ComparsionWithOtherLibraries/#mindsdb","text":"from mindsdb import Predictor # tell mindsDB what we want to learn and from what data Predictor ( name = 'home_rentals_price' ) . learn ( to_predict = 'rental_price' , # the column we want to learn to predict given all the data in the file from_data = 'train.csv' # the path to the file where we can learn from, (note: can be url) ) # use the model to make predictions result = Predictor ( name = 'home_rentals_price' ) . predict ( when = { 'number_of_rooms' : 2 , 'initial_price' : 2000 , 'number_of_bathrooms' : 1 , 'sqft' : 1190 }) # now print the results print ( 'The predicted price is between $ {price} with {conf} confidence' . format ( price = result [ 0 ] . explanation [ 'rental_price' ][ 'confidence_interval' ], conf = result [ 0 ] . explanation [ 'rental_price' ][ 'confidence' ])) Generally speaking, Mindsdb differentiates itself from other libraries by its simplicity . Lastly, Mindsdb Scout provides you with an easy way to visiualize more insights about the model. This can also be done by calling mdb.get_model_data('model_name') , but it's easier to use Mindsdb Scout to visualize the data, rather than looking at the raw json.","title":"Mindsdb"},{"location":"comparisons/ComparsionWithOtherLibraries/#comparing-mindsdb-accuracy-with-state-of-the-art-models","text":"We have a few example datasets where we try to compare the accuracy obtained by Mindsdb with that of the best models we could find. It should be noted, Mindsdb accuracy doesn't always beat or match stat-of-the-art models, but the main goal of Mindsdb is to learn quickly, be very easy to use and be adaptable on any dataset. If you have the time and know-how to build a model that performs better than Mindsdb, but you still want the insights into the model and the data, as well as the pre-processing that Mindsdb provides, you can always plug in a custom machine learning model into Mindsdb. We are currently creating a new Benchmarks repository where you can find detailed list with up to date examples. Untill we release that you can check the old list of accuracy comparisons on our examples repo . Each directory containes different examples, datasets and README.md . To see the accuracies and the models, simply run mindsdb_acc.py to run mindsdb on the dataset. At some point we might keep a more easy to read list of these comparisons, but for now the results change to often and there are too many models to make this practical to maintain. We invite anyone with an interesting dataset and a well performing models to send it to us, or contribute to this repository, so that we can see how mindsdb stands up to it (or try it themselves and tell us the results they got).","title":"Comparing Mindsdb accuracy with state-of-the-art models"},{"location":"databases/","text":"AI Tables - Add Automated Machine Learning capabilities into databases You can make Machine Learning predictions directly inside the database by using MindsDB AI-Tables with the most popular database management systems like MySQL, MariaDB, PostgreSQL, ClickHouse. Any database user can now create, train and deploy machine learning models with the same knowledge they have of SQL. AI Tables benefits AI tables help to accelerate development speed and reduce complexity of machine learning workflows: Automated model training and deployment Predictions done at the Data Layer Saves time of moving to production Greater insight into model accuracy Easy to manage and use Open-source based Please have a look at the video below to learn more about AI tables concept: AITables example AITables differ from normal tables in that they can generate predictions upon being queried and returning such predictions like if it was data that existed on the table. Simply put, an AI-Table allows you to use machine learning models as if they were normal database tables, in something that in plain SQL looks like this; SELECT < predicted_variable > FROM < ML_model > WHERE < conditions > To really sink in this idea, let us expand the concept through an example. The used car price example Imagine that you want to solve the problem of estimating the right price for a car on your website that has been selling used cars over the past 2 years. The data is persistend in your database inside a table called used_cars_data where you keep records of every car you have sold so far, storing information such as: price, transmission, mileage, fuel_type, road_tax, mpg (Miles Per Gallon) and engine_size. Since you have historical data, you know that you could use Machine Learning to solve this problem. Wouldn't it be nice if you could simply tell your database server to do and manage the Machine Learning parts for you? At MindsDB we think so too! And AI-Tables baked directly to your database are here to do exactly that. You can for instance with a single INSERT statement, create a machine learning model/predictor trained to predict \u2018price\u2019 using the data that lives in the table sold_cars and publish it as an AI-Table called 'used_cars_model'. INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'used_cars_model' , 'price' , ' SELECT * FROM used_cars_data ); After that you can get price predictions by querying the generated 'used_cars_model' AI-Table, as follows: SELECT price , confidence FROM mindsdb . used_cars_model WHERE model = \"a6\" AND mileage = 36203 AND transmission = \"automatic\" AND fueltype = \"diesel\" AND mpg = \"64.2\" AND enginesize = 2 AND year = 2016 AND tax = 20 ; As you can see with AI-Tables, we are aiming to simplify Machine Learning mechanics to simple SQL queries, so that you can focus on the important part; which is to think about what predictions you need and what data you want your ML to learn from to make such predictions. Currently, we are supporting the integration with: MySQL PostgreSQL MariaDB ClickHouse","title":"AI Tables"},{"location":"databases/#ai-tables-add-automated-machine-learning-capabilities-into-databases","text":"You can make Machine Learning predictions directly inside the database by using MindsDB AI-Tables with the most popular database management systems like MySQL, MariaDB, PostgreSQL, ClickHouse. Any database user can now create, train and deploy machine learning models with the same knowledge they have of SQL.","title":"AI Tables - Add Automated Machine Learning capabilities into databases"},{"location":"databases/#ai-tables-benefits","text":"AI tables help to accelerate development speed and reduce complexity of machine learning workflows: Automated model training and deployment Predictions done at the Data Layer Saves time of moving to production Greater insight into model accuracy Easy to manage and use Open-source based Please have a look at the video below to learn more about AI tables concept:","title":"AI Tables benefits"},{"location":"databases/#aitables-example","text":"AITables differ from normal tables in that they can generate predictions upon being queried and returning such predictions like if it was data that existed on the table. Simply put, an AI-Table allows you to use machine learning models as if they were normal database tables, in something that in plain SQL looks like this; SELECT < predicted_variable > FROM < ML_model > WHERE < conditions > To really sink in this idea, let us expand the concept through an example.","title":"AITables example"},{"location":"databases/#the-used-car-price-example","text":"Imagine that you want to solve the problem of estimating the right price for a car on your website that has been selling used cars over the past 2 years. The data is persistend in your database inside a table called used_cars_data where you keep records of every car you have sold so far, storing information such as: price, transmission, mileage, fuel_type, road_tax, mpg (Miles Per Gallon) and engine_size. Since you have historical data, you know that you could use Machine Learning to solve this problem. Wouldn't it be nice if you could simply tell your database server to do and manage the Machine Learning parts for you? At MindsDB we think so too! And AI-Tables baked directly to your database are here to do exactly that. You can for instance with a single INSERT statement, create a machine learning model/predictor trained to predict \u2018price\u2019 using the data that lives in the table sold_cars and publish it as an AI-Table called 'used_cars_model'. INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'used_cars_model' , 'price' , ' SELECT * FROM used_cars_data ); After that you can get price predictions by querying the generated 'used_cars_model' AI-Table, as follows: SELECT price , confidence FROM mindsdb . used_cars_model WHERE model = \"a6\" AND mileage = 36203 AND transmission = \"automatic\" AND fueltype = \"diesel\" AND mpg = \"64.2\" AND enginesize = 2 AND year = 2016 AND tax = 20 ; As you can see with AI-Tables, we are aiming to simplify Machine Learning mechanics to simple SQL queries, so that you can focus on the important part; which is to think about what predictions you need and what data you want your ML to learn from to make such predictions. Currently, we are supporting the integration with: MySQL PostgreSQL MariaDB ClickHouse","title":"The used car price example"},{"location":"databases/Clickhouse/","text":"AI Tables in ClickHouse Now, you can train machine learning models straight from the database by using MindsDB and ClickHouse . Prerequisite You will need MindsDB version >= 2.0.0 and ClickHouse installed. Install MindsDB Install ClickHouse Configuration Default configuration MindsDB will try to use the default configuration(hosts, ports, usernames) for each of the database integrations. If you want to extend that or you are using different parameters create a new config.json file. The avaiable configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 0.0.0.0.) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default localhost). port(default 47335). log -- The logging configuration: console_level - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. file_level - \"INFO\", \"DEBUG\", \"ERROR\". folder logs - Directory of log files. format - Format of log message e.g \"%(asctime)s - %(levelname)s - %(message)s\". integrations['default_clickhouse'] -- This key specifies the integration type in this case default_clickhouse . The required keys are: user(default default) - The ClickHouse user name. host(default localhost) - Connect to the ClickHouse server on the given host. password - The password of the ClickHouse user. type - Integration type(mariadb, postgresql, mysql, clickhouse). port(default 8123) - The TCP/IP port number to use for the connection. interface -- This key is used by MindsDB and provides the path to the directory where MindsDB shall save configuration and model files: datastore enabled(default false) - If not provided MindsDB will use default storage inside /var. storage_dir - Path to the storage directory for datastore. mindsdb_native enabled - If not provided mindsdb_native will use default storage inside /var. storage_dir - Path to the storage directory for datastore. Configuration example { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_clickhouse\" : { \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"root\" , \"port\" : 8123 , \"type\" : \"mysql\" , \"user\" : \"root\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } Start MindsDB To start mindsdb run following command: python3 - m mindsdb -- api = mysql -- config = config . json The --api parameter specifies the type of API to use in this case mysql. The --config specifies the location of the configuration file. Train new model To train a new model, insert a new record inside the mindsdb.predictors table as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'airq_predictor' , 'SO2' , 'SELECT * FROM data.pollution_measurement' , { \"option\" : \"value\" } ); name (string) -- The name of the predictor. predict (string) -- The feature you want to predict, in this example SO2. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface . Query the model To query the model and get the predictions SELECT the target variable, confidence and explanation for that prediction. SELECT SO2 AS predicted , SO2_confidence AS confidence , SO2_explain AS info FROM airq_predictor WHERE ( NO2 = 0 . 005 ) AND ( CO = 1 . 2 ) AND ( PM10 = 5 ) You should get a similar response from MindsDB as: price predicted info 0.001156540079952395 0.9869 Check JSON bellow { \"predicted_value\" : 0.001156540079952395 , \"confidence\" : 0.9869 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 0.003184904620383531 , 0.013975553923630717 ], \"important_missing_information\" : [ \"Station code\" , \"Latitude\" , \"O3\" ], \"confidence_composition\" : { \"CO\" : 0.006 }, \"extra_insights\" : { \"if_missing\" : [{ \"NO2\" : 0.007549311956155897 }, { \"CO\" : 0.005459383721227349 }, { \"PM10\" : 0.003870252306568623 }] } } Delete the model If you want to delete the predictor that you have previously created run: INSERT INTO mindsdb . commands values ( 'DELETE predictor airq_predictor' ); To get additional information about the integration check out Machine Learning Models as Tables with ClickHouse tutorial.","title":"AI Tables in ClickHouse"},{"location":"databases/Clickhouse/#ai-tables-in-clickhouse","text":"Now, you can train machine learning models straight from the database by using MindsDB and ClickHouse .","title":"AI Tables in ClickHouse"},{"location":"databases/Clickhouse/#prerequisite","text":"You will need MindsDB version >= 2.0.0 and ClickHouse installed. Install MindsDB Install ClickHouse","title":"Prerequisite"},{"location":"databases/Clickhouse/#configuration","text":"Default configuration MindsDB will try to use the default configuration(hosts, ports, usernames) for each of the database integrations. If you want to extend that or you are using different parameters create a new config.json file. The avaiable configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 0.0.0.0.) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default localhost). port(default 47335). log -- The logging configuration: console_level - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. file_level - \"INFO\", \"DEBUG\", \"ERROR\". folder logs - Directory of log files. format - Format of log message e.g \"%(asctime)s - %(levelname)s - %(message)s\". integrations['default_clickhouse'] -- This key specifies the integration type in this case default_clickhouse . The required keys are: user(default default) - The ClickHouse user name. host(default localhost) - Connect to the ClickHouse server on the given host. password - The password of the ClickHouse user. type - Integration type(mariadb, postgresql, mysql, clickhouse). port(default 8123) - The TCP/IP port number to use for the connection. interface -- This key is used by MindsDB and provides the path to the directory where MindsDB shall save configuration and model files: datastore enabled(default false) - If not provided MindsDB will use default storage inside /var. storage_dir - Path to the storage directory for datastore. mindsdb_native enabled - If not provided mindsdb_native will use default storage inside /var. storage_dir - Path to the storage directory for datastore. Configuration example { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_clickhouse\" : { \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"root\" , \"port\" : 8123 , \"type\" : \"mysql\" , \"user\" : \"root\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" }","title":"Configuration"},{"location":"databases/Clickhouse/#start-mindsdb","text":"To start mindsdb run following command: python3 - m mindsdb -- api = mysql -- config = config . json The --api parameter specifies the type of API to use in this case mysql. The --config specifies the location of the configuration file.","title":"Start MindsDB"},{"location":"databases/Clickhouse/#train-new-model","text":"To train a new model, insert a new record inside the mindsdb.predictors table as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'airq_predictor' , 'SO2' , 'SELECT * FROM data.pollution_measurement' , { \"option\" : \"value\" } ); name (string) -- The name of the predictor. predict (string) -- The feature you want to predict, in this example SO2. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface .","title":"Train new model"},{"location":"databases/Clickhouse/#query-the-model","text":"To query the model and get the predictions SELECT the target variable, confidence and explanation for that prediction. SELECT SO2 AS predicted , SO2_confidence AS confidence , SO2_explain AS info FROM airq_predictor WHERE ( NO2 = 0 . 005 ) AND ( CO = 1 . 2 ) AND ( PM10 = 5 ) You should get a similar response from MindsDB as: price predicted info 0.001156540079952395 0.9869 Check JSON bellow { \"predicted_value\" : 0.001156540079952395 , \"confidence\" : 0.9869 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 0.003184904620383531 , 0.013975553923630717 ], \"important_missing_information\" : [ \"Station code\" , \"Latitude\" , \"O3\" ], \"confidence_composition\" : { \"CO\" : 0.006 }, \"extra_insights\" : { \"if_missing\" : [{ \"NO2\" : 0.007549311956155897 }, { \"CO\" : 0.005459383721227349 }, { \"PM10\" : 0.003870252306568623 }] } }","title":"Query the model"},{"location":"databases/Clickhouse/#delete-the-model","text":"If you want to delete the predictor that you have previously created run: INSERT INTO mindsdb . commands values ( 'DELETE predictor airq_predictor' ); To get additional information about the integration check out Machine Learning Models as Tables with ClickHouse tutorial.","title":"Delete the model"},{"location":"databases/MariaDB/","text":"AI Tables in MariaDB Now, you can train machine learning models straight from the database by using MindsDB and MariaDB . Prerequisite You will need MindsDB version >= 2.0.0 and MariaDB installed: Install MindsDB Install MariaDB Configuration Default configuration MindsDB will try to use the default configuration(hosts, ports, usernames) for each of the database integrations. If you want to extend that or you are using different parameters creata a new config.json file. The avaiable configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 0.0.0.0.) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default localhost). port(default 47335). log -- The logging configuration: console_level - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. file_level - \"INFO\", \"DEBUG\", \"ERROR\". folder logs - Directory of log files. format - Format of log message e.g \"%(asctime)s - %(levelname)s - %(message)s\". integrations -- This key specifies the integration type in this case default_mariadb . The required keys are: user(default root) - The MariaDB user name. host(default localhost) - Connect to the MariaDB server on the given host. password - The password of the MariaDB account. type - Integration type(mariadb, postgresql, mysql, clickhouse). port(default 3306) - The TCP/IP port number to use for the connection. interface -- This key is used by MindsDB and provides the path to the directory where MindsDB shall save configuration and model files: datastore enabled(default false) - If not provided MindsDB will use default storage inside /var. storage_dir - Path to the storage directory for datastore. mindsdb_native enabled - If not provided mindsdb_native will use default storage inside /var. storage_dir - Path to the storage directory for datastore. Configuration example { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_mariadb\" : { \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"password\" , \"port\" : 3306 , \"type\" : \"mariadb\" , \"user\" : \"root\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } Install CONNECT Storage Engine Also you need to install the CONNECT Storage Engine to access external local data. Checkout MariaDB docs on how to do that. Start MindsDB To start mindsdb run following command: python3 - m mindsdb -- api = mysql -- config = config . json The --api parameter specifies the type of API to use in this case mysql. The --config specifies the location of the configuration file. Train new model To train a new model, insert a new record inside the mindsdb.predictors table as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'used_cars_model' , 'price' , 'SELECT * FROM test.UsedCarsData' , \"option,value\" ); name (string) -- The name of the predictor. predict (string) -- The feature you want to predict, in this example price. To predict multiple featurs include a comma separated string e.g 'price,year'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface . Query the model To query the model and get the predictions SELECT the target variable, confidence and explanation for that prediction. SELECT price AS predicted , price_confidence AS confidence , price_explain AS info FROM mindsdb . used_cars_model WHERE model = \"A6\" AND mileage = 36203 AND transmission = \"Automatic\" AND fuelType = \"Diesel\" AND mpg = \"64.2\" AND engineSize = 2 AND year = 2016 AND tax = 20 ; You should get a similar response from MindsDB as: price predicted info 13111 0.9921 Check JSON bellow { \"predicted_value\" : 13111 , \"confidence\" : 0.9921 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 10792 , 32749 ], \"important_missing_information\" : [], \"confidence_composition\" : { \"Model\" : 0.009 , \"year\" : 0.013 }, \"extra_insights\" : { \"if_missing\" : [{ \"Model\" : 12962 }, { \"year\" : 12137 }, { \"transmission\" : 2136 }, { \"mileage\" : 22706 }, { \"fuelType\" : 7134 }, { \"tax\" : 13210 }, { \"mpg\" : 27409 }, { \"engineSize\" : 13111 }] } } Delete the model To delete the predictor that you have previously created, you need to delete it from mindsdb.predictors table. The name should be equal to name added in the INSERT statment while creating the predictor, e.g: DELETE FROM mindsdb . predictors WHERE name = 'used_cars_model' Train and predict multiple features You can train a model that will predict multiple features by adding a comma separated features values in the predict column. e.g to predict the price and a year : INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'used_cars_model' , 'price,year' , 'SELECT * FROM test.UsedCarsData' , \"option,value\" } ); And query it using the select_data_query : SELECT price AS predicted , FROM mindsdb . used_cars_model WHERE select_data_query = 'SELECT year FROM price_data' ; The requirements to query with select_data_query are: It must be a valid SQL statement It must return columns with names the same as predictor fields. If you want to follow along with a tutorial check out AI Tables in MariaDB tutorial.","title":"AI Tables in MariaDB"},{"location":"databases/MariaDB/#ai-tables-in-mariadb","text":"Now, you can train machine learning models straight from the database by using MindsDB and MariaDB .","title":"AI Tables in MariaDB"},{"location":"databases/MariaDB/#prerequisite","text":"You will need MindsDB version >= 2.0.0 and MariaDB installed: Install MindsDB Install MariaDB","title":"Prerequisite"},{"location":"databases/MariaDB/#configuration","text":"Default configuration MindsDB will try to use the default configuration(hosts, ports, usernames) for each of the database integrations. If you want to extend that or you are using different parameters creata a new config.json file. The avaiable configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 0.0.0.0.) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default localhost). port(default 47335). log -- The logging configuration: console_level - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. file_level - \"INFO\", \"DEBUG\", \"ERROR\". folder logs - Directory of log files. format - Format of log message e.g \"%(asctime)s - %(levelname)s - %(message)s\". integrations -- This key specifies the integration type in this case default_mariadb . The required keys are: user(default root) - The MariaDB user name. host(default localhost) - Connect to the MariaDB server on the given host. password - The password of the MariaDB account. type - Integration type(mariadb, postgresql, mysql, clickhouse). port(default 3306) - The TCP/IP port number to use for the connection. interface -- This key is used by MindsDB and provides the path to the directory where MindsDB shall save configuration and model files: datastore enabled(default false) - If not provided MindsDB will use default storage inside /var. storage_dir - Path to the storage directory for datastore. mindsdb_native enabled - If not provided mindsdb_native will use default storage inside /var. storage_dir - Path to the storage directory for datastore. Configuration example { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_mariadb\" : { \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"password\" , \"port\" : 3306 , \"type\" : \"mariadb\" , \"user\" : \"root\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } Install CONNECT Storage Engine Also you need to install the CONNECT Storage Engine to access external local data. Checkout MariaDB docs on how to do that.","title":"Configuration"},{"location":"databases/MariaDB/#start-mindsdb","text":"To start mindsdb run following command: python3 - m mindsdb -- api = mysql -- config = config . json The --api parameter specifies the type of API to use in this case mysql. The --config specifies the location of the configuration file.","title":"Start MindsDB"},{"location":"databases/MariaDB/#train-new-model","text":"To train a new model, insert a new record inside the mindsdb.predictors table as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'used_cars_model' , 'price' , 'SELECT * FROM test.UsedCarsData' , \"option,value\" ); name (string) -- The name of the predictor. predict (string) -- The feature you want to predict, in this example price. To predict multiple featurs include a comma separated string e.g 'price,year'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface .","title":"Train new model"},{"location":"databases/MariaDB/#query-the-model","text":"To query the model and get the predictions SELECT the target variable, confidence and explanation for that prediction. SELECT price AS predicted , price_confidence AS confidence , price_explain AS info FROM mindsdb . used_cars_model WHERE model = \"A6\" AND mileage = 36203 AND transmission = \"Automatic\" AND fuelType = \"Diesel\" AND mpg = \"64.2\" AND engineSize = 2 AND year = 2016 AND tax = 20 ; You should get a similar response from MindsDB as: price predicted info 13111 0.9921 Check JSON bellow { \"predicted_value\" : 13111 , \"confidence\" : 0.9921 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 10792 , 32749 ], \"important_missing_information\" : [], \"confidence_composition\" : { \"Model\" : 0.009 , \"year\" : 0.013 }, \"extra_insights\" : { \"if_missing\" : [{ \"Model\" : 12962 }, { \"year\" : 12137 }, { \"transmission\" : 2136 }, { \"mileage\" : 22706 }, { \"fuelType\" : 7134 }, { \"tax\" : 13210 }, { \"mpg\" : 27409 }, { \"engineSize\" : 13111 }] } }","title":"Query the model"},{"location":"databases/MariaDB/#delete-the-model","text":"To delete the predictor that you have previously created, you need to delete it from mindsdb.predictors table. The name should be equal to name added in the INSERT statment while creating the predictor, e.g: DELETE FROM mindsdb . predictors WHERE name = 'used_cars_model'","title":"Delete the model"},{"location":"databases/MariaDB/#train-and-predict-multiple-features","text":"You can train a model that will predict multiple features by adding a comma separated features values in the predict column. e.g to predict the price and a year : INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'used_cars_model' , 'price,year' , 'SELECT * FROM test.UsedCarsData' , \"option,value\" } ); And query it using the select_data_query : SELECT price AS predicted , FROM mindsdb . used_cars_model WHERE select_data_query = 'SELECT year FROM price_data' ; The requirements to query with select_data_query are: It must be a valid SQL statement It must return columns with names the same as predictor fields. If you want to follow along with a tutorial check out AI Tables in MariaDB tutorial.","title":"Train and predict multiple features"},{"location":"databases/MySQL/","text":"AI Tables in MySQL Now, you can train machine learning models straight from the database by using MindsDB and MySQL . Prerequisite You will need MindsDB version >= 2.3.0 and MySQL installed: Install MindsDB Install MySQL Enable FEDERATED Storage Engine Configuration Default configuration MindsDB will try to use the default configuration(hosts, ports, usernames) for each of the database integrations. If you want to extend that or you are using different parameters create a new config.json file. The available configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_mysql'] -- This key specifies the integration type in this case default_mysql . The required keys are: user(default root) - The MySQL user name. host(default 127.0.0.1) - Connect to the MySQL server on the given host. password - The password of the MySQL account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 3306) - The TCP/IP port number to use for the connection. enabled(true|false) - Enable MySQL integration. log['level'] -- The logging configuration(optional): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsdb will store models and configuration. Configuration example { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_mysql\" : { \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"root\" , \"port\" : 3307 , \"type\" : \"mysql\" , \"user\" : \"root\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } Enable FEDERATED storage engine The FEDERATED storage engine is not enabled by default in the running server; to enable FEDERATED, you must start the MySQL server binary using the --federated option. Check official docs for more info. Start MindsDB To start mindsdb run following command: python3 - m mindsdb -- api = mysql -- config = config . json The --api parameter specifies the type of API to use in this case mysql. The --config specifies the location of the configuration file. Train new model To train a new model, insert a new record inside the mindsdb.predictors table as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'us_consumption' , 'consumption' , 'SELECT * FROM us_consumption' ); name (string) -- The name of the predictor. predict (string) -- The feature you want to predict, in this example consumption. To predict multiple featurs include a comma separated string e.g 'consumption,income'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface . Query the model To query the model and get the predictions SELECT the target variable, confidence and explanation for that prediction. SELECT consumption AS predicted , consumption_confidence AS confidence , consumption_explain AS info FROM mindsdb . us_consumption WHERE when_data = '{\"income\": 1.182497938, \"production\": 5.854555956,\"savings\": 3.183292657, \"unemployment\": 0.1, \"t\":\"2020-01-02\"}' ; You should get a similar response from MindsDB as: consumption predicted info 1.252233223 0.923 Check JSON bellow { \"predicted_value\" : 1.252233223 , \"confidence\" : 0.923 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 1.025658879956537 , 1.9702775375019028 ], \"important_missing_information\" : [], \"confidence_composition\" : {}, \"extra_insights\" : { \"if_missing\" : [{ \"income\" : 0.6966906986877563 }, { \"production\" : 2.5382917051924445 }, { \"savings\" : 1.169812868271305 }, { \"unemployment\" : 1.3 443338862946717 }] } } Delete the model To delete the predictor that you have previously created, you need to delete it from mindsdb.predictors table. The name should be equal to name added in the INSERT statment while creating the predictor, e.g: DELETE FROM mindsdb . predictors WHERE name = 'us_consumption' Train and predict multiple features You can train a model that will predict multiple features by adding a comma separated features values in the predict column. e.g to predict the consumption and a income : INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'us_consumption' , 'consumption, income' , 'SELECT * FROM us_consumption' , \"option,value\" } ); And query it using the select_data_query : SELECT consumption AS predicted , FROM mindsdb . us_consumption WHERE select_data_query = 'SELECT income FROM us_consumption' ; The requirements to query with select_data_query are: It must be a valid SQL statement It must return columns with names the same as predictor fields. To get additional information follow the AiTables in MySQL tutorial .","title":"AI Tables in MySQL"},{"location":"databases/MySQL/#ai-tables-in-mysql","text":"Now, you can train machine learning models straight from the database by using MindsDB and MySQL .","title":"AI Tables in MySQL"},{"location":"databases/MySQL/#prerequisite","text":"You will need MindsDB version >= 2.3.0 and MySQL installed: Install MindsDB Install MySQL Enable FEDERATED Storage Engine","title":"Prerequisite"},{"location":"databases/MySQL/#configuration","text":"Default configuration MindsDB will try to use the default configuration(hosts, ports, usernames) for each of the database integrations. If you want to extend that or you are using different parameters create a new config.json file. The available configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_mysql'] -- This key specifies the integration type in this case default_mysql . The required keys are: user(default root) - The MySQL user name. host(default 127.0.0.1) - Connect to the MySQL server on the given host. password - The password of the MySQL account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 3306) - The TCP/IP port number to use for the connection. enabled(true|false) - Enable MySQL integration. log['level'] -- The logging configuration(optional): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsdb will store models and configuration. Configuration example { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_mysql\" : { \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"root\" , \"port\" : 3307 , \"type\" : \"mysql\" , \"user\" : \"root\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } Enable FEDERATED storage engine The FEDERATED storage engine is not enabled by default in the running server; to enable FEDERATED, you must start the MySQL server binary using the --federated option. Check official docs for more info.","title":"Configuration"},{"location":"databases/MySQL/#start-mindsdb","text":"To start mindsdb run following command: python3 - m mindsdb -- api = mysql -- config = config . json The --api parameter specifies the type of API to use in this case mysql. The --config specifies the location of the configuration file.","title":"Start MindsDB"},{"location":"databases/MySQL/#train-new-model","text":"To train a new model, insert a new record inside the mindsdb.predictors table as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'us_consumption' , 'consumption' , 'SELECT * FROM us_consumption' ); name (string) -- The name of the predictor. predict (string) -- The feature you want to predict, in this example consumption. To predict multiple featurs include a comma separated string e.g 'consumption,income'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface .","title":"Train new model"},{"location":"databases/MySQL/#query-the-model","text":"To query the model and get the predictions SELECT the target variable, confidence and explanation for that prediction. SELECT consumption AS predicted , consumption_confidence AS confidence , consumption_explain AS info FROM mindsdb . us_consumption WHERE when_data = '{\"income\": 1.182497938, \"production\": 5.854555956,\"savings\": 3.183292657, \"unemployment\": 0.1, \"t\":\"2020-01-02\"}' ; You should get a similar response from MindsDB as: consumption predicted info 1.252233223 0.923 Check JSON bellow { \"predicted_value\" : 1.252233223 , \"confidence\" : 0.923 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 1.025658879956537 , 1.9702775375019028 ], \"important_missing_information\" : [], \"confidence_composition\" : {}, \"extra_insights\" : { \"if_missing\" : [{ \"income\" : 0.6966906986877563 }, { \"production\" : 2.5382917051924445 }, { \"savings\" : 1.169812868271305 }, { \"unemployment\" : 1.3 443338862946717 }] } }","title":"Query the model"},{"location":"databases/MySQL/#delete-the-model","text":"To delete the predictor that you have previously created, you need to delete it from mindsdb.predictors table. The name should be equal to name added in the INSERT statment while creating the predictor, e.g: DELETE FROM mindsdb . predictors WHERE name = 'us_consumption'","title":"Delete the model"},{"location":"databases/MySQL/#train-and-predict-multiple-features","text":"You can train a model that will predict multiple features by adding a comma separated features values in the predict column. e.g to predict the consumption and a income : INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'us_consumption' , 'consumption, income' , 'SELECT * FROM us_consumption' , \"option,value\" } ); And query it using the select_data_query : SELECT consumption AS predicted , FROM mindsdb . us_consumption WHERE select_data_query = 'SELECT income FROM us_consumption' ; The requirements to query with select_data_query are: It must be a valid SQL statement It must return columns with names the same as predictor fields. To get additional information follow the AiTables in MySQL tutorial .","title":"Train and predict multiple features"},{"location":"databases/PostgreSQL/","text":"AI Tables in PostgreSQL Now, you can train machine learning models straight from the database by using MindsDB and PostgreSQL . Prerequisite You will need MindsDB version >= 2.3.0 and PostgreSQL installed: Install MindsDB Install PostgreSQL Install PostgreSQL foreign data wrapper for MySQL Configuration Default configuration MindsDB will try to use the default configuration(hosts, ports, usernames) for each of the database integrations. If you want to extend that or you are using different parameters create a new config.json file. The avaiable configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_postgres'] -- This key specifies the integration type in this case default_postgres . The required keys are: user(default postgres) - The Postgres user name. host(default 127.0.0.1) - Connect to the PostgreSQL server on the given host. password - The password of the Postgres account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 5432) - The TCP/IP port number to use for the connection. log['level'] -- The logging configuration(not required): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsdb will store models and configuration. Configuration example { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_postgres\" : { \"database\" : \"postgres\" , \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"postgres\" , \"port\" : 5432 , \"type\" : \"postgres\" , \"user\" : \"postgres\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } Install PostgreSQL foreign data wrapper for MySQL The Foreign Data Wrapper (mysql_fwd) can be installed from the EnterpriseDB repo . Start MindsDB To start mindsdb run following command: python3 - m mindsdb -- api = mysql -- config = config . json The --api parameter specifies the type of API to use in this case mysql. The --config specifies the location of the configuration file. Train new model To train a new model, insert a new record inside the mindsdb.predictors table as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'us_consumption' , 'consumption' , 'SELECT * FROM us_consumption' , '{\"timeseries_settings\":{\"order_by\": [\"t\"], \"window\":20}}' ); name (string) -- The name of the predictor. predict (string) -- The feature you want to predict, in this example consumption. To predict multiple featurs include a comma separated string e.g 'consumption,income'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface . If you are using timeseries data check the Timeseries settings . Query the model To query the model and get the predictions SELECT the target variable, confidence and explanation for that prediction. SELECT consumption AS predicted , consumption_confidence AS confidence , consumption_explain AS info FROM mindsdb . us_consumption WHERE income = 1 . 182497938 AND production = 5 . 854555956 AND savings = 3 . 183292657 AND unemployment = 0 . 1 You should get a similar response from MindsDB as: consumption predicted info 1.4979682087292199 0.9475 Check JSON bellow { \"predicted_value\" : 1.4979682087292199 , \"confidence\" : 0.9475 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 1.025658879956537 , 1.9702775375019028 ], \"important_missing_information\" : [], \"confidence_composition\" : {}, \"extra_insights\" : { \"if_missing\" : [{ \"income\" : 0.6966906986877563 }, { \"production\" : 2.5382917051924445 }, { \"savings\" : 1.169812868271305 }, { \"unemployment\" : 1.3 443338862946717 }] } } Delete the model To delete the predictor that you have previously created, you need to delete it from mindsdb.predictors table. The name should be equal to name added in the INSERT statment while creating the predictor, e.g: DELETE FROM mindsdb . predictors WHERE name = 'us_consumption' Train and predict multiple features You can train a model that will predict multiple features by adding a comma separated features values in the predict column. e.g to predict the consumption and a income : INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'us_consumption' , 'consumption, income' , 'SELECT * FROM us_consumption' , \"option,value\" } ); And query it using the select_data_query : SELECT consumption AS predicted , FROM mindsdb . used_cars_model WHERE select_data_query = 'SELECT income FROM us_consumption' ; The requirements to query with select_data_query are: It must be a valid SQL statement It must return columns with names the same as predictor fields. To get additional information follow the AiTables in PostgreSQL tutorail .","title":"AI Tables in PostgreSQL"},{"location":"databases/PostgreSQL/#ai-tables-in-postgresql","text":"Now, you can train machine learning models straight from the database by using MindsDB and PostgreSQL .","title":"AI Tables in PostgreSQL"},{"location":"databases/PostgreSQL/#prerequisite","text":"You will need MindsDB version >= 2.3.0 and PostgreSQL installed: Install MindsDB Install PostgreSQL Install PostgreSQL foreign data wrapper for MySQL","title":"Prerequisite"},{"location":"databases/PostgreSQL/#configuration","text":"Default configuration MindsDB will try to use the default configuration(hosts, ports, usernames) for each of the database integrations. If you want to extend that or you are using different parameters create a new config.json file. The avaiable configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_postgres'] -- This key specifies the integration type in this case default_postgres . The required keys are: user(default postgres) - The Postgres user name. host(default 127.0.0.1) - Connect to the PostgreSQL server on the given host. password - The password of the Postgres account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 5432) - The TCP/IP port number to use for the connection. log['level'] -- The logging configuration(not required): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsdb will store models and configuration. Configuration example { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_postgres\" : { \"database\" : \"postgres\" , \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"postgres\" , \"port\" : 5432 , \"type\" : \"postgres\" , \"user\" : \"postgres\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } Install PostgreSQL foreign data wrapper for MySQL The Foreign Data Wrapper (mysql_fwd) can be installed from the EnterpriseDB repo .","title":"Configuration"},{"location":"databases/PostgreSQL/#start-mindsdb","text":"To start mindsdb run following command: python3 - m mindsdb -- api = mysql -- config = config . json The --api parameter specifies the type of API to use in this case mysql. The --config specifies the location of the configuration file.","title":"Start MindsDB"},{"location":"databases/PostgreSQL/#train-new-model","text":"To train a new model, insert a new record inside the mindsdb.predictors table as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'us_consumption' , 'consumption' , 'SELECT * FROM us_consumption' , '{\"timeseries_settings\":{\"order_by\": [\"t\"], \"window\":20}}' ); name (string) -- The name of the predictor. predict (string) -- The feature you want to predict, in this example consumption. To predict multiple featurs include a comma separated string e.g 'consumption,income'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface . If you are using timeseries data check the Timeseries settings .","title":"Train new model"},{"location":"databases/PostgreSQL/#query-the-model","text":"To query the model and get the predictions SELECT the target variable, confidence and explanation for that prediction. SELECT consumption AS predicted , consumption_confidence AS confidence , consumption_explain AS info FROM mindsdb . us_consumption WHERE income = 1 . 182497938 AND production = 5 . 854555956 AND savings = 3 . 183292657 AND unemployment = 0 . 1 You should get a similar response from MindsDB as: consumption predicted info 1.4979682087292199 0.9475 Check JSON bellow { \"predicted_value\" : 1.4979682087292199 , \"confidence\" : 0.9475 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 1.025658879956537 , 1.9702775375019028 ], \"important_missing_information\" : [], \"confidence_composition\" : {}, \"extra_insights\" : { \"if_missing\" : [{ \"income\" : 0.6966906986877563 }, { \"production\" : 2.5382917051924445 }, { \"savings\" : 1.169812868271305 }, { \"unemployment\" : 1.3 443338862946717 }] } }","title":"Query the model"},{"location":"databases/PostgreSQL/#delete-the-model","text":"To delete the predictor that you have previously created, you need to delete it from mindsdb.predictors table. The name should be equal to name added in the INSERT statment while creating the predictor, e.g: DELETE FROM mindsdb . predictors WHERE name = 'us_consumption'","title":"Delete the model"},{"location":"databases/PostgreSQL/#train-and-predict-multiple-features","text":"You can train a model that will predict multiple features by adding a comma separated features values in the predict column. e.g to predict the consumption and a income : INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'us_consumption' , 'consumption, income' , 'SELECT * FROM us_consumption' , \"option,value\" } ); And query it using the select_data_query : SELECT consumption AS predicted , FROM mindsdb . used_cars_model WHERE select_data_query = 'SELECT income FROM us_consumption' ; The requirements to query with select_data_query are: It must be a valid SQL statement It must return columns with names the same as predictor fields. To get additional information follow the AiTables in PostgreSQL tutorail .","title":"Train and predict multiple features"},{"location":"databases/tutorials/AiTablesInMySQL/","text":"How to enable Automated Machine Learning in MySQL Database is surely the best place for Machine Learning - because data is the main ingredient of it. And now you can build, train, test & query Machine Learning models using standard SQL queries within MySQL database! This doesn't require hardcore data science knowledge - the whole Machine Learning workflow is automated. This solution is called AI-Tables and is available in MySQL thanks to integration with an open-source predictive engine from MindsDB. AI-Tables look like normal database tables and return predictions upon being queried as if it is data that exists in the table. In plain SQL it looks like this: SELECT < predicted_variable > FROM < ML_model > WHERE < conditions > This video explains how it works: In this tutorial below, you will get step-by-step instructions on how to enable AI-Tables in MySQL database. Based on the churn prediction example, you see how to build, train and query Machine Learning models only by using SQL statements with MindsDB! How to install MySQL? If you don\u2019t have MySQL installed you can download the installers for various platforms from the official documentation . Example dataset In this tutorial, we will use the Churn Modelling Data Set . If you have other datasets in your MySQL database please skip this section. This data set contains details of a bank's customers and the target variable is a binary variable reflecting the fact whether the customer left the bank (closed his account) or he continues to be a customer. Import dataset to MySQL The first thing we need to do is to import the dataset in MySQL. Create a new table called bank_churn: -- test.bank_churn definition CREATE TABLE test . bank_churn ( CreditScore NUMERIC NULL , Geography varchar ( 100 ) NULL , Gender varchar ( 100 ) NULL , Age NUMERIC NULL , Tenure NUMERIC NULL , Balance NUMERIC NULL , NumOfProducts NUMERIC NULL , HasCrCard NUMERIC NULL , IsActiveMember NUMERIC NULL , EstimatedSalary NUMERIC NULL , Exited NUMERIC NULL ) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4 COLLATE = utf8mb4_0900_ai_ci ; Next, we need to import the data inside the table. There are a few options to do that: Using the Load Data statement: LOAD DATA LOCAL INFILE 'data.csv' INTO bank_churn FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\r\\n'; Using MySQL import: mysqlimport --local --fields-terminated-by=\",\" bank_churn data.csv Using pgAdmin, DBeaver or another SQL client just use the import from CSV file option from the navigation menu. Let\u2019s Select some data from bank_churn table to check that the data was successfully imported to MySQL: SELECT * FROM bank_churn LIMIT 1 ; Add Configuration As a prerequisite for using MySQL we need to enable the Federated Storage engine. Check out the official MySQL documentation to see how you can do that. The last step is to create the MindsDB\u2019s configuration file. MindsDB will try to use the default configuration options like host, port or username for MySQL. In case you want to extend them or change the default values you need to add a config.json file. Create a new file config.json and include the following information: { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_mysql\" : { \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"root\" , \"port\" : 3307 , \"type\" : \"mysql\" , \"user\" : \"root\" } }, \"storage_dir\" : \"storage/\" } The values provided in the configuration file are: api['http\u2019] -- This key is used for starting the MindsDB HTTP server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that work through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations[default_mysql] -- This key specifies the integration type in this case default_mysql. The required keys are: user(default root) - The MySQL user name. host(default localhost) - Connect to the MySQL server on the given host. password - The password of the MySQL account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 5432) - The TCP/IP port number to use for the connection. storage_dir -- The directory where mindsdb will store models and configuration files. Now, we have successfully set up all of the requirements for AI Tables in MySQL. AutoML with AI Tables in MySQL If you don't have MindsDB installed, check out our Installation guide and find an option that works for you. After that start the MindsDB server: python3 -m mindsdb --api=mysql --config=config.json The arguments sent to MindsDB are: --api - That tells MindsDB which API should be started (HTTP or MySQL). --config - The path to the configuration file that we have created. If everything works as expected you should see the following message: Upon successful setup, MindsDB should create a new database called mindsdb. In the mindsdb database, two new tables should be created called commands and predictors. The mindsdb.predictors table is the table where MindsDB will keep information about trained models. Train new Machine Learning Model Training the machine learning model using MindsDB is quite simple. It can be done by executing the INSERT query inside the mindsdb.predictors table. In our example, we want to predict if the bank's customer left the bank from the bank_churn table, so let\u2019s run the INSERT query as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'churn_model' , 'Exited' , 'SELECT * FROM test.bank_churn' ); This query will create a new model called 'churn_model', and a new table 'churn_model' inside mindsdb database. The required columns(parameters) added in the INSERT for training the predictor are: * name (string) - the name of the predictor. * predict (string) - the feature you want to predict, in this example it will be Exited. * select_data_query (string) - the SELECT query that will get the data from MySQL. To check that the training successfully finished we can SELECT from mindsdb.predictors table and get the status: SELECT * FROM mindsdb . predictors WHERE name = 'churn_model' ; The status complete means that training successfully finished. Now, let\u2019s query the model. The trained model behaves like an AI Table and can be queried as it is a standard database table. To get the prediction we need to execute a SELECT query and in the WHERE clause include the when_data as a JSON string that includes features values such as CreditScore, EstimatedSalary, Gender, Balance etc. SELECT * FROM mindsdb . churn_model WHERE when_data = '{\"CreditScore\": \"619\",\"Geography\": \"France\",\"Gender\": \"Female\", \"EstimatedSalary\": 100000, \"Balance\": 0.0, \"Age\":42, \"Tenure\": 2}' ; In a second we should get the prediction back from MindsDB. So, MindsDB thinks that the above customer closed the account in the bank (predicted_value 1) with around 98% confidence. Information in JSON format in the explain column: { \"predicted_value\" : \"1.0\" , \"confidence\" : 0.98 , \"prediction_quality\" : \"very confident\" , \"important_missing_information\" : [ \"NumOfProducts\" ] } The important_missing_information shows the list of features that MindsDB things are quite important for better prediction, in this case, the \"NumOfProducts\". Congratulations, you have successfully trained and queried the Machine Learning Model only by using SQL Statements. Note that even if we used MySQL, you can still query the same model from the other databases too.","title":"AI Tables in MySQL"},{"location":"databases/tutorials/AiTablesInMySQL/#how-to-enable-automated-machine-learning-in-mysql","text":"Database is surely the best place for Machine Learning - because data is the main ingredient of it. And now you can build, train, test & query Machine Learning models using standard SQL queries within MySQL database! This doesn't require hardcore data science knowledge - the whole Machine Learning workflow is automated. This solution is called AI-Tables and is available in MySQL thanks to integration with an open-source predictive engine from MindsDB. AI-Tables look like normal database tables and return predictions upon being queried as if it is data that exists in the table. In plain SQL it looks like this: SELECT < predicted_variable > FROM < ML_model > WHERE < conditions > This video explains how it works: In this tutorial below, you will get step-by-step instructions on how to enable AI-Tables in MySQL database. Based on the churn prediction example, you see how to build, train and query Machine Learning models only by using SQL statements with MindsDB!","title":"How to enable Automated Machine Learning in MySQL"},{"location":"databases/tutorials/AiTablesInMySQL/#how-to-install-mysql","text":"If you don\u2019t have MySQL installed you can download the installers for various platforms from the official documentation .","title":"How to install MySQL?"},{"location":"databases/tutorials/AiTablesInMySQL/#example-dataset","text":"In this tutorial, we will use the Churn Modelling Data Set . If you have other datasets in your MySQL database please skip this section. This data set contains details of a bank's customers and the target variable is a binary variable reflecting the fact whether the customer left the bank (closed his account) or he continues to be a customer.","title":"Example dataset"},{"location":"databases/tutorials/AiTablesInMySQL/#import-dataset-to-mysql","text":"The first thing we need to do is to import the dataset in MySQL. Create a new table called bank_churn: -- test.bank_churn definition CREATE TABLE test . bank_churn ( CreditScore NUMERIC NULL , Geography varchar ( 100 ) NULL , Gender varchar ( 100 ) NULL , Age NUMERIC NULL , Tenure NUMERIC NULL , Balance NUMERIC NULL , NumOfProducts NUMERIC NULL , HasCrCard NUMERIC NULL , IsActiveMember NUMERIC NULL , EstimatedSalary NUMERIC NULL , Exited NUMERIC NULL ) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4 COLLATE = utf8mb4_0900_ai_ci ; Next, we need to import the data inside the table. There are a few options to do that: Using the Load Data statement: LOAD DATA LOCAL INFILE 'data.csv' INTO bank_churn FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\r\\n'; Using MySQL import: mysqlimport --local --fields-terminated-by=\",\" bank_churn data.csv Using pgAdmin, DBeaver or another SQL client just use the import from CSV file option from the navigation menu. Let\u2019s Select some data from bank_churn table to check that the data was successfully imported to MySQL: SELECT * FROM bank_churn LIMIT 1 ;","title":"Import dataset to MySQL"},{"location":"databases/tutorials/AiTablesInMySQL/#add-configuration","text":"As a prerequisite for using MySQL we need to enable the Federated Storage engine. Check out the official MySQL documentation to see how you can do that. The last step is to create the MindsDB\u2019s configuration file. MindsDB will try to use the default configuration options like host, port or username for MySQL. In case you want to extend them or change the default values you need to add a config.json file. Create a new file config.json and include the following information: { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_mysql\" : { \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"root\" , \"port\" : 3307 , \"type\" : \"mysql\" , \"user\" : \"root\" } }, \"storage_dir\" : \"storage/\" } The values provided in the configuration file are: api['http\u2019] -- This key is used for starting the MindsDB HTTP server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that work through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations[default_mysql] -- This key specifies the integration type in this case default_mysql. The required keys are: user(default root) - The MySQL user name. host(default localhost) - Connect to the MySQL server on the given host. password - The password of the MySQL account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 5432) - The TCP/IP port number to use for the connection. storage_dir -- The directory where mindsdb will store models and configuration files. Now, we have successfully set up all of the requirements for AI Tables in MySQL.","title":"Add Configuration"},{"location":"databases/tutorials/AiTablesInMySQL/#automl-with-ai-tables-in-mysql","text":"If you don't have MindsDB installed, check out our Installation guide and find an option that works for you. After that start the MindsDB server: python3 -m mindsdb --api=mysql --config=config.json The arguments sent to MindsDB are: --api - That tells MindsDB which API should be started (HTTP or MySQL). --config - The path to the configuration file that we have created. If everything works as expected you should see the following message: Upon successful setup, MindsDB should create a new database called mindsdb. In the mindsdb database, two new tables should be created called commands and predictors. The mindsdb.predictors table is the table where MindsDB will keep information about trained models.","title":"AutoML with AI Tables in MySQL"},{"location":"databases/tutorials/AiTablesInMySQL/#train-new-machine-learning-model","text":"Training the machine learning model using MindsDB is quite simple. It can be done by executing the INSERT query inside the mindsdb.predictors table. In our example, we want to predict if the bank's customer left the bank from the bank_churn table, so let\u2019s run the INSERT query as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'churn_model' , 'Exited' , 'SELECT * FROM test.bank_churn' ); This query will create a new model called 'churn_model', and a new table 'churn_model' inside mindsdb database. The required columns(parameters) added in the INSERT for training the predictor are: * name (string) - the name of the predictor. * predict (string) - the feature you want to predict, in this example it will be Exited. * select_data_query (string) - the SELECT query that will get the data from MySQL. To check that the training successfully finished we can SELECT from mindsdb.predictors table and get the status: SELECT * FROM mindsdb . predictors WHERE name = 'churn_model' ; The status complete means that training successfully finished. Now, let\u2019s query the model. The trained model behaves like an AI Table and can be queried as it is a standard database table. To get the prediction we need to execute a SELECT query and in the WHERE clause include the when_data as a JSON string that includes features values such as CreditScore, EstimatedSalary, Gender, Balance etc. SELECT * FROM mindsdb . churn_model WHERE when_data = '{\"CreditScore\": \"619\",\"Geography\": \"France\",\"Gender\": \"Female\", \"EstimatedSalary\": 100000, \"Balance\": 0.0, \"Age\":42, \"Tenure\": 2}' ; In a second we should get the prediction back from MindsDB. So, MindsDB thinks that the above customer closed the account in the bank (predicted_value 1) with around 98% confidence. Information in JSON format in the explain column: { \"predicted_value\" : \"1.0\" , \"confidence\" : 0.98 , \"prediction_quality\" : \"very confident\" , \"important_missing_information\" : [ \"NumOfProducts\" ] } The important_missing_information shows the list of features that MindsDB things are quite important for better prediction, in this case, the \"NumOfProducts\". Congratulations, you have successfully trained and queried the Machine Learning Model only by using SQL Statements. Note that even if we used MySQL, you can still query the same model from the other databases too.","title":"Train new Machine Learning Model"},{"location":"databases/tutorials/AiTablesInPostgreSQL/","text":"AI-Tables in PostgreSQL - get neural-network-based predictions using simple SQL queries Anyone that has dealt with Machine Learning understands that data is a fundamental ingredient to it. Given that a great deal of the world\u2019s organized data already exists inside databases, doesn't it make sense to bring machine learning capabilities straight to the database itself? Bringing Machine Learning to those who know their data best can significantly augment the capacity to solve important problems. To do so, we have developed a concept called AI-Tables. What is AI Tables AI-Tables differ from normal tables in that they can generate predictions upon being queried and returning such predictions as if it was data that existed in the table. Simply put, an AI-Table allows you to use machine learning models as if they were normal database tables, in something that in plain SQL looks like this: SELECT < predicted_variable > FROM < ML_model > WHERE < conditions > Now, in this tutorial, you will get step-by-step instructions on how to enable AI-Tables in your database and how to build, train and query a Machine Learning model only by using SQL statements! How to install PostgreSQL? If you don\u2019t have PostgreSQL installed you can download the installers for various platforms from the official documentation. Example dataset In this tutorial, we will use the Airline Passenger Satisfaction dataset . If you have other datasets in your PostgreSQL database please skip this section. This dataset contains airline passenger satisfaction survey data and we will try to predict passenger satisfaction based on the other factors in the data. Import dataset to PostgreSQL First, let's create a us_consumption table. -- public.airline_passenger_satisfaction definition CREATE TABLE public . airline_passenger_satisfaction ( id numeric NULL , gender varchar NULL , \"Customer Type\" varchar NULL , age numeric NULL , \"Type of Travel\" varchar NULL , \"Class\" varchar NULL , \"Flight Distance\" numeric NULL , \"Inflight wifi service\" numeric NULL , \"Departure/Arrival time convenient\" numeric NULL , \"Ease of Online booking\" numeric NULL , \"Gate location\" numeric NULL , \"Food and drink\" numeric NULL , \"Online boarding\" numeric NULL , \"Seat comfort\" numeric NULL , \"Inflight entertainment\" numeric NULL , \"On-board service\" numeric NULL , \"Leg room service\" numeric NULL , \"Baggage handling\" numeric NULL , \"Checkin service\" numeric NULL , \"Inflight service\" numeric NULL , cleanliness numeric NULL , \"Departure Delay in Minutes\" numeric NULL , \"Arrival Delay in Minutes\" numeric NULL , satisfaction varchar NULL ); After you create the table, you can use the \\copy command to import the data from the CSV file to PostgreSQL: \\copy usconsumption FROM '/path/to/csv/usconsumption.csv' DELIMITER ',' CSV Or, if you are using pgAdmin, DBeaver or another SQL client just use the import from CSV file option from the navigation menu. To check if the data was successfully imported execute SELECT query: SELECT * FROM airline_passenger_satisfaction LIMIT 10 ; Add Configuration We have the data inside PostgreSQL, so the next step is to install PostgreSQL foreign data wrapper for MySQL. Please check the EnterpriseDB documentation on how to do that. The last step is to create the MindsDB\u2019s configuration file. MindsDB will try to use the default configuration options like host, port, username for the PostgreSQL integration. In case you want to extend them or change the default values you need to add a config.json file. Create a new file config.json and include the following information: { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_postgres\" : { \"database\" : \"postgres\" , \"enabled\" : true , \"host\" : \"127.0.0.1\" , \"password\" : \"postgres\" , \"port\" : 5432 , \"type\" : \"postgres\" , \"user\" : \"postgres\" } }, \"storage_dir\" : \"storage/\" } The values provided in the configuration file are: api['http\u2019] -- This key is used for starting the MindsDB HTTP server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that work through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_postgres'] -- This key specifies the integration type in this case default_postgres. The required keys are: user(default postgres) - The Postgres user name. host(default 127.0.0.1) - Connect to the PostgreSQL server on the given host. password - The password of the Postgres account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 5432) - The TCP/IP port number to use for the connection. storage_dir -- The directory where mindsdb will store models and configuration. That\u2019s all for setting up the AI Tables in PostgreSQL. AutoML with AI Tables in PostgreSQL If you don't have MindsDB installed, check out our Installation guide and find an option that works for you. After that start the MindsDB server: python3 -m mindsdb --api=http,mysql --config=config.json The arguments sent to MindsDB are: --api - That tells MindsDB which API should be started (HTTP, MySQL or both). --config - The path to the configuration file that we have created. If everything works as expected you should see the following message: Upon successful setup, MindsDB should create a new schema called mindsdb. In the mindsdb schema, two new tables should be created called commands and predictors. The mindsdb.predictors table is the table where MindsDB will keep information about trained and in training models. Train new Machine Learning Model Training the machine learning model using MindsDB is quite simple. It can be done by executing the INSERT query inside the mindsdb.predictors table. In our example we want to predict the consumption from the us_consumption table, so let\u2019s run the INSERT query as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'passenger_satisfaction_model' , 'satisfaction' , 'SELECT * FROM airline_passenger_satisfaction' ); This query will create a new model called 'passenger_satisfaction_model', and a new table 'airline_passenger_satisfaction' inside mindsdb schema. The required columns(parameters) added in the INSERT for training the predictor are: name (string) - the name of the predictor. predict (string) - the feature you want to predict, in this example it will be satisfaction. select_data_query (string) - the SELECT query that will get the data from PostgreSQL. training_options (dictionary) - optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface . To check that the training successfully finished we can SELECT from mindsdb.predictors table and get the status: SELECT * FROM mindsdb . predictors WHERE name = 'passenger_satisfaction_model' ; The status complete means that training successfully finished. Now, let\u2019s query the model. The trained model behaves like an AI Table and can be queried as it is a standard database table. To get the prediction we need to execute SELECT query and in the WHERE clause include the other features values as Customer Type, Type of Travel, Seat comfort etc. SELECT satisfaction AS predicted , satisfaction_confidence AS confidence FROM mindsdb . passenger_satisfaction_model WHERE \"Customer Type\" = 'Loyal Customer' AND \"Inflight wifi service\" = 5 AND \"Type of Travel\" = 'Business travel' AND \"Class\" = 'Eco' ; In a second we should get the prediction back from MindsDB. So, MindsDB thinks that the value for consumption rate is around 0.87 with pretty much big confidence 97%. There is additional information that we can get back from MindsDB by selecting the explain column from the model as: SELECT satisfaction AS predicted , satisfaction_confidence AS confidence , satisfaction_explain AS info FROM mindsdb . passenger_satisfaction_model WHERE \"Customer Type\" = 'Loyal Customer' AND age = 52 AND \"Type of Travel\" = 'Business travel' AND \"Class\" = 'Eco' ; Now, apart from the predicted and confidence values, MindsDB will return additional Information in JSON format in the explain column: { \"predicted_value\" : \"satisfied\" , \"confidence\" : 0.94 , \"prediction_quality\" : \"very confident\" , \"important_missing_information\" : [ \"Flight Distance\" , \"Inflight wifi service\" , \"Online boarding\" , \"Baggage handling\" ], \"confidence_composition\" : { \"age\" : 0.935 }, \"extra_insights\" : { \"if_missing\" : [{ \"Customer Type\" : \"satisfied\" }, { \"Type of Travel\" : \"satisfied\" }, { \"Class\" : \"satisfied\" }] } } The confidence_interva l shows a possible range of values where consumption lies in. The important_missing_information shows the list of features that MindsDB things are quite important for better prediction, in this case, the \u201cFlight Distance\u201d, \"Inflight wifi service\", \"Online boarding\" or \"Baggage handling\". The extra_insights shows a list of rows that we have included in the WHERE clause and show the consumption value if some of those were missing. Congratulations, you have successfully trained and queried the Machine Learning Model only by using SQL Statements. Note that even if we used PostgreSQL, you can still query the same model from the other databases too.","title":"AI Tables in PostgreSQL"},{"location":"databases/tutorials/AiTablesInPostgreSQL/#ai-tables-in-postgresql-get-neural-network-based-predictions-using-simple-sql-queries","text":"Anyone that has dealt with Machine Learning understands that data is a fundamental ingredient to it. Given that a great deal of the world\u2019s organized data already exists inside databases, doesn't it make sense to bring machine learning capabilities straight to the database itself? Bringing Machine Learning to those who know their data best can significantly augment the capacity to solve important problems. To do so, we have developed a concept called AI-Tables.","title":"AI-Tables in PostgreSQL - get neural-network-based predictions using simple SQL queries"},{"location":"databases/tutorials/AiTablesInPostgreSQL/#what-is-ai-tables","text":"AI-Tables differ from normal tables in that they can generate predictions upon being queried and returning such predictions as if it was data that existed in the table. Simply put, an AI-Table allows you to use machine learning models as if they were normal database tables, in something that in plain SQL looks like this: SELECT < predicted_variable > FROM < ML_model > WHERE < conditions > Now, in this tutorial, you will get step-by-step instructions on how to enable AI-Tables in your database and how to build, train and query a Machine Learning model only by using SQL statements!","title":"What is AI Tables"},{"location":"databases/tutorials/AiTablesInPostgreSQL/#how-to-install-postgresql","text":"If you don\u2019t have PostgreSQL installed you can download the installers for various platforms from the official documentation.","title":"How to install PostgreSQL?"},{"location":"databases/tutorials/AiTablesInPostgreSQL/#example-dataset","text":"In this tutorial, we will use the Airline Passenger Satisfaction dataset . If you have other datasets in your PostgreSQL database please skip this section. This dataset contains airline passenger satisfaction survey data and we will try to predict passenger satisfaction based on the other factors in the data.","title":"Example dataset"},{"location":"databases/tutorials/AiTablesInPostgreSQL/#import-dataset-to-postgresql","text":"First, let's create a us_consumption table. -- public.airline_passenger_satisfaction definition CREATE TABLE public . airline_passenger_satisfaction ( id numeric NULL , gender varchar NULL , \"Customer Type\" varchar NULL , age numeric NULL , \"Type of Travel\" varchar NULL , \"Class\" varchar NULL , \"Flight Distance\" numeric NULL , \"Inflight wifi service\" numeric NULL , \"Departure/Arrival time convenient\" numeric NULL , \"Ease of Online booking\" numeric NULL , \"Gate location\" numeric NULL , \"Food and drink\" numeric NULL , \"Online boarding\" numeric NULL , \"Seat comfort\" numeric NULL , \"Inflight entertainment\" numeric NULL , \"On-board service\" numeric NULL , \"Leg room service\" numeric NULL , \"Baggage handling\" numeric NULL , \"Checkin service\" numeric NULL , \"Inflight service\" numeric NULL , cleanliness numeric NULL , \"Departure Delay in Minutes\" numeric NULL , \"Arrival Delay in Minutes\" numeric NULL , satisfaction varchar NULL ); After you create the table, you can use the \\copy command to import the data from the CSV file to PostgreSQL: \\copy usconsumption FROM '/path/to/csv/usconsumption.csv' DELIMITER ',' CSV Or, if you are using pgAdmin, DBeaver or another SQL client just use the import from CSV file option from the navigation menu. To check if the data was successfully imported execute SELECT query: SELECT * FROM airline_passenger_satisfaction LIMIT 10 ;","title":"Import dataset to PostgreSQL"},{"location":"databases/tutorials/AiTablesInPostgreSQL/#add-configuration","text":"We have the data inside PostgreSQL, so the next step is to install PostgreSQL foreign data wrapper for MySQL. Please check the EnterpriseDB documentation on how to do that. The last step is to create the MindsDB\u2019s configuration file. MindsDB will try to use the default configuration options like host, port, username for the PostgreSQL integration. In case you want to extend them or change the default values you need to add a config.json file. Create a new file config.json and include the following information: { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_postgres\" : { \"database\" : \"postgres\" , \"enabled\" : true , \"host\" : \"127.0.0.1\" , \"password\" : \"postgres\" , \"port\" : 5432 , \"type\" : \"postgres\" , \"user\" : \"postgres\" } }, \"storage_dir\" : \"storage/\" } The values provided in the configuration file are: api['http\u2019] -- This key is used for starting the MindsDB HTTP server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that work through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_postgres'] -- This key specifies the integration type in this case default_postgres. The required keys are: user(default postgres) - The Postgres user name. host(default 127.0.0.1) - Connect to the PostgreSQL server on the given host. password - The password of the Postgres account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 5432) - The TCP/IP port number to use for the connection. storage_dir -- The directory where mindsdb will store models and configuration. That\u2019s all for setting up the AI Tables in PostgreSQL.","title":"Add Configuration"},{"location":"databases/tutorials/AiTablesInPostgreSQL/#automl-with-ai-tables-in-postgresql","text":"If you don't have MindsDB installed, check out our Installation guide and find an option that works for you. After that start the MindsDB server: python3 -m mindsdb --api=http,mysql --config=config.json The arguments sent to MindsDB are: --api - That tells MindsDB which API should be started (HTTP, MySQL or both). --config - The path to the configuration file that we have created. If everything works as expected you should see the following message: Upon successful setup, MindsDB should create a new schema called mindsdb. In the mindsdb schema, two new tables should be created called commands and predictors. The mindsdb.predictors table is the table where MindsDB will keep information about trained and in training models.","title":"AutoML with AI Tables in PostgreSQL"},{"location":"databases/tutorials/AiTablesInPostgreSQL/#train-new-machine-learning-model","text":"Training the machine learning model using MindsDB is quite simple. It can be done by executing the INSERT query inside the mindsdb.predictors table. In our example we want to predict the consumption from the us_consumption table, so let\u2019s run the INSERT query as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'passenger_satisfaction_model' , 'satisfaction' , 'SELECT * FROM airline_passenger_satisfaction' ); This query will create a new model called 'passenger_satisfaction_model', and a new table 'airline_passenger_satisfaction' inside mindsdb schema. The required columns(parameters) added in the INSERT for training the predictor are: name (string) - the name of the predictor. predict (string) - the feature you want to predict, in this example it will be satisfaction. select_data_query (string) - the SELECT query that will get the data from PostgreSQL. training_options (dictionary) - optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface . To check that the training successfully finished we can SELECT from mindsdb.predictors table and get the status: SELECT * FROM mindsdb . predictors WHERE name = 'passenger_satisfaction_model' ; The status complete means that training successfully finished. Now, let\u2019s query the model. The trained model behaves like an AI Table and can be queried as it is a standard database table. To get the prediction we need to execute SELECT query and in the WHERE clause include the other features values as Customer Type, Type of Travel, Seat comfort etc. SELECT satisfaction AS predicted , satisfaction_confidence AS confidence FROM mindsdb . passenger_satisfaction_model WHERE \"Customer Type\" = 'Loyal Customer' AND \"Inflight wifi service\" = 5 AND \"Type of Travel\" = 'Business travel' AND \"Class\" = 'Eco' ; In a second we should get the prediction back from MindsDB. So, MindsDB thinks that the value for consumption rate is around 0.87 with pretty much big confidence 97%. There is additional information that we can get back from MindsDB by selecting the explain column from the model as: SELECT satisfaction AS predicted , satisfaction_confidence AS confidence , satisfaction_explain AS info FROM mindsdb . passenger_satisfaction_model WHERE \"Customer Type\" = 'Loyal Customer' AND age = 52 AND \"Type of Travel\" = 'Business travel' AND \"Class\" = 'Eco' ; Now, apart from the predicted and confidence values, MindsDB will return additional Information in JSON format in the explain column: { \"predicted_value\" : \"satisfied\" , \"confidence\" : 0.94 , \"prediction_quality\" : \"very confident\" , \"important_missing_information\" : [ \"Flight Distance\" , \"Inflight wifi service\" , \"Online boarding\" , \"Baggage handling\" ], \"confidence_composition\" : { \"age\" : 0.935 }, \"extra_insights\" : { \"if_missing\" : [{ \"Customer Type\" : \"satisfied\" }, { \"Type of Travel\" : \"satisfied\" }, { \"Class\" : \"satisfied\" }] } } The confidence_interva l shows a possible range of values where consumption lies in. The important_missing_information shows the list of features that MindsDB things are quite important for better prediction, in this case, the \u201cFlight Distance\u201d, \"Inflight wifi service\", \"Online boarding\" or \"Baggage handling\". The extra_insights shows a list of rows that we have included in the WHERE clause and show the consumption value if some of those were missing. Congratulations, you have successfully trained and queried the Machine Learning Model only by using SQL Statements. Note that even if we used PostgreSQL, you can still query the same model from the other databases too.","title":"Train new Machine Learning Model"},{"location":"features/ColumnMetrics/","text":"@TODO: Outdated, pending refacotr","title":"ColumnMetrics"},{"location":"features/DataSources/","text":"Our goal is to make it very simple to ingest and prepare data that can be feed into MindsDB as DataSources . Here are the basics: mindsdb.learn from_data argument can be any of the following: file : can be a path in the local system, stringio object or a url to a file, supported types are (csv, json, xlsx, xls). data frame : A pandas DataFrame , pandas is one of the most useful data preparation libraries out there, so it makes sense that we support this. MindsDB data source : MindsDB has a special class for datasources which lends itself for simple data ingestion and preparation, as well as to combine various datasources to learn from. Note: By default, only the FileDS data source is available, to make sure the other data sources work, install mindsdb via pip install mindsdb[extra_data_sources] , or replace mindsdb to mindsdb[extra_data_sources] in whatever install instructions are given for your platform MindsDB data source: MindsDB datasource is an enriched version of a pandas dataframe so all methods in the pandas dataframe also apply to MindsDB, However, the DataSource class provides a way to implement data loading transformations and cleaning of data. Some special implementations of datasources that already do cleaning and various tasks: You can learn about them in mindsdb repository . FileDS An important one is the one MindsDB uses to work with files. from mindsdb import FileDS ds = FileDS ( file , clean_rows = True , custom_parser = None ) # Now you can pass this DataSource to MindsDB mdb . learn ( from_data = ds , predict = '<what column you want to predict>' , # the column we want to learn to predict given all the data in the file model_name = '<the name you want to give to this model>' # the name of this model ) FileDS arguments are: file can be a path in the local system, stringio object or a url to a file, supported types are (csv, json, xlsx, xls). clean_header (default=True) clean column names, so that they don't have special characters or white spaces. clean_rows (default=True) Goes row by row making sure that nulls are nulls and that no corrupt data exists in them, it also cleans empty rows custom_parser (default=None) special function to extract the actual table structure from the file in case you need special transformations. S3DS Use an s3 object as the input. from mindsdb import Predictor , S3DS s3_ds = S3DS ( bucket_name = 'mindsdb-example-data' , file_path = 'home_rentals.csv' ) ` Predictor ( name = 'test' ) . learn ( from_data = s3_ds , to_predict = 'target' ) This data source also takes the optional initialization/constructor arguments: access_key -- The AWS access key [string] secret_key -- The AWS secret key [string] use_default_credentails -- Whether or not to use the default credentials on your machine (e.g. the credentials inside ~/.aws or the role of the machine), defaults to False [boolean] MySqlDS Used to select data from MySQL or MariaDB . from mindsdb import Predictor , MySqlDS mysql_ds = MySqlDS ( query = \"SELECT COUNT(*), SUM(spend), SUM(is_click), website FROM advertising_data\" , user = \"my_user\" , password = \"my very secret password\" , database = \"main_db\" , table = 'advertising_data' , port = 3306 ) Predictor ( name = 'test' ) . learn ( from_data = mysql_ds , to_predict = 'target' ) This data source also takes the optional initialization/constructor arguments: query -- Query which extracts the data, mutually exclusive with table [string] host -- The host of the database (e.g. localhost or some ip) [string] user -- User for the database [string] password -- Password for the user [string] database -- Database to use [string] port -- Port of the database [integer] table -- Table from which to select all the data, mutually exclusive with query [string] PostgresDS Used to select data from PostgreSQL . from mindsdb import Predictor , PostgresDS pg_ds = PostgresDS ( query = \"SELECT COUNT(*), SUM(spend), SUM(is_click), website FROM advertising_data\" , user = \"my_user\" , password = \"my very secret password\" , database = \"main_db\" ) Predictor ( name = 'test' ) . learn ( from_data = pg_ds , to_predict = 'target' ) This data source also takes the optional initialization/constructor arguments: query -- Query which extracts the data, mutually exclusive with table [string] host -- The host of the database (e.g. localhost or some ip) [string] user -- User for the database [string] password -- Password for the user [string] database -- Database to use [string] port -- Port of the database [integer] table -- Table from which to select all the data, mutually exclusive with query [string] ClickhouseDS Used to select data from ClickHouse . from mindsdb import Predictor , ClickhouseDS ch_ds = ClickhouseDS ( query = \"SELECT COUNT(*), SUM(spend), SUM(is_click), website FROM default.advertising_data\" , user = \"my_user\" , password = \"my very secret password\" ) Predictor ( name = 'test' ) . learn ( from_data = ch_ds , to_predict = 'target' ) This data source also take the optional initialization/constructor arguments: query -- Query whith which to extract the data, mutually exclusive with table [string] host -- The host of the database (e.g. localhost or some ip) [string] user -- User for the database [string] password -- Password for the user [string] port -- Port of the database [integer] table -- Table from which to select all the data, mutually exclusive with query [string]","title":"Data sources"},{"location":"features/DataSources/#mindsdb-data-source","text":"MindsDB datasource is an enriched version of a pandas dataframe so all methods in the pandas dataframe also apply to MindsDB, However, the DataSource class provides a way to implement data loading transformations and cleaning of data. Some special implementations of datasources that already do cleaning and various tasks: You can learn about them in mindsdb repository .","title":"MindsDB data source:"},{"location":"features/DataSources/#fileds","text":"An important one is the one MindsDB uses to work with files. from mindsdb import FileDS ds = FileDS ( file , clean_rows = True , custom_parser = None ) # Now you can pass this DataSource to MindsDB mdb . learn ( from_data = ds , predict = '<what column you want to predict>' , # the column we want to learn to predict given all the data in the file model_name = '<the name you want to give to this model>' # the name of this model ) FileDS arguments are: file can be a path in the local system, stringio object or a url to a file, supported types are (csv, json, xlsx, xls). clean_header (default=True) clean column names, so that they don't have special characters or white spaces. clean_rows (default=True) Goes row by row making sure that nulls are nulls and that no corrupt data exists in them, it also cleans empty rows custom_parser (default=None) special function to extract the actual table structure from the file in case you need special transformations.","title":"FileDS"},{"location":"features/DataSources/#s3ds","text":"Use an s3 object as the input. from mindsdb import Predictor , S3DS s3_ds = S3DS ( bucket_name = 'mindsdb-example-data' , file_path = 'home_rentals.csv' ) ` Predictor ( name = 'test' ) . learn ( from_data = s3_ds , to_predict = 'target' ) This data source also takes the optional initialization/constructor arguments: access_key -- The AWS access key [string] secret_key -- The AWS secret key [string] use_default_credentails -- Whether or not to use the default credentials on your machine (e.g. the credentials inside ~/.aws or the role of the machine), defaults to False [boolean]","title":"S3DS"},{"location":"features/DataSources/#mysqlds","text":"Used to select data from MySQL or MariaDB . from mindsdb import Predictor , MySqlDS mysql_ds = MySqlDS ( query = \"SELECT COUNT(*), SUM(spend), SUM(is_click), website FROM advertising_data\" , user = \"my_user\" , password = \"my very secret password\" , database = \"main_db\" , table = 'advertising_data' , port = 3306 ) Predictor ( name = 'test' ) . learn ( from_data = mysql_ds , to_predict = 'target' ) This data source also takes the optional initialization/constructor arguments: query -- Query which extracts the data, mutually exclusive with table [string] host -- The host of the database (e.g. localhost or some ip) [string] user -- User for the database [string] password -- Password for the user [string] database -- Database to use [string] port -- Port of the database [integer] table -- Table from which to select all the data, mutually exclusive with query [string]","title":"MySqlDS"},{"location":"features/DataSources/#postgresds","text":"Used to select data from PostgreSQL . from mindsdb import Predictor , PostgresDS pg_ds = PostgresDS ( query = \"SELECT COUNT(*), SUM(spend), SUM(is_click), website FROM advertising_data\" , user = \"my_user\" , password = \"my very secret password\" , database = \"main_db\" ) Predictor ( name = 'test' ) . learn ( from_data = pg_ds , to_predict = 'target' ) This data source also takes the optional initialization/constructor arguments: query -- Query which extracts the data, mutually exclusive with table [string] host -- The host of the database (e.g. localhost or some ip) [string] user -- User for the database [string] password -- Password for the user [string] database -- Database to use [string] port -- Port of the database [integer] table -- Table from which to select all the data, mutually exclusive with query [string]","title":"PostgresDS"},{"location":"features/DataSources/#clickhouseds","text":"Used to select data from ClickHouse . from mindsdb import Predictor , ClickhouseDS ch_ds = ClickhouseDS ( query = \"SELECT COUNT(*), SUM(spend), SUM(is_click), website FROM default.advertising_data\" , user = \"my_user\" , password = \"my very secret password\" ) Predictor ( name = 'test' ) . learn ( from_data = ch_ds , to_predict = 'target' ) This data source also take the optional initialization/constructor arguments: query -- Query whith which to extract the data, mutually exclusive with table [string] host -- The host of the database (e.g. localhost or some ip) [string] user -- User for the database [string] password -- Password for the user [string] port -- Port of the database [integer] table -- Table from which to select all the data, mutually exclusive with query [string]","title":"ClickhouseDS"},{"location":"installation/Linux/","text":"Install using MindsDB installers Install MindsDB on your Linux machine using an easy-to-use shell script. Download the script MindsDB for Linux This script will install MindsDB and MindsDB's dependencies, and start the MindsDB server. Note that you need Python 3.6.x, 3.7.x or 3.8.x installed. Install using pip We suggest you to install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python version is >=3.6 . Create and activate venv: python -m venv mindsdb source mindsdb/bin/activate Install MindsDB: pip install mindsdb To verify that mindsdb was installed run: pip freeze Install using Anaconda You will need Anaconda or Conda installed and Python 64bit version. Then open Anaconda Prompt and: Create new virtual environment and install mindsdb: conda create -n mindsdb conda activate mindsdb pip install mindsdb To verify that mindsdb was installed run: conda list Installation fail Don't worry, simply follow the below bellow instruction which should fix most issues. Python 64 bit version is required. If you are using Python 3.9 you may get installation errors. Some of the MindsDB's dependencies are not working with Python 3.9 , so please downgrade to older versions for now. We are working on this and Python 3.9 will be supported soon. If you are using Linux install tkinter from your package manager in certain situations. Ubuntu/Debian: sudo apt-get install python3-tk tk Fedora: sudo dnf -y install python3-tkinter Arch: sudo pacman -S tk If none of this works, try installing mindsdb using the docker container and create an issue with the installation errors you got on our Github repository and we'll try to review it within a few hours.","title":"Install on Linux"},{"location":"installation/Linux/#install-using-mindsdb-installers","text":"Install MindsDB on your Linux machine using an easy-to-use shell script. Download the script MindsDB for Linux This script will install MindsDB and MindsDB's dependencies, and start the MindsDB server. Note that you need Python 3.6.x, 3.7.x or 3.8.x installed.","title":"Install using MindsDB installers"},{"location":"installation/Linux/#install-using-pip","text":"We suggest you to install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python version is >=3.6 . Create and activate venv: python -m venv mindsdb source mindsdb/bin/activate Install MindsDB: pip install mindsdb To verify that mindsdb was installed run: pip freeze","title":"Install using pip"},{"location":"installation/Linux/#install-using-anaconda","text":"You will need Anaconda or Conda installed and Python 64bit version. Then open Anaconda Prompt and: Create new virtual environment and install mindsdb: conda create -n mindsdb conda activate mindsdb pip install mindsdb To verify that mindsdb was installed run: conda list Installation fail Don't worry, simply follow the below bellow instruction which should fix most issues. Python 64 bit version is required. If you are using Python 3.9 you may get installation errors. Some of the MindsDB's dependencies are not working with Python 3.9 , so please downgrade to older versions for now. We are working on this and Python 3.9 will be supported soon. If you are using Linux install tkinter from your package manager in certain situations. Ubuntu/Debian: sudo apt-get install python3-tk tk Fedora: sudo dnf -y install python3-tkinter Arch: sudo pacman -S tk If none of this works, try installing mindsdb using the docker container and create an issue with the installation errors you got on our Github repository and we'll try to review it within a few hours.","title":"Install using Anaconda"},{"location":"installation/MacOS/","text":"Install using MindsDB installers Install MindsDB on your MacOS machine using an easy-to-use shell script. Download the script MindsDB for MacOS This script will install MindsDB and MindsDB's dependencies, and start the MindsDB server. Note that you need Python 3.6.x, 3.7.x or 3.8.x installed. Install using Anaconda You will need Anaconda or Conda installed and Python 64bit version. Then open Anaconda Prompt and: Create new virtual environment and install mindsdb: conda create -n mindsdb conda activate mindsdb pip install mindsdb To verify that mindsdb was installed run: conda list Install using pip We suggest you to install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python version is >=3.6 . Create and activate venv: python -m venv mindsdb source mindsdb/bin/activate Install MindsDB: pip install mindsdb To verify that mindsdb was installed run: pip freeze Installation fail Don't worry, simply follow the below bellow instruction which should fix most issues. If you got numpy.distutils.system_info.NotFoundError: No lapack/blas resources found. Note: Accelerate is no longer supported. error when installing on macOS and you are using Python 3.9 please, downgrade to older versions for now. We are working on this and Python 3.9 will be supported soon. Python 64 bit version is required. If you are using macOS and got error about system dependencies, try installing MindsDB with Anaconda and run the installation from the anaconda prompt . If none of this works, try installing mindsdb using the docker container and create an issue with the installation errors you got on our Github repository and we'll try to review it within a few hours.","title":"Install on macOS"},{"location":"installation/MacOS/#install-using-mindsdb-installers","text":"Install MindsDB on your MacOS machine using an easy-to-use shell script. Download the script MindsDB for MacOS This script will install MindsDB and MindsDB's dependencies, and start the MindsDB server. Note that you need Python 3.6.x, 3.7.x or 3.8.x installed.","title":"Install using MindsDB installers"},{"location":"installation/MacOS/#install-using-anaconda","text":"You will need Anaconda or Conda installed and Python 64bit version. Then open Anaconda Prompt and: Create new virtual environment and install mindsdb: conda create -n mindsdb conda activate mindsdb pip install mindsdb To verify that mindsdb was installed run: conda list","title":"Install using Anaconda"},{"location":"installation/MacOS/#install-using-pip","text":"We suggest you to install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python version is >=3.6 . Create and activate venv: python -m venv mindsdb source mindsdb/bin/activate Install MindsDB: pip install mindsdb To verify that mindsdb was installed run: pip freeze Installation fail Don't worry, simply follow the below bellow instruction which should fix most issues. If you got numpy.distutils.system_info.NotFoundError: No lapack/blas resources found. Note: Accelerate is no longer supported. error when installing on macOS and you are using Python 3.9 please, downgrade to older versions for now. We are working on this and Python 3.9 will be supported soon. Python 64 bit version is required. If you are using macOS and got error about system dependencies, try installing MindsDB with Anaconda and run the installation from the anaconda prompt . If none of this works, try installing mindsdb using the docker container and create an issue with the installation errors you got on our Github repository and we'll try to review it within a few hours.","title":"Install using pip"},{"location":"installation/Windows/","text":"Install using MindsDB installers Install MindsDB on your Windows machine using an easy-to-use installler. Download the installer MindsDB for Windows This installer will install Python3, MindsDB and MindsDB's dependencies, and create a shortcut on the desktop for starting MindsDB server. Install using Anaconda You will need Anaconda or Conda installed and Python 64bit version. Then open Anaconda Prompt and: Create new virtual environment and install mindsdb: conda create -n mindsdb conda activate mindsdb pip install mindsdb To verify that mindsdb was installed run: conda list Install using pip We suggest you to install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python version is >=3.6. Create new virtual environment called mindsdb: py -m venv mindsdb And, activate it: .\\mindsdb\\Scripts\\activate Install MindsDB: pip install mindsdb To verify that mindsdb was installed run: pip freeze Installation fail Don't worry, simply follow the below bellow instruction which should fix most issues. Python 64 bit version is required. Depending on your environment, you might have to use pip3 instead of pip , and python3.x instead of py in the above commands e.g pip3 install mindsdb If you are using Python 3.9 you may get installation errors. Some of the MindsDB's dependencies are not working with Python 3.9 , so please downgrade to older versions for now. We are working on this and Python 3.9 will be supported soon. If installation fails when installing torch or torchvision try manually installing them following the simple instructions on their official website . If you are using Windows, but are not using Anaconda or Conda, try installing one of them and running the installation from the anaconda prompt . If you've previously installed mindsdb and are having issues upgrading to a new version, try installing with the command: pip install mindsdb --upgrade If that still fails, try: pip install mindsdb --no-cache-dir --force-reinstall If none of this works, try installing mindsdb using the docker container and create an issue with the installation errors you got on our Github repository and we'll try to review it within a few hours.","title":"Install on Windows"},{"location":"installation/Windows/#install-using-mindsdb-installers","text":"Install MindsDB on your Windows machine using an easy-to-use installler. Download the installer MindsDB for Windows This installer will install Python3, MindsDB and MindsDB's dependencies, and create a shortcut on the desktop for starting MindsDB server.","title":"Install using MindsDB installers"},{"location":"installation/Windows/#install-using-anaconda","text":"You will need Anaconda or Conda installed and Python 64bit version. Then open Anaconda Prompt and: Create new virtual environment and install mindsdb: conda create -n mindsdb conda activate mindsdb pip install mindsdb To verify that mindsdb was installed run: conda list","title":"Install using Anaconda"},{"location":"installation/Windows/#install-using-pip","text":"We suggest you to install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python version is >=3.6. Create new virtual environment called mindsdb: py -m venv mindsdb And, activate it: .\\mindsdb\\Scripts\\activate Install MindsDB: pip install mindsdb To verify that mindsdb was installed run: pip freeze Installation fail Don't worry, simply follow the below bellow instruction which should fix most issues. Python 64 bit version is required. Depending on your environment, you might have to use pip3 instead of pip , and python3.x instead of py in the above commands e.g pip3 install mindsdb If you are using Python 3.9 you may get installation errors. Some of the MindsDB's dependencies are not working with Python 3.9 , so please downgrade to older versions for now. We are working on this and Python 3.9 will be supported soon. If installation fails when installing torch or torchvision try manually installing them following the simple instructions on their official website . If you are using Windows, but are not using Anaconda or Conda, try installing one of them and running the installation from the anaconda prompt . If you've previously installed mindsdb and are having issues upgrading to a new version, try installing with the command: pip install mindsdb --upgrade If that still fails, try: pip install mindsdb --no-cache-dir --force-reinstall If none of this works, try installing mindsdb using the docker container and create an issue with the installation errors you got on our Github repository and we'll try to review it within a few hours.","title":"Install using pip"},{"location":"integrations/AwsSageMaker/","text":"In the next sections, you can find general informations on how to build MindsDB container image, train models, deploy them on SageMaker and get the predictions. The code can be found inside the mindsdb-sagemaker-container GitHub repository. Training and Inference flow on SageMaker","title":"Amazon SageMaker"},{"location":"integrations/AwsSageMaker/#training-and-inference-flow-on-sagemaker","text":"","title":"Training and Inference flow on SageMaker"},{"location":"integrations/CallSageMakerEndpoint/","text":"To call the SageMaker endpint you can create Jupyter Notebook or use call.py script. Jupyter Notebook import boto3 endpointName = 'mindsdb-impl' # load test dataset with open ( 'diabetest-test.csv' , 'r' ) as reader : payload = reader . read () # Talk to SageMaker client = boto3 . client ( 'sagemaker-runtime' ) response = client . invoke_endpoint ( EndpointName = endpointName , Body = payload , ContentType = text / csv , Accept = 'Accept' ) print ( response [ 'Body' ] . read () . decode ( 'ascii' )) Run the code and you should see the prediction response from the endpoint: { \"prediction\" : \"* We are 96% confident the value of \" Class \" is positive.\" , \"class_confidence\" : [ 0.964147493532568 ] } call.py Script The required arguments are: endpoint - The name of the SageMaker endpoint. dataset - The location of test dataset. content type - The mime type of the data. python3 call . py -- endpoint mindsdb - impl -- dataset test_data / diabetes - test . json -- content - type application / json","title":"Call SageMaker Endpoint"},{"location":"integrations/CallSageMakerEndpoint/#jupyter-notebook","text":"import boto3 endpointName = 'mindsdb-impl' # load test dataset with open ( 'diabetest-test.csv' , 'r' ) as reader : payload = reader . read () # Talk to SageMaker client = boto3 . client ( 'sagemaker-runtime' ) response = client . invoke_endpoint ( EndpointName = endpointName , Body = payload , ContentType = text / csv , Accept = 'Accept' ) print ( response [ 'Body' ] . read () . decode ( 'ascii' )) Run the code and you should see the prediction response from the endpoint: { \"prediction\" : \"* We are 96% confident the value of \" Class \" is positive.\" , \"class_confidence\" : [ 0.964147493532568 ] }","title":"Jupyter Notebook"},{"location":"integrations/CallSageMakerEndpoint/#callpy-script","text":"The required arguments are: endpoint - The name of the SageMaker endpoint. dataset - The location of test dataset. content type - The mime type of the data. python3 call . py -- endpoint mindsdb - impl -- dataset test_data / diabetes - test . json -- content - type application / json","title":"call.py Script"},{"location":"integrations/MindsDBSageContainer/","text":"The general flow is to train and deploy models within SageMaker, create endpoints and take advantage of automated machine learning with MindsDB. The mindsdb_impl Code Structure All of the components we need to package MindsDB for Amazon SageMager are located inside the mindsdb_impl directory: |-- Dockerfile |-- build_and_push.sh `-- mindsdb_impl |-- nginx.conf |-- predictor.py |-- serve |-- train `-- wsgi.py All of the files that will be packaged in the container working directory are inside mindsdb_impl: * nginx.conf - is the configuration file for the nginx . * predictor.py is the program that actually implements the Flask web server and the MindsDB predictions for this app. We have modified this to use the MindsDB Predictor and to accept different types of tabular data for predictions. * serve is the program that is started when the container is started for hosting. * train is the program that is invoked when the container is being run for training. We have modified this program to use MindsDB Predictor interface. * wsgi.py is a small wrapper used to invoke the Flask app. Build Docker Image The docker command for building the image is build and -t parameter provides the name of the image: docker build -t mindsdb-impl . After getting the Successfully built message, we should be able to list the image by running the list command: docker image list Test the Container Locally The local_test directory contains all of the scripts and data samples for testing the built container on the local machine. train_local.sh: Instantiate the container configured for training. serve_local.sh: Instantiate the container configured for serving. predict.sh: Run predictions against a locally instantiated server. test-dir: This directory is mounted in the container. test_data: This directory contains a few tabular format datasets used for getting the predictions. input/data/training/file.csv`: The training data. model: The directory where MindsDB writes the model files. call.py: This cli script can be used for testing the deployed model on the SageMaker endpoint To train the model execute: ./train_local.sh mindsdb-impl Next, start the inference server that will provide an endpoint for getting the predictions. Inside the local_test directory execute serve_local.sh script: ./serve_local.sh mindsdb-impl To run predictions against the invocations endpoint, use predict.sh script: ./predict.sh test_data/diabetest-test.csv text/csv The arguments sent to the script are the test data and content type of the data. Deploy the Image on Amazon ECR (Elastic Container Repository) To push the image use build-and-push.sh script. Note that the script will look for an AWS EC Repository in the default region that you are using, and create a new one if that doesn't exist. ./build-and-push.sh mindsdb-impl After the mindsdb-impl image is deployed to Amazon ECR, you can use it inside SageMaker.","title":"MindsDB container"},{"location":"integrations/MindsDBSageContainer/#the-mindsdb_impl-code-structure","text":"All of the components we need to package MindsDB for Amazon SageMager are located inside the mindsdb_impl directory: |-- Dockerfile |-- build_and_push.sh `-- mindsdb_impl |-- nginx.conf |-- predictor.py |-- serve |-- train `-- wsgi.py All of the files that will be packaged in the container working directory are inside mindsdb_impl: * nginx.conf - is the configuration file for the nginx . * predictor.py is the program that actually implements the Flask web server and the MindsDB predictions for this app. We have modified this to use the MindsDB Predictor and to accept different types of tabular data for predictions. * serve is the program that is started when the container is started for hosting. * train is the program that is invoked when the container is being run for training. We have modified this program to use MindsDB Predictor interface. * wsgi.py is a small wrapper used to invoke the Flask app.","title":"The mindsdb_impl Code Structure"},{"location":"integrations/MindsDBSageContainer/#build-docker-image","text":"The docker command for building the image is build and -t parameter provides the name of the image: docker build -t mindsdb-impl . After getting the Successfully built message, we should be able to list the image by running the list command: docker image list","title":"Build Docker Image"},{"location":"integrations/MindsDBSageContainer/#test-the-container-locally","text":"The local_test directory contains all of the scripts and data samples for testing the built container on the local machine. train_local.sh: Instantiate the container configured for training. serve_local.sh: Instantiate the container configured for serving. predict.sh: Run predictions against a locally instantiated server. test-dir: This directory is mounted in the container. test_data: This directory contains a few tabular format datasets used for getting the predictions. input/data/training/file.csv`: The training data. model: The directory where MindsDB writes the model files. call.py: This cli script can be used for testing the deployed model on the SageMaker endpoint To train the model execute: ./train_local.sh mindsdb-impl Next, start the inference server that will provide an endpoint for getting the predictions. Inside the local_test directory execute serve_local.sh script: ./serve_local.sh mindsdb-impl To run predictions against the invocations endpoint, use predict.sh script: ./predict.sh test_data/diabetest-test.csv text/csv The arguments sent to the script are the test data and content type of the data.","title":"Test the Container Locally"},{"location":"integrations/MindsDBSageContainer/#deploy-the-image-on-amazon-ecr-elastic-container-repository","text":"To push the image use build-and-push.sh script. Note that the script will look for an AWS EC Repository in the default region that you are using, and create a new one if that doesn't exist. ./build-and-push.sh mindsdb-impl After the mindsdb-impl image is deployed to Amazon ECR, you can use it inside SageMaker.","title":"Deploy the Image on Amazon ECR (Elastic Container Repository)"},{"location":"integrations/SageMakerSDK/","text":"The following section explaines how to train and host models using Amazon SageMaker SDK . Add dependencies Install SageMaker SDK : pip install sagemaker First, add IAM Role that have AmazonSageMakerFullAccess Policy. import sagemaker as sage role = \"arn:aws:iam::123213143532:role/service-role/AmazonSageMaker-ExecutionRole-20199\" sess = sage . Session () account = sess . boto_session . client ( 'sts' ) . get_caller_identity ()[ 'Account' ] Next, provide s3 bucket where the models will be saved, get aws region from session and add URI to MindsDB image in AWS ECR: bucket_path = \"s3://mdb-sagemaker/models/\" region = sess . boto_session . region_name image = ' {} .dkr.ecr. {} .amazonaws.com/mindsdb_lts:latest' . format ( account , region ) Start training The required properties for invoking SageMaker training using Estimator are: The image name(str) -- The MindsDB container URI on ECR. The role(str) -- AWS arn with SageMaker execution role. The instance count(int) -- The number of machines to use for training. The instance type(str) -- The type of machine to use for training. The output path(str) -- Path to the s3 bucket where the model artifact will be saved. The session(sagemaker.session.Session) -- The SageMaker session object that we\u2019ve defined in the code above. The base job name(str) -- The name of the training job. The hyperparameters(dict) -- The MindsDB container requires to_predict value so it knows which column to predict. mindsdb_impl = sage . estimator . Estimator ( image , role , 1 , 'ml.m4.xlarge' , output_path = bucket_path , sagemaker_session = sess , base_job_name = \"mindsdb-lts-sdk\" , hyperparameters = { \"to_predict\" : \"Class\" }) Deploy the model The required configuration for deploying model: initial_instance_count (int) -- The initial number of instances to run in the Endpoint created from this Model. instance_type (str) -- The EC2 instance type to deploy this Model to. For example, \u2018ml.p2.xlarge\u2019, or \u2018local\u2019 for local mode. endpoint_name (str) -- The name of the endpoint on SageMaker. dataset_location = 's3://mdb-sagemaker/diabetes.csv' mindsdb_impl . fit ( dataset_location ) predictor = mindsdb_impl . deploy ( 1 , 'ml.m4.xlarge' , endpoint_name = 'mindsdb-impl' ) ``` ### Make a prediction Load the test dataset and then call the Predictor predict method with test data . ``` python with open ( 'test_data/diabetes-test.csv' , 'r' ) as reader : when_data = reader . read () print ( predictor . predict ( when_data ) . decode ( 'utf-8' )) Delete the endpoint Don't forget to delete the endpoint after using it. sess . delete_endpoint ( 'mindsdb-impl' ) Full code example import sagemaker as sage # Add AmazonSageMaker Execution role here role = \"arn:aws:iam:\" sess = sage . Session () account = sess . boto_session . client ( 'sts' ) . get_caller_identity ()[ 'Account' ] bucket_path = \"s3://mdb-sagemaker/models/\" region = sess . boto_session . region_name image = ' {} .dkr.ecr. {} .amazonaws.com/mindsdb_lts:latest' . format ( account , region ) #Hyperparameters to_predict is required for MindsDB container mindsdb_impl = sage . estimator . Estimator ( image , role , 1 , 'ml.m4.xlarge' , output_path = bucket_path , sagemaker_session = sess , base_job_name = \"mindsdb-lts-sdk\" , hyperparameters = { \"to_predict\" : \"Class\" }) dataset_location = 's3://mdb-sagemaker/diabetes.csv' mindsdb_impl . fit ( dataset_location ) predictor = mindsdb_impl . deploy ( 1 , 'ml.m4.xlarge' , endpoint_name = 'mindsdb-impl' ) with open ( 'test_data/diabetes-test.csv' , 'r' ) as reader : when_data = reader . read () print ( predictor . predict ( when_data ) . decode ( 'utf-8' ))","title":"Train and host MindsDB Models using SageMaker SDK"},{"location":"integrations/SageMakerSDK/#add-dependencies","text":"Install SageMaker SDK : pip install sagemaker First, add IAM Role that have AmazonSageMakerFullAccess Policy. import sagemaker as sage role = \"arn:aws:iam::123213143532:role/service-role/AmazonSageMaker-ExecutionRole-20199\" sess = sage . Session () account = sess . boto_session . client ( 'sts' ) . get_caller_identity ()[ 'Account' ] Next, provide s3 bucket where the models will be saved, get aws region from session and add URI to MindsDB image in AWS ECR: bucket_path = \"s3://mdb-sagemaker/models/\" region = sess . boto_session . region_name image = ' {} .dkr.ecr. {} .amazonaws.com/mindsdb_lts:latest' . format ( account , region )","title":"Add dependencies"},{"location":"integrations/SageMakerSDK/#start-training","text":"The required properties for invoking SageMaker training using Estimator are: The image name(str) -- The MindsDB container URI on ECR. The role(str) -- AWS arn with SageMaker execution role. The instance count(int) -- The number of machines to use for training. The instance type(str) -- The type of machine to use for training. The output path(str) -- Path to the s3 bucket where the model artifact will be saved. The session(sagemaker.session.Session) -- The SageMaker session object that we\u2019ve defined in the code above. The base job name(str) -- The name of the training job. The hyperparameters(dict) -- The MindsDB container requires to_predict value so it knows which column to predict. mindsdb_impl = sage . estimator . Estimator ( image , role , 1 , 'ml.m4.xlarge' , output_path = bucket_path , sagemaker_session = sess , base_job_name = \"mindsdb-lts-sdk\" , hyperparameters = { \"to_predict\" : \"Class\" })","title":"Start training"},{"location":"integrations/SageMakerSDK/#deploy-the-model","text":"The required configuration for deploying model: initial_instance_count (int) -- The initial number of instances to run in the Endpoint created from this Model. instance_type (str) -- The EC2 instance type to deploy this Model to. For example, \u2018ml.p2.xlarge\u2019, or \u2018local\u2019 for local mode. endpoint_name (str) -- The name of the endpoint on SageMaker. dataset_location = 's3://mdb-sagemaker/diabetes.csv' mindsdb_impl . fit ( dataset_location ) predictor = mindsdb_impl . deploy ( 1 , 'ml.m4.xlarge' , endpoint_name = 'mindsdb-impl' ) ``` ### Make a prediction Load the test dataset and then call the Predictor predict method with test data . ``` python with open ( 'test_data/diabetes-test.csv' , 'r' ) as reader : when_data = reader . read () print ( predictor . predict ( when_data ) . decode ( 'utf-8' ))","title":"Deploy the model"},{"location":"integrations/SageMakerSDK/#delete-the-endpoint","text":"Don't forget to delete the endpoint after using it. sess . delete_endpoint ( 'mindsdb-impl' )","title":"Delete the endpoint"},{"location":"integrations/SageMakerSDK/#full-code-example","text":"import sagemaker as sage # Add AmazonSageMaker Execution role here role = \"arn:aws:iam:\" sess = sage . Session () account = sess . boto_session . client ( 'sts' ) . get_caller_identity ()[ 'Account' ] bucket_path = \"s3://mdb-sagemaker/models/\" region = sess . boto_session . region_name image = ' {} .dkr.ecr. {} .amazonaws.com/mindsdb_lts:latest' . format ( account , region ) #Hyperparameters to_predict is required for MindsDB container mindsdb_impl = sage . estimator . Estimator ( image , role , 1 , 'ml.m4.xlarge' , output_path = bucket_path , sagemaker_session = sess , base_job_name = \"mindsdb-lts-sdk\" , hyperparameters = { \"to_predict\" : \"Class\" }) dataset_location = 's3://mdb-sagemaker/diabetes.csv' mindsdb_impl . fit ( dataset_location ) predictor = mindsdb_impl . deploy ( 1 , 'ml.m4.xlarge' , endpoint_name = 'mindsdb-impl' ) with open ( 'test_data/diabetes-test.csv' , 'r' ) as reader : when_data = reader . read () print ( predictor . predict ( when_data ) . decode ( 'utf-8' ))","title":"Full code example"},{"location":"integrations/UseMindsDBinSage/","text":"The following section explains how to train and host models using Amazon SageMaker console . Create Train Job Follow the steps below to successfully start a train job and use MindsDB to create the models: 1. Open the Amazon SageMaker console at https://console.aws.amazon.com/sagemaker/. From the left panel choose Create Training Job and provide the following information Job name IAM role - it\u2019s best if you provide AmazonSageMakeFullAccess IAM policy Algorithm source - Your own algorithm container in ECR Provide container ECR path Container - the ECR registry Image URI that we have pushed Input mode - File Resource configuration - leave the default instance type and count Hyperparameters - MindsDB requires to_predict column name, so it knows which column we want to predict, e.g. Key - to_predict Value - Class(the column in diabetes dataset) Input data configuration Channel name - training Data source - s3 S3 location - path to the s3 bucket where the dataset is located 7.Output data configuration - path to the s3 where the models will be saved Model creation Create model and add the required settings: 1. Model name - must be unique IAM role - it\u2019s best if you provide AmazonSageMakeFullAccess IAM policy Container input options Provide model artifacts and inference image location Use a single model Location of the inference code - 846763053924.dkr.ecr.us-east-1.amazonaws.com/mindsdb_impl:latest Location of model artifacts - path to model.tar.gz inside s3 bucket. Endpoint configuration In the endpoint configuration, add the models to deploy, and the hardware requirements: Endpoint configuration name. Add model - select the previously created model. Choose Create endpoint configuration. Create endpoint The last step is to create endpoint and provide endpoint configuration that specify which models to deploy and the requirements: Endpoint name. Attach endpoint configuration - select the previously created endpoint configuration. Choose Create endpoint. After finishing the above steps, SageMaker will create a new instance and start the inference code.","title":"Train and host MindsDB models"},{"location":"integrations/UseMindsDBinSage/#create-train-job","text":"Follow the steps below to successfully start a train job and use MindsDB to create the models: 1. Open the Amazon SageMaker console at https://console.aws.amazon.com/sagemaker/. From the left panel choose Create Training Job and provide the following information Job name IAM role - it\u2019s best if you provide AmazonSageMakeFullAccess IAM policy Algorithm source - Your own algorithm container in ECR Provide container ECR path Container - the ECR registry Image URI that we have pushed Input mode - File Resource configuration - leave the default instance type and count Hyperparameters - MindsDB requires to_predict column name, so it knows which column we want to predict, e.g. Key - to_predict Value - Class(the column in diabetes dataset) Input data configuration Channel name - training Data source - s3 S3 location - path to the s3 bucket where the dataset is located 7.Output data configuration - path to the s3 where the models will be saved","title":"Create Train Job"},{"location":"integrations/UseMindsDBinSage/#model-creation","text":"Create model and add the required settings: 1. Model name - must be unique IAM role - it\u2019s best if you provide AmazonSageMakeFullAccess IAM policy Container input options Provide model artifacts and inference image location Use a single model Location of the inference code - 846763053924.dkr.ecr.us-east-1.amazonaws.com/mindsdb_impl:latest Location of model artifacts - path to model.tar.gz inside s3 bucket.","title":"Model creation"},{"location":"integrations/UseMindsDBinSage/#endpoint-configuration","text":"In the endpoint configuration, add the models to deploy, and the hardware requirements: Endpoint configuration name. Add model - select the previously created model. Choose Create endpoint configuration.","title":"Endpoint configuration"},{"location":"integrations/UseMindsDBinSage/#create-endpoint","text":"The last step is to create endpoint and provide endpoint configuration that specify which models to deploy and the requirements: Endpoint name. Attach endpoint configuration - select the previously created endpoint configuration. Choose Create endpoint. After finishing the above steps, SageMaker will create a new instance and start the inference code.","title":"Create endpoint"},{"location":"lightwood/API/","text":"Predictor API from lightwood import Predictor Lightwood has one main class; The Predictor , which is a modular construct that you can train and get predictions from. It is made out of 3 main building blocks ( features, encoders, mixers ) that you can configure, modify and expand as you wish. Building blocks Features : input_features : These are the columns in your dataset that you want to take as input for your predictor. output_features : These are the columns in your dataset that you want to learn how to predict. Encoders : These are tools to turn the data in your input or output features into vector/tensor representations and vice-versa. Mixers : How you mix the output of encoded features and also other mixers Constructor, __init__() my_predictor = Predictor ( output = [] | config = { ... } | load_from_path =< file_path > ) Predictor, can take any of the following arguments load_from_path : If you have a saved predictor that you want to load, just give the path to the file output : A list with the column names you want to predict. ( Note: If you pass this argument, lightwood will simply try to guess the best config possible ) config : A dictionary, containing the configuration on how to glue all the building blocks. config The config argument allows you to pass a dictionary that defines and gives you absolute control over how to build your predictive model. A config example goes as follows: from lightwood import COLUMN_DATA_TYPES , BUILTIN_MIXERS , BUILTIN_ENCODERS config = { ## REQUIRED: 'input_features' : [ # by default each feature has an encoder, so all you have to do is specify the data type { 'name' : 'sensor1' , 'type' : COLUMN_DATA_TYPES . NUMERIC }, { 'name' : 'sensor2' , 'type' : COLUMN_DATA_TYPES . NUMERIC }, # some encoders have attributes that can be specified on the configuration # in this particular lets assume we have a photo of the product, we would like to encode this image and optimize for speed { 'name' : 'product_photo' , 'type' : COLUMN_DATA_TYPES . IMAGE , 'encoder_class' : BUILTIN_ENCODERS . Image . Img2VecEncoder , # note that this is just a class, you can build your own if you wish 'encoder_attrs' : { 'aim' : 'speed' # you can check the encoder attributes here: # https://github.com/mindsdb/lightwood/blob/master/lightwood/encoders/image/img_2_vec.py } } ], 'output_features' : [ { 'name' : 'action_to_take' , 'type' : COLUMN_DATA_TYPES . CATEGORICAL } ], ## OPTIONAL 'mixer' : { 'class' : BUILTIN_MIXERS . NnMixer } } features Both input_features and output_features configs are simple dicts that have the following schema name : is the name of the column as it is in the input data frame type : is the type of data contained. Where out of the box, supported COLUMN_DATA_TYPES are NUMERIC, CATEGORICAL, DATETIME, IMAGE, TEXT, TIME_SERIES : If you specify the type, lightwood will use the default encoder for that type, however, you can specify/define any encoder that you want to use. encoder_class : This is if you want to replace the default encoder with a different one, so you put the encoder class there encoder_attrs : These are the attributes that you want to setup on the encoder once the class its initialized mixer The default_mixer key, provides information as to what mixer to use. The schema for this variable is as follows: mixer_schema = Schema ({ 'class' : object , Optional ( 'attrs' ): dict }) class : Its the actual class, that defines the Mixer, you can use any of the BUILTIN_MIXERS or pass your own. attrs : This is a dictionary containing the attributes you want to replace on the mixer object once its initialized. We do this, so you have maximum flexibility as to what you can customize on your Mixers. learn() This method is used to make the predictor learn from some data, thus the learn method takes the following arguments. from_data : A pandas dataframe, that has some or all the columns in the config. The reason why we decide to only suppor pandas dataframes, its because, its easy to load any data to a pandas draframe, and spark for python dataframe is a format we support. test_data : (Optional) This is if you want to specify what data to test with, if no test_data passed, lightwood will break the from_data into test and train automatically. callback_on_iter : (Optional) This is function callback that is called every 100 epocs the during the learn process. predict() This method is used to make predictions and it can take one of the following arguments when : this is a dictionary of conditions to predict under. when_data : Sometimes you want to predict more than one row a ta time, so here it is: a pandas dataframe containing the conditional values you want to use to make a prediction. save() Use this method to save the predictor into a desired path calculate_accuracy() Returns the predictors overall accuracy.","title":"Predictor API"},{"location":"lightwood/API/#predictor-api","text":"from lightwood import Predictor Lightwood has one main class; The Predictor , which is a modular construct that you can train and get predictions from. It is made out of 3 main building blocks ( features, encoders, mixers ) that you can configure, modify and expand as you wish. Building blocks Features : input_features : These are the columns in your dataset that you want to take as input for your predictor. output_features : These are the columns in your dataset that you want to learn how to predict. Encoders : These are tools to turn the data in your input or output features into vector/tensor representations and vice-versa. Mixers : How you mix the output of encoded features and also other mixers","title":"Predictor API"},{"location":"lightwood/API/#constructor-__init__","text":"my_predictor = Predictor ( output = [] | config = { ... } | load_from_path =< file_path > ) Predictor, can take any of the following arguments load_from_path : If you have a saved predictor that you want to load, just give the path to the file output : A list with the column names you want to predict. ( Note: If you pass this argument, lightwood will simply try to guess the best config possible ) config : A dictionary, containing the configuration on how to glue all the building blocks.","title":"Constructor, __init__()"},{"location":"lightwood/API/#config","text":"The config argument allows you to pass a dictionary that defines and gives you absolute control over how to build your predictive model. A config example goes as follows: from lightwood import COLUMN_DATA_TYPES , BUILTIN_MIXERS , BUILTIN_ENCODERS config = { ## REQUIRED: 'input_features' : [ # by default each feature has an encoder, so all you have to do is specify the data type { 'name' : 'sensor1' , 'type' : COLUMN_DATA_TYPES . NUMERIC }, { 'name' : 'sensor2' , 'type' : COLUMN_DATA_TYPES . NUMERIC }, # some encoders have attributes that can be specified on the configuration # in this particular lets assume we have a photo of the product, we would like to encode this image and optimize for speed { 'name' : 'product_photo' , 'type' : COLUMN_DATA_TYPES . IMAGE , 'encoder_class' : BUILTIN_ENCODERS . Image . Img2VecEncoder , # note that this is just a class, you can build your own if you wish 'encoder_attrs' : { 'aim' : 'speed' # you can check the encoder attributes here: # https://github.com/mindsdb/lightwood/blob/master/lightwood/encoders/image/img_2_vec.py } } ], 'output_features' : [ { 'name' : 'action_to_take' , 'type' : COLUMN_DATA_TYPES . CATEGORICAL } ], ## OPTIONAL 'mixer' : { 'class' : BUILTIN_MIXERS . NnMixer } }","title":"config"},{"location":"lightwood/API/#features","text":"Both input_features and output_features configs are simple dicts that have the following schema name : is the name of the column as it is in the input data frame type : is the type of data contained. Where out of the box, supported COLUMN_DATA_TYPES are NUMERIC, CATEGORICAL, DATETIME, IMAGE, TEXT, TIME_SERIES : If you specify the type, lightwood will use the default encoder for that type, however, you can specify/define any encoder that you want to use. encoder_class : This is if you want to replace the default encoder with a different one, so you put the encoder class there encoder_attrs : These are the attributes that you want to setup on the encoder once the class its initialized","title":"features"},{"location":"lightwood/API/#mixer","text":"The default_mixer key, provides information as to what mixer to use. The schema for this variable is as follows: mixer_schema = Schema ({ 'class' : object , Optional ( 'attrs' ): dict }) class : Its the actual class, that defines the Mixer, you can use any of the BUILTIN_MIXERS or pass your own. attrs : This is a dictionary containing the attributes you want to replace on the mixer object once its initialized. We do this, so you have maximum flexibility as to what you can customize on your Mixers.","title":"mixer"},{"location":"lightwood/API/#learn","text":"This method is used to make the predictor learn from some data, thus the learn method takes the following arguments. from_data : A pandas dataframe, that has some or all the columns in the config. The reason why we decide to only suppor pandas dataframes, its because, its easy to load any data to a pandas draframe, and spark for python dataframe is a format we support. test_data : (Optional) This is if you want to specify what data to test with, if no test_data passed, lightwood will break the from_data into test and train automatically. callback_on_iter : (Optional) This is function callback that is called every 100 epocs the during the learn process.","title":"learn()"},{"location":"lightwood/API/#predict","text":"This method is used to make predictions and it can take one of the following arguments when : this is a dictionary of conditions to predict under. when_data : Sometimes you want to predict more than one row a ta time, so here it is: a pandas dataframe containing the conditional values you want to use to make a prediction.","title":"predict()"},{"location":"lightwood/API/#save","text":"Use this method to save the predictor into a desired path","title":"save()"},{"location":"lightwood/API/#calculate_accuracy","text":"Returns the predictors overall accuracy.","title":"calculate_accuracy()"},{"location":"lightwood/info/","text":"Lightwood is a Pytorch based framework with two objectives: Make it so simple that you can build predictive models with a line of code. Make it so flexible that you can change and customize everything. Lightwood was inspired on Keras + Ludwig but runs on Pytorch and gives you full control of what you can do. Prerequisites Python >=3.6 64bit version Installing Lightwood You can install Lightwood using pip : pip3 install lightwood If this fails, please report the bug on github and try installing the current master branch: git clone git@github.com:mindsdb/lightwood.git ; cd lightwood ; pip install --no-cache-dir -e . Please note that, depending on your os and python setup, you might want to use pip instead of pip3 . You need python 3.6 or higher. Note on MacOS, you need to install libomp: brew install libomp Install using virtual environment We suggest you to install Lightwood on a virtual environment to avoid dependency issues. Make sure your Python version is >=3.6. To set up a virtual environment: Install on Windows Install the latest version of pip : python -m pip install --upgrade pip pip --version Activate your virtual environment and install lightwood: py -m pip install --user virtualenv . \\e nv \\S cripts \\a ctivate pip install lightwood You can also use python instead of py Install on Linux or macOS Before installing Lightwood in a virtual environment you need to first create and activate the venv : python -m venv env source env/bin/activate pip install lightwood Quick example Assume that you have a training file (sensor_data.csv) such as this one. sensor1 sensor2 sensor3 1 -1 -1 0 1 0 -1 -1 1 1 0 0 0 1 0 -1 1 -1 0 0 0 -1 -1 1 1 0 0 And you would like to learn to predict the values of sensor3 given the readings in sensor1 and sensor2 . Learn You can train a Predictor as follows: from lightwood import Predictor import pandas sensor3_predictor = Predictor ( output = [ 'sensor3' ]) sensor3_predictor . learn ( from_data = pandas . read_csv ( 'sensor_data.csv' )) Predict You can now be given new readings from sensor1 and sensor2 predict what sensor3 will be. prediction = sensor3_predictor . predict ( when = { 'sensor1' : 1 , 'sensor2' : - 1 }) print ( prediction ) Of course, that example was just the tip of the iceberg, please read about the main concepts of lightwood, the API and then jump into examples.","title":"Introduction to Lightwood"},{"location":"lightwood/info/#prerequisites","text":"Python >=3.6 64bit version","title":"Prerequisites"},{"location":"lightwood/info/#installing-lightwood","text":"You can install Lightwood using pip : pip3 install lightwood If this fails, please report the bug on github and try installing the current master branch: git clone git@github.com:mindsdb/lightwood.git ; cd lightwood ; pip install --no-cache-dir -e . Please note that, depending on your os and python setup, you might want to use pip instead of pip3 . You need python 3.6 or higher. Note on MacOS, you need to install libomp: brew install libomp","title":"Installing Lightwood"},{"location":"lightwood/info/#install-using-virtual-environment","text":"We suggest you to install Lightwood on a virtual environment to avoid dependency issues. Make sure your Python version is >=3.6. To set up a virtual environment:","title":"Install using virtual environment"},{"location":"lightwood/info/#install-on-windows","text":"Install the latest version of pip : python -m pip install --upgrade pip pip --version Activate your virtual environment and install lightwood: py -m pip install --user virtualenv . \\e nv \\S cripts \\a ctivate pip install lightwood You can also use python instead of py","title":"Install on Windows"},{"location":"lightwood/info/#install-on-linux-or-macos","text":"Before installing Lightwood in a virtual environment you need to first create and activate the venv : python -m venv env source env/bin/activate pip install lightwood","title":"Install on Linux or macOS"},{"location":"lightwood/info/#quick-example","text":"Assume that you have a training file (sensor_data.csv) such as this one. sensor1 sensor2 sensor3 1 -1 -1 0 1 0 -1 -1 1 1 0 0 0 1 0 -1 1 -1 0 0 0 -1 -1 1 1 0 0 And you would like to learn to predict the values of sensor3 given the readings in sensor1 and sensor2 .","title":"Quick example"},{"location":"lightwood/info/#learn","text":"You can train a Predictor as follows: from lightwood import Predictor import pandas sensor3_predictor = Predictor ( output = [ 'sensor3' ]) sensor3_predictor . learn ( from_data = pandas . read_csv ( 'sensor_data.csv' ))","title":"Learn"},{"location":"lightwood/info/#predict","text":"You can now be given new readings from sensor1 and sensor2 predict what sensor3 will be. prediction = sensor3_predictor . predict ( when = { 'sensor1' : 1 , 'sensor2' : - 1 }) print ( prediction ) Of course, that example was just the tip of the iceberg, please read about the main concepts of lightwood, the API and then jump into examples.","title":"Predict"},{"location":"scout/Datasources/","text":"Data In the Data dashboard, you can upload your dataset to MindsDB Server (on your remote/local machine). The available options to upload the data are: From the database(MySQL, PostgreSQL, MariaDB, MongoDB, ClickHouse, MsSQL) From the local file system From URL After upload, you will be able to preview (search, sort, order) the data and get the data quality statistics. Data Quality The Data Quality view shows the quality score of each column(input) value and the variability in the dataset. MindsDB looks in every column, analyzes the data, calculates the quality score and shows suggestions and warnings about the data.","title":"Datasources"},{"location":"scout/Datasources/#data","text":"In the Data dashboard, you can upload your dataset to MindsDB Server (on your remote/local machine). The available options to upload the data are: From the database(MySQL, PostgreSQL, MariaDB, MongoDB, ClickHouse, MsSQL) From the local file system From URL After upload, you will be able to preview (search, sort, order) the data and get the data quality statistics.","title":"Data"},{"location":"scout/Datasources/#data-quality","text":"The Data Quality view shows the quality score of each column(input) value and the variability in the dataset. MindsDB looks in every column, analyzes the data, calculates the quality score and shows suggestions and warnings about the data.","title":"Data Quality"},{"location":"scout/Introduction/","text":"Introduction to MindsDB Scout MindsDB Scout is a graphical user interface that sits on top of MindsDB and MindsDB Server. It lets you upload, analyze and visualize your data, train machine learning models and query them with few clicks directly from the Databases, your local file system or external resources. Start Scout You don't need to install anything to run Scout. Starting the MindsDB server will automatically run Scout on your default browser. To do start mindsdb: python3 -m mindsdb When MindsDB server is started it will display the URL for accessing Scout: GUI should be available by http://localhost:47334/index.html And automatically start Scout on that URL. Tutorial We have created a few tutorials that will help you to successfully install and use MindsDB Scout: An Overview of MindsDB Scout Perform Data Science with MindsDB Scout Overview of MindsDB Scout The MindsDB Scout interface is divided into 3 dashboards. Data Predictors Query","title":"Getting Started"},{"location":"scout/Introduction/#introduction-to-mindsdb-scout","text":"MindsDB Scout is a graphical user interface that sits on top of MindsDB and MindsDB Server. It lets you upload, analyze and visualize your data, train machine learning models and query them with few clicks directly from the Databases, your local file system or external resources.","title":"Introduction to MindsDB Scout"},{"location":"scout/Introduction/#start-scout","text":"You don't need to install anything to run Scout. Starting the MindsDB server will automatically run Scout on your default browser. To do start mindsdb: python3 -m mindsdb When MindsDB server is started it will display the URL for accessing Scout: GUI should be available by http://localhost:47334/index.html And automatically start Scout on that URL.","title":"Start Scout"},{"location":"scout/Introduction/#tutorial","text":"We have created a few tutorials that will help you to successfully install and use MindsDB Scout: An Overview of MindsDB Scout Perform Data Science with MindsDB Scout","title":"Tutorial"},{"location":"scout/Introduction/#overview-of-mindsdb-scout","text":"The MindsDB Scout interface is divided into 3 dashboards. Data Predictors Query","title":"Overview of MindsDB Scout"},{"location":"scout/Predictors/","text":"Predictors The Predictor, in MindsDB\u2019s words, means the machine learning model. In this dashboard, you will train the models from the Datasources. How to train Predictor? MindsDB Scout offers two options for Predictor training: Upload the already existing model. Train a new one. Basic Mode To Train New Predictor, the required options are the Predictor Name and the columns to be predicted (target variables). Advanced Mode Inside the Advanced Mode, you can specify additional options before training new Predictor as: Sample Margin of Error : the amount of random sampling error. Stop Training After : stop the model training after n minutes. Use GPU : use Graphics Processing Unit for training. Select columns to be removed for training : speed the training or improve accuracy by ignoring un relevant columns. Predictor Results The Predictor Results dashboard, provides powerful insights related to the model. The 3 important questions that this section answers are: How accurate is the model? What is relevant for this model? When can you trust this model?","title":"Predictors"},{"location":"scout/Predictors/#predictors","text":"The Predictor, in MindsDB\u2019s words, means the machine learning model. In this dashboard, you will train the models from the Datasources.","title":"Predictors"},{"location":"scout/Predictors/#how-to-train-predictor","text":"MindsDB Scout offers two options for Predictor training: Upload the already existing model. Train a new one.","title":"How to train Predictor?"},{"location":"scout/Predictors/#basic-mode","text":"To Train New Predictor, the required options are the Predictor Name and the columns to be predicted (target variables).","title":"Basic Mode"},{"location":"scout/Predictors/#advanced-mode","text":"Inside the Advanced Mode, you can specify additional options before training new Predictor as: Sample Margin of Error : the amount of random sampling error. Stop Training After : stop the model training after n minutes. Use GPU : use Graphics Processing Unit for training. Select columns to be removed for training : speed the training or improve accuracy by ignoring un relevant columns.","title":"Advanced Mode"},{"location":"scout/Predictors/#predictor-results","text":"The Predictor Results dashboard, provides powerful insights related to the model. The 3 important questions that this section answers are: How accurate is the model? What is relevant for this model? When can you trust this model?","title":"Predictor Results"},{"location":"scout/Query/","text":"Query The Query dashboard is where you can get the predictions from the model. Creating a New Query is simple as select the model and choose what you want to predict. After querying the Predictor, MindsDB will provide: Explanation about the predicted value and the confidence.","title":"Query"},{"location":"scout/Query/#query","text":"The Query dashboard is where you can get the predictions from the model. Creating a New Query is simple as select the model and choose what you want to predict. After querying the Predictor, MindsDB will provide: Explanation about the predicted value and the confidence.","title":"Query"},{"location":"server/SDKs/","text":"The MindsDB SDK's are providing all of the MindsDB's native functionalities through MindsDB HTTP Interface. Currently, MindsDB provides SDK's for JavaScript and Python. Installing JavaScript SDK The JavaScript SDK can be installed with npm or yarn: npm install mindsdb-js-sdk or yarn add mindsdb-js-sdk Also, you can install it from source: git clone git@github.com:mindsdb/mindsdb_js_sdk.git cd mindsdb_js_sdk npm install Usage example The following example covers the basic flow: connect to MindsDB Server, train new model, make predictions. import MindsDB from 'mindsdb-js-sdk' ; //connection MindsDB . connect ( url ); const connected = await MindsDB . ping (); if ( ! connected ) return ; // lists of predictors and datasources const predictorsList = MindsDB . dataSources (); const predictors = MindsDB . predictors (); // get datasource const rentalsDatasource = await MindsDB . DataSource ({ name : 'home_rentals' }). load (); // get predictor const rentalsPredictor = await MindsDB . Predictor ({ name : 'home_rentals' }). load (); // query const result = rentalsPredictor . queryPredict ({ 'initial)|_price' : 2000 , 'sqft' : 500 }); console . log ( result ); MindsDB . disconnect (); Installing Python SDK The Python SDK can be installed from PyPI: pip install mindsdb-client Or you can install it from source: git clone git@github.com:mindsdb/mindsdb_python_sdk.git cd mindsdb_python_sdk python setup.py develop pip install -r requirements.txt Usage example The following example covers the basic flow: connect to MindsDB Server, train new model, make predictions. from mindsdb_client import MindsDB # connect mdb = MindsDB ( server = 'http://mindsdb.server:47334' , params = { 'email' : 'login@email.com' , 'password' : 'loginpass' }) # upload datasource mdb . datasources . add ( 'home_rentals_data' , path = 'home_rentals.csv' ) # create a new predictor and learn to predict predictor = mdb . predictors . learn ( name = 'home_rentals' , data_source_name = 'home_rentals_data' , to_predict = 'rental_price' ) # predict result = predictor . predict ({ 'initial_price' : '2000' , 'number_of_bathrooms' : '1' , 'sqft' : '700' })","title":"MindsDB SDK"},{"location":"server/SDKs/#installing-javascript-sdk","text":"The JavaScript SDK can be installed with npm or yarn: npm install mindsdb-js-sdk or yarn add mindsdb-js-sdk Also, you can install it from source: git clone git@github.com:mindsdb/mindsdb_js_sdk.git cd mindsdb_js_sdk npm install","title":"Installing JavaScript SDK"},{"location":"server/SDKs/#usage-example","text":"The following example covers the basic flow: connect to MindsDB Server, train new model, make predictions. import MindsDB from 'mindsdb-js-sdk' ; //connection MindsDB . connect ( url ); const connected = await MindsDB . ping (); if ( ! connected ) return ; // lists of predictors and datasources const predictorsList = MindsDB . dataSources (); const predictors = MindsDB . predictors (); // get datasource const rentalsDatasource = await MindsDB . DataSource ({ name : 'home_rentals' }). load (); // get predictor const rentalsPredictor = await MindsDB . Predictor ({ name : 'home_rentals' }). load (); // query const result = rentalsPredictor . queryPredict ({ 'initial)|_price' : 2000 , 'sqft' : 500 }); console . log ( result ); MindsDB . disconnect ();","title":"Usage example"},{"location":"server/SDKs/#installing-python-sdk","text":"The Python SDK can be installed from PyPI: pip install mindsdb-client Or you can install it from source: git clone git@github.com:mindsdb/mindsdb_python_sdk.git cd mindsdb_python_sdk python setup.py develop pip install -r requirements.txt","title":"Installing Python SDK"},{"location":"server/SDKs/#usage-example_1","text":"The following example covers the basic flow: connect to MindsDB Server, train new model, make predictions. from mindsdb_client import MindsDB # connect mdb = MindsDB ( server = 'http://mindsdb.server:47334' , params = { 'email' : 'login@email.com' , 'password' : 'loginpass' }) # upload datasource mdb . datasources . add ( 'home_rentals_data' , path = 'home_rentals.csv' ) # create a new predictor and learn to predict predictor = mdb . predictors . learn ( name = 'home_rentals' , data_source_name = 'home_rentals_data' , to_predict = 'rental_price' ) # predict result = predictor . predict ({ 'initial_price' : '2000' , 'number_of_bathrooms' : '1' , 'sqft' : '700' })","title":"Usage example"},{"location":"tutorials/AdvancedExamples/","text":"Multiple column predictions What is a multiple column prediction ? In some cases, you might want to predict more than one column of your data. In order for mindsdb to predict multiple columns, you simply need to change the to_predict argument from a string (denoting the name of the column) to an array containing the names of the columns you want to predict. In the following example, we've altered the real estate model to predict the location and neighborhood both, instead of the rental_price . Code example import mindsdb mdb = mindsdb . Predictor ( name = 'multilabel_real_estate_model' ) mdb . learn ( from_data = \"https://s3.eu-west-2.amazonaws.com/mindsdb-example-data/home_rentals.csv\" , to_predict = [ 'location' , 'neighborhood' ] # Array with the names of the columns we want to predict ) Multimedia inputs (images, audio and video) Currently, we only support images as inputs into models. We are working on support audio, you can check this issue to track the progress. Video input support is not yet planned. For any sort of media files, simply provide the full path to the file in the column. For example: [ { 'img' : '/mnt/data/img_1.jpg' 'is_bicycle' : True } ,{ 'img' : '/mnt/data/img_2.jpg' 'is_bicycle' : True } ,{ 'img' : '/mnt/data/img_3.jpg' 'is_bicycle' : False } ] Please provide the full path to a file on your local machine, not a url or the binary data from an image loaded up in a dataframe. Currently, the timeline on supporting multimedia output is still undecided, if you need that feature or want to implement, feel free to contact us. That being said, image outputs might actually work, we just haven't tested anything yet. Unbalanced dataset Given a dataset that is \"imbalanced\", the model being trained might not give the results you expect. For example, let's say you have the following dataset: data . csv x , is_power_of_9 1 , False 2 , False 3 , False 4 , False 5 , False 6 , False 7 , False 8 , False 9 , True 10 , False 11 , False . . . 81 , True 82 , False 83 , False 84 , False . . . 100 , False On which we want to predicted the aptly-named column is_power_of_9 . We have 2 occurrences of the output True and 98 occurrences of the False output. So a model could always predict False and it would have an accuracy of 98%, which is pretty decent, so unless there's a way for the model to learn those 2 instance of True without losing on accuracy, it might just decided 98% is the best possible model. However, let's say we really care about those True predictions being correct, or at least we want the model to consider them equally important, in that case we would call the learn function using the equal_accuracy_for_all_output_categories argument set to true. This essentially means that a model with 50% accuracy, with 2 correct predictions for True (100%) and 48 for False (49%) is considered better than a model with 98% accuracy that only predictions False when mindsdb trains the model. We could call this as: predictior.learn(from_data='data.csv', equal_accuracy_for_all_output_categories=True)","title":"Advanced usecases"},{"location":"tutorials/AdvancedExamples/#multiple-column-predictions","text":"","title":"Multiple column predictions"},{"location":"tutorials/AdvancedExamples/#what-is-a-multiple-column-prediction","text":"In some cases, you might want to predict more than one column of your data. In order for mindsdb to predict multiple columns, you simply need to change the to_predict argument from a string (denoting the name of the column) to an array containing the names of the columns you want to predict. In the following example, we've altered the real estate model to predict the location and neighborhood both, instead of the rental_price .","title":"What is a multiple column prediction ?"},{"location":"tutorials/AdvancedExamples/#code-example","text":"import mindsdb mdb = mindsdb . Predictor ( name = 'multilabel_real_estate_model' ) mdb . learn ( from_data = \"https://s3.eu-west-2.amazonaws.com/mindsdb-example-data/home_rentals.csv\" , to_predict = [ 'location' , 'neighborhood' ] # Array with the names of the columns we want to predict )","title":"Code example"},{"location":"tutorials/AdvancedExamples/#multimedia-inputs-images-audio-and-video","text":"Currently, we only support images as inputs into models. We are working on support audio, you can check this issue to track the progress. Video input support is not yet planned. For any sort of media files, simply provide the full path to the file in the column. For example: [ { 'img' : '/mnt/data/img_1.jpg' 'is_bicycle' : True } ,{ 'img' : '/mnt/data/img_2.jpg' 'is_bicycle' : True } ,{ 'img' : '/mnt/data/img_3.jpg' 'is_bicycle' : False } ] Please provide the full path to a file on your local machine, not a url or the binary data from an image loaded up in a dataframe. Currently, the timeline on supporting multimedia output is still undecided, if you need that feature or want to implement, feel free to contact us. That being said, image outputs might actually work, we just haven't tested anything yet.","title":"Multimedia inputs (images, audio and video)"},{"location":"tutorials/AdvancedExamples/#unbalanced-dataset","text":"Given a dataset that is \"imbalanced\", the model being trained might not give the results you expect. For example, let's say you have the following dataset: data . csv x , is_power_of_9 1 , False 2 , False 3 , False 4 , False 5 , False 6 , False 7 , False 8 , False 9 , True 10 , False 11 , False . . . 81 , True 82 , False 83 , False 84 , False . . . 100 , False On which we want to predicted the aptly-named column is_power_of_9 . We have 2 occurrences of the output True and 98 occurrences of the False output. So a model could always predict False and it would have an accuracy of 98%, which is pretty decent, so unless there's a way for the model to learn those 2 instance of True without losing on accuracy, it might just decided 98% is the best possible model. However, let's say we really care about those True predictions being correct, or at least we want the model to consider them equally important, in that case we would call the learn function using the equal_accuracy_for_all_output_categories argument set to true. This essentially means that a model with 50% accuracy, with 2 correct predictions for True (100%) and 48 for False (49%) is considered better than a model with 98% accuracy that only predictions False when mindsdb trains the model. We could call this as: predictior.learn(from_data='data.csv', equal_accuracy_for_all_output_categories=True)","title":"Unbalanced dataset"},{"location":"tutorials/BasicExample/","text":"This is a basic example of mindsdb_native usage in predicting the real estate prices for an area. If you want to follow out visually, watch bellow video: Goal The goal is to be able to predict the best rental_price for new properties given the information that we have in home_rentals.csv. Learning from mindsdb import Predictor # We tell the Predictor what column or key we want to learn and from what data Predictor ( name = 'real_estate_model' ) . learn ( from_data = \"https://s3.eu-west-2.amazonaws.com/mindsdb-example-data/home_rentals.csv\" , # the path to the file where we can learn from, (note: can be url) to_predict = 'rental_price' , # the column we want to learn to predict given all the data in the file ) Note : that the argument from_data can be a path to a json, csv (or other separators), excel given as a file or as a URL, or a pandas Dataframe Predicting mdb = mindsdb . Predictor ( name = 'real_estate_model' ) # use the model to make predictions result = Predictor ( name = 'home_rentals_price' ) . predict ( when_data = { 'number_of_rooms' : 1 , 'initial_price' : 1222 , 'sqft' : 1190 }) # The result will be an array containing predictions for each data point (in this case only one), a confidence for said prediction and a few other extra informations print ( 'The predicted price is between $ {price} with {conf} confidence' . format ( price = result [ 0 ] . explanation [ 'rental_price' ][ 'confidence_interval' ], conf = result [ 0 ] . explanation [ 'rental_price' ][ 'confidence' ])) Notes About the Learning The first thing we can do is to learn from the csv file. Learn in the scope of MindsDB is to let it figure out a neural network that can best learn from this data as well as train and test such a model given the data that we have. When you run this script, note that it will start logging various information about the data and about the training process. This information can be useful in allowing you to figure out which parts of your data are of low quality or might contain erroneous values. About getting predictions from the model Please note the when_data argument, in this case assuming we only know that: 'number_of_rooms': 1, 'initial_price':1222 'sqft': 1190 So, as long as the columns that you pass in the when_data statement exists in the data it learned from it will work (see columns in home_rentals.csv ). Running online You can follow this example on Google Colab.","title":"Starter Example"},{"location":"tutorials/BasicExample/#goal","text":"The goal is to be able to predict the best rental_price for new properties given the information that we have in home_rentals.csv.","title":"Goal"},{"location":"tutorials/BasicExample/#learning","text":"from mindsdb import Predictor # We tell the Predictor what column or key we want to learn and from what data Predictor ( name = 'real_estate_model' ) . learn ( from_data = \"https://s3.eu-west-2.amazonaws.com/mindsdb-example-data/home_rentals.csv\" , # the path to the file where we can learn from, (note: can be url) to_predict = 'rental_price' , # the column we want to learn to predict given all the data in the file ) Note : that the argument from_data can be a path to a json, csv (or other separators), excel given as a file or as a URL, or a pandas Dataframe","title":"Learning"},{"location":"tutorials/BasicExample/#predicting","text":"mdb = mindsdb . Predictor ( name = 'real_estate_model' ) # use the model to make predictions result = Predictor ( name = 'home_rentals_price' ) . predict ( when_data = { 'number_of_rooms' : 1 , 'initial_price' : 1222 , 'sqft' : 1190 }) # The result will be an array containing predictions for each data point (in this case only one), a confidence for said prediction and a few other extra informations print ( 'The predicted price is between $ {price} with {conf} confidence' . format ( price = result [ 0 ] . explanation [ 'rental_price' ][ 'confidence_interval' ], conf = result [ 0 ] . explanation [ 'rental_price' ][ 'confidence' ]))","title":"Predicting"},{"location":"tutorials/BasicExample/#notes","text":"","title":"Notes"},{"location":"tutorials/BasicExample/#about-the-learning","text":"The first thing we can do is to learn from the csv file. Learn in the scope of MindsDB is to let it figure out a neural network that can best learn from this data as well as train and test such a model given the data that we have. When you run this script, note that it will start logging various information about the data and about the training process. This information can be useful in allowing you to figure out which parts of your data are of low quality or might contain erroneous values.","title":"About the Learning"},{"location":"tutorials/BasicExample/#about-getting-predictions-from-the-model","text":"Please note the when_data argument, in this case assuming we only know that: 'number_of_rooms': 1, 'initial_price':1222 'sqft': 1190 So, as long as the columns that you pass in the when_data statement exists in the data it learned from it will work (see columns in home_rentals.csv ).","title":"About getting predictions from the model"},{"location":"tutorials/BasicExample/#running-online","text":"You can follow this example on Google Colab.","title":"Running online"},{"location":"tutorials/ChurnReduction/","text":"Industry Department Role Telecomunications Marketing Marketing Lead Processed Dataset Customer churn or customer turnover is the loss of clients or customers. Telecommunication companies often use customer attrition analysis and customer attrition rates as one of their key business metrics because the cost of retaining an existing customer is far less than acquiring a new one. Use churn prediction models that predict customer churn by assessing their propensity of risk to churn. CreditScore Geography Gender Age Tenure Balance NumOfProducts HasCrCard IsActiveMember EstimatedSalary Exited 502 France Female 42 8 159661 3 1 0 113932 1 822 France Male 50 7 0 2 1 1 10062.8 0 549 France Female 25 5 0 2 0 0 190858 0 587 Spain Male 45 6 0 1 0 0 158685 0 582 Germany Male 41 6 70349.5 2 0 1 178074 0 556 France Female 61 2 117419 1 1 1 94153.8 0 550 Germany Male 38 2 103391 1 0 1 90878.1 0 Click to expand Features Informations: 1. customerIDCustomer ID 2. gender Whether the customer is a male or a female 3. SeniorCitizen Whether the customer is a senior citizen or not (1, 0) 4. Partner Whether the customer has a partner or not (Yes, No) 5. Dependents Whether the customer has dependents or not (Yes, No) tenureNumber of months the customer has stayed with the company 6. PhoneService Whether the customer has a phone service or not (Yes, No) 7. MultipleLines Whether the customer has multiple lines or not (Yes, No, No phone service) 8. InternetServiceCustomer\u2019s internet service provider (DSL, Fiber optic, No) 9. OnlineSecurity Whether the customer has online security or not (Yes, No, No internet service) 10. OnlineBackup Whether the customer has online backup or not (Yes, No, No internet service) 11. DeviceProtection Whether the customer has device protection or not (Yes, No, No internet service) 12. TechSupport Whether the customer has tech support or not (Yes, No, No internet service) 13. StreamingTV Whether the customer has streaming TV or not (Yes, No, No internet service) 14. StreamingMovies Whether the customer has streaming movies or not (Yes, No, No internet service) 15. ContractThe contract term of the customer (Month-to-month, One year, Two year) 16. PaperlessBilling Whether the customer has paperless billing or not (Yes, No) 17. PaymentMethodThe customer\u2019s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic)) 18. MonthlyChargesThe amount charged to the customer monthly 19. TotalChargesThe total amount charged to the customer 20 Churn Whether the customer churned or not (Yes or No) MindsDB Code example import mindsdb import pandas as pd from sklearn.metrics import accuracy_score def run (): mdb = mindsdb . Predictor ( name = 'employee_retention_model' ) mdb . learn ( from_data = 'dataset/train.csv' , to_predict = 'Churn' ) test_df = pd . read_csv ( 'dataset/test.csv' ) predictions = mdb . predict ( when_data = 'dataset/test.csv' ) predicted_val = [ x . explanation [ 'Churn' ][ 'predicted_value' ] for x in predictions ] real_val = list ( map ( str , list ( test_df [ 'Churn' ]))) accuracy = accuracy_score ( real_val , predicted_val ) #show additional info for each transaction row additional_info = [ x . explanation for x in predictions ] return { 'accuracy' : accuracy , 'accuracy_function' : 'accuracy_score' , 'backend' : backend , 'prediction_per_row' : additional_info } if __name__ == '__main__' : result = run () print ( result ) Mindsdb accuracy Accuraccy Backend Last run MindsDB Version Latest Version 0.7659574468085106 Lightwood 17 April 2020","title":"Customer Churn Reduction"},{"location":"tutorials/ChurnReduction/#processed-dataset","text":"","title":"Processed Dataset"},{"location":"tutorials/ChurnReduction/#_1","text":"Customer churn or customer turnover is the loss of clients or customers. Telecommunication companies often use customer attrition analysis and customer attrition rates as one of their key business metrics because the cost of retaining an existing customer is far less than acquiring a new one. Use churn prediction models that predict customer churn by assessing their propensity of risk to churn. CreditScore Geography Gender Age Tenure Balance NumOfProducts HasCrCard IsActiveMember EstimatedSalary Exited 502 France Female 42 8 159661 3 1 0 113932 1 822 France Male 50 7 0 2 1 1 10062.8 0 549 France Female 25 5 0 2 0 0 190858 0 587 Spain Male 45 6 0 1 0 0 158685 0 582 Germany Male 41 6 70349.5 2 0 1 178074 0 556 France Female 61 2 117419 1 1 1 94153.8 0 550 Germany Male 38 2 103391 1 0 1 90878.1 0 Click to expand Features Informations: 1. customerIDCustomer ID 2. gender Whether the customer is a male or a female 3. SeniorCitizen Whether the customer is a senior citizen or not (1, 0) 4. Partner Whether the customer has a partner or not (Yes, No) 5. Dependents Whether the customer has dependents or not (Yes, No) tenureNumber of months the customer has stayed with the company 6. PhoneService Whether the customer has a phone service or not (Yes, No) 7. MultipleLines Whether the customer has multiple lines or not (Yes, No, No phone service) 8. InternetServiceCustomer\u2019s internet service provider (DSL, Fiber optic, No) 9. OnlineSecurity Whether the customer has online security or not (Yes, No, No internet service) 10. OnlineBackup Whether the customer has online backup or not (Yes, No, No internet service) 11. DeviceProtection Whether the customer has device protection or not (Yes, No, No internet service) 12. TechSupport Whether the customer has tech support or not (Yes, No, No internet service) 13. StreamingTV Whether the customer has streaming TV or not (Yes, No, No internet service) 14. StreamingMovies Whether the customer has streaming movies or not (Yes, No, No internet service) 15. ContractThe contract term of the customer (Month-to-month, One year, Two year) 16. PaperlessBilling Whether the customer has paperless billing or not (Yes, No) 17. PaymentMethodThe customer\u2019s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic)) 18. MonthlyChargesThe amount charged to the customer monthly 19. TotalChargesThe total amount charged to the customer 20 Churn Whether the customer churned or not (Yes or No)","title":""},{"location":"tutorials/ChurnReduction/#mindsdb-code-example","text":"import mindsdb import pandas as pd from sklearn.metrics import accuracy_score def run (): mdb = mindsdb . Predictor ( name = 'employee_retention_model' ) mdb . learn ( from_data = 'dataset/train.csv' , to_predict = 'Churn' ) test_df = pd . read_csv ( 'dataset/test.csv' ) predictions = mdb . predict ( when_data = 'dataset/test.csv' ) predicted_val = [ x . explanation [ 'Churn' ][ 'predicted_value' ] for x in predictions ] real_val = list ( map ( str , list ( test_df [ 'Churn' ]))) accuracy = accuracy_score ( real_val , predicted_val ) #show additional info for each transaction row additional_info = [ x . explanation for x in predictions ] return { 'accuracy' : accuracy , 'accuracy_function' : 'accuracy_score' , 'backend' : backend , 'prediction_per_row' : additional_info } if __name__ == '__main__' : result = run () print ( result )","title":"MindsDB Code example"},{"location":"tutorials/ChurnReduction/#mindsdb-accuracy","text":"Accuraccy Backend Last run MindsDB Version Latest Version 0.7659574468085106 Lightwood 17 April 2020","title":"Mindsdb accuracy"},{"location":"tutorials/CreditScoring/","text":"Industry Department Role Financial Services Finance Business executive Processed Dataset The German Credit dataset is a publically available from the UCI Machine Learning Repository . The dataset contains data on 20 variables and the classification whether an applicant is considered a Good or a Bad credit risks. over_draft credit_usage credit_history purpose current_balance Average_Credit_Balance employment location personal_status other_parties residence_since property_magnitude cc_age other_payment_plans housing existing_credits job num_dependents own_telephone foreign_worker class no checking 9 existing paid education 3832 no known savings >=7 1 male single none 4 real estate 64 none own 1 unskilled resident 1 none yes good 0<=X<200 12 existing paid radio/tv 1092 <100 1<=X<4 4 female div/dep/mar guarantor 4 real estate 49 none own 2 skilled 1 yes yes good <0 12 critical/other existing credit furniture/equipment 2246 <100 >=7 3 male single none 3 life insurance 60 none own 2 skilled 1 none yes bad 0<=X<200 24 existing paid furniture/equipment 4057 <100 4<=X<7 3 male div/sep none 3 car 43 none own 1 skilled 1 yes yes bad no checking 24 existing paid furniture/equipment 929 no known savings 4<=X<7 4 male single none 2 car 31 stores own 1 skilled 1 yes yes good no checking 15 critical/other existing credit furniture/equipment 2788 <100 4<=X<7 2 female div/dep/mar co applicant 3 car 24 bank own 2 skilled 1 none yes good <0 36 all paid furniture/equipment 2746 <100 >=7 4 male single none 4 car 31 bank own 1 skilled 1 none yes bad Click to expand Features Informations: * Attribute 1: (qualitative) * Status of existing checking account * A11 : ... < 0 DM * A12 : 0 <= ... < 200 DM * A13 : ... >= 200 DM / salary assignments for at least 1 year * A14 : no checking account * Attribute 2: (numerical) * Duration in month * Attribute 3: (qualitative) * Credit history * A30 : no credits taken/ all credits paid back duly * A31 : all credits at this bank paid back duly * A32 : existing credits paid back duly till now * A33 : delay in paying off in the past * A34 : critical account/ other credits existing (not at this bank) * Attribute 4: (qualitative) * Purpose * A40 : car (new) * A41 : car (used) * A42 : furniture/equipment * A43 : radio/television * A44 : domestic appliances * A45 : repairs * A46 : education * A47 : (vacation - does not exist?) * A48 : retraining * A49 : business * A410 : others * Attribute 5: (numerical) * Credit amount * Attibute 6: (qualitative) * Savings account/bonds * A61 : ... < 100 DM * A62 : 100 <= ... < 500 DM * A63 : 500 <= ... < 1000 DM * A64 : .. >= 1000 DM * A65 : unknown/ no savings account * Attribute 7: (qualitative) * Present employment since * A71 : unemployed * A72 : ... < 1 year * A73 : 1 <= ... < 4 years * A74 : 4 <= ... < 7 years * A75 : .. >= 7 years * Attribute 8: (numerical) * Installment rate in percentage of disposable income * Attribute 9: (qualitative) * Personal status and sex * A91 : male : divorced/separated * A92 : female : divorced/separated/married * A93 : male : single * A94 : male : married/widowed * A95 : female : single * Attribute 10: (qualitative) * Other debtors / guarantors * A101 : none * A102 : co-applicant * A103 : guarantor * Attribute 11: (numerical) * Present residence since * Attribute 12: (qualitative) * Property * A121 : real estate * A122 : if not A121 : building society savings agreement/ life insurance * A123 : if not A121/A122 : car or other, not in attribute 6 * A124 : unknown / no property * Attribute 13: (numerical) * Age in years * Attribute 14: (qualitative) * Other installment plans * A141 : bank * A142 : stores * A143 : none * Attribute 15: (qualitative) * Housing * A151 : rent * A152 : own * A153 : for free * Attribute 16: (numerical) * Number of existing credits at this bank * Attribute 17: (qualitative) * Job * A171 : unemployed/ unskilled - non-resident * A172 : unskilled - resident * A173 : skilled employee / official * A174 : management/ self-employed/ * highly qualified employee/ officer * Attribute 18: (numerical) * Number of people being liable to provide maintenance for * Attribute 19: (qualitative) * Telephone * A191 : none * A192 : yes, registered under the customers name * Attribute 20: (qualitative) * foreign worker * A201 : yes * A202 : no MindsDB Code example from mindsdb import Predictor import pandas as pd from sklearn.metrics import balanced_accuracy_score , confusion_matrix def run ( sample = False ): mdb = Predictor ( name = 'german_data' ) mdb . learn ( to_predict = 'class' , from_data = 'processed_data/train.csv' ) predictions = mdb . predict ( when_data = 'processed_data/test.csv' ) predicted_val = [ x . explanation [ 'class' ][ 'predicted_value' ] for x in predictions ] real_val = list ( pd . read_csv ( 'processed_data/test.csv' )[ 'class' ]) accuracy = balanced_accuracy_score ( real_val , predicted_val ) #show additional info for each transaction row additional_info = [ x . explanation for x in predictions ] return { 'accuracy' : accuracy , 'accuracy_function' : 'balanced_accuracy_score' , 'backend' : backend , 'single_row_predictions' : additional_info } # Run as main if __name__ == '__main__' : print ( run ()) Mindsdb accuracy Accuracy Backend Last run MindsDB Version Latest Version 0.64880952380 Lightwood 15 April 2020","title":"Credit Scoring"},{"location":"tutorials/CreditScoring/#processed-dataset","text":"","title":"Processed Dataset"},{"location":"tutorials/CreditScoring/#_1","text":"The German Credit dataset is a publically available from the UCI Machine Learning Repository . The dataset contains data on 20 variables and the classification whether an applicant is considered a Good or a Bad credit risks. over_draft credit_usage credit_history purpose current_balance Average_Credit_Balance employment location personal_status other_parties residence_since property_magnitude cc_age other_payment_plans housing existing_credits job num_dependents own_telephone foreign_worker class no checking 9 existing paid education 3832 no known savings >=7 1 male single none 4 real estate 64 none own 1 unskilled resident 1 none yes good 0<=X<200 12 existing paid radio/tv 1092 <100 1<=X<4 4 female div/dep/mar guarantor 4 real estate 49 none own 2 skilled 1 yes yes good <0 12 critical/other existing credit furniture/equipment 2246 <100 >=7 3 male single none 3 life insurance 60 none own 2 skilled 1 none yes bad 0<=X<200 24 existing paid furniture/equipment 4057 <100 4<=X<7 3 male div/sep none 3 car 43 none own 1 skilled 1 yes yes bad no checking 24 existing paid furniture/equipment 929 no known savings 4<=X<7 4 male single none 2 car 31 stores own 1 skilled 1 yes yes good no checking 15 critical/other existing credit furniture/equipment 2788 <100 4<=X<7 2 female div/dep/mar co applicant 3 car 24 bank own 2 skilled 1 none yes good <0 36 all paid furniture/equipment 2746 <100 >=7 4 male single none 4 car 31 bank own 1 skilled 1 none yes bad Click to expand Features Informations: * Attribute 1: (qualitative) * Status of existing checking account * A11 : ... < 0 DM * A12 : 0 <= ... < 200 DM * A13 : ... >= 200 DM / salary assignments for at least 1 year * A14 : no checking account * Attribute 2: (numerical) * Duration in month * Attribute 3: (qualitative) * Credit history * A30 : no credits taken/ all credits paid back duly * A31 : all credits at this bank paid back duly * A32 : existing credits paid back duly till now * A33 : delay in paying off in the past * A34 : critical account/ other credits existing (not at this bank) * Attribute 4: (qualitative) * Purpose * A40 : car (new) * A41 : car (used) * A42 : furniture/equipment * A43 : radio/television * A44 : domestic appliances * A45 : repairs * A46 : education * A47 : (vacation - does not exist?) * A48 : retraining * A49 : business * A410 : others * Attribute 5: (numerical) * Credit amount * Attibute 6: (qualitative) * Savings account/bonds * A61 : ... < 100 DM * A62 : 100 <= ... < 500 DM * A63 : 500 <= ... < 1000 DM * A64 : .. >= 1000 DM * A65 : unknown/ no savings account * Attribute 7: (qualitative) * Present employment since * A71 : unemployed * A72 : ... < 1 year * A73 : 1 <= ... < 4 years * A74 : 4 <= ... < 7 years * A75 : .. >= 7 years * Attribute 8: (numerical) * Installment rate in percentage of disposable income * Attribute 9: (qualitative) * Personal status and sex * A91 : male : divorced/separated * A92 : female : divorced/separated/married * A93 : male : single * A94 : male : married/widowed * A95 : female : single * Attribute 10: (qualitative) * Other debtors / guarantors * A101 : none * A102 : co-applicant * A103 : guarantor * Attribute 11: (numerical) * Present residence since * Attribute 12: (qualitative) * Property * A121 : real estate * A122 : if not A121 : building society savings agreement/ life insurance * A123 : if not A121/A122 : car or other, not in attribute 6 * A124 : unknown / no property * Attribute 13: (numerical) * Age in years * Attribute 14: (qualitative) * Other installment plans * A141 : bank * A142 : stores * A143 : none * Attribute 15: (qualitative) * Housing * A151 : rent * A152 : own * A153 : for free * Attribute 16: (numerical) * Number of existing credits at this bank * Attribute 17: (qualitative) * Job * A171 : unemployed/ unskilled - non-resident * A172 : unskilled - resident * A173 : skilled employee / official * A174 : management/ self-employed/ * highly qualified employee/ officer * Attribute 18: (numerical) * Number of people being liable to provide maintenance for * Attribute 19: (qualitative) * Telephone * A191 : none * A192 : yes, registered under the customers name * Attribute 20: (qualitative) * foreign worker * A201 : yes * A202 : no","title":""},{"location":"tutorials/CreditScoring/#mindsdb-code-example","text":"from mindsdb import Predictor import pandas as pd from sklearn.metrics import balanced_accuracy_score , confusion_matrix def run ( sample = False ): mdb = Predictor ( name = 'german_data' ) mdb . learn ( to_predict = 'class' , from_data = 'processed_data/train.csv' ) predictions = mdb . predict ( when_data = 'processed_data/test.csv' ) predicted_val = [ x . explanation [ 'class' ][ 'predicted_value' ] for x in predictions ] real_val = list ( pd . read_csv ( 'processed_data/test.csv' )[ 'class' ]) accuracy = balanced_accuracy_score ( real_val , predicted_val ) #show additional info for each transaction row additional_info = [ x . explanation for x in predictions ] return { 'accuracy' : accuracy , 'accuracy_function' : 'balanced_accuracy_score' , 'backend' : backend , 'single_row_predictions' : additional_info } # Run as main if __name__ == '__main__' : print ( run ())","title":"MindsDB Code example"},{"location":"tutorials/CreditScoring/#mindsdb-accuracy","text":"Accuracy Backend Last run MindsDB Version Latest Version 0.64880952380 Lightwood 15 April 2020","title":"Mindsdb accuracy"},{"location":"tutorials/CustomerLifetimeValue/","text":"Industry Department Role Retail & Online Marketing Marketing Lead Processed Dataset This is a dataset for binary sentiment classification containing a set of 25,000 highly popular movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. Features informations review sentiment MindsDB Code example import mindsdb from sklearn.metrics import accuracy_score predictor = mindsdb . Predictor ( name = 'movie_sentiment_predictor' ) predictor . learn ( from_data = 'train.tsv' , to_predict = [ 'sentiment' ]) accuracy_data = predictions . test ( 'test.tsv' , accuracy_score ) accuracy_pct = accuracy_data [ 'sentiment_accuracy' ] * 100 print ( f 'Accuracy of { accuracy_pct } % !' ) Mindsdb accuracy Accuracy Backend Last run MindsDB Version Latest Version 0.8573 Lightwood 06 February 2020","title":"Custiner Lifetime Value Optimization"},{"location":"tutorials/CustomerLifetimeValue/#processed-dataset","text":"","title":"Processed Dataset"},{"location":"tutorials/CustomerLifetimeValue/#_1","text":"This is a dataset for binary sentiment classification containing a set of 25,000 highly popular movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided.","title":""},{"location":"tutorials/CustomerLifetimeValue/#features-informations","text":"review sentiment","title":"Features informations"},{"location":"tutorials/CustomerLifetimeValue/#mindsdb-code-example","text":"import mindsdb from sklearn.metrics import accuracy_score predictor = mindsdb . Predictor ( name = 'movie_sentiment_predictor' ) predictor . learn ( from_data = 'train.tsv' , to_predict = [ 'sentiment' ]) accuracy_data = predictions . test ( 'test.tsv' , accuracy_score ) accuracy_pct = accuracy_data [ 'sentiment_accuracy' ] * 100 print ( f 'Accuracy of { accuracy_pct } % !' )","title":"MindsDB Code example"},{"location":"tutorials/CustomerLifetimeValue/#mindsdb-accuracy","text":"Accuracy Backend Last run MindsDB Version Latest Version 0.8573 Lightwood 06 February 2020","title":"Mindsdb accuracy"},{"location":"tutorials/FraudDetection/","text":"Industry Department Role Retail & Online Finance Business executive Processed Dataset The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions. The goal is to identify fraudulent credit card transactions. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise. Time V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 V21 V22 V23 V24 V25 V26 V27 V28 Amount Class 1 -1.35835 -1.34016 1.77321 0.37978 -0.503198 1.8005 0.791461 0.247676 -1.51465 0.207643 0.624501 0.0660837 0.717293 -0.165946 2.34586 -2.89008 1.10997 -0.121359 -2.26186 0.52498 0.247998 0.771679 0.909412 -0.689281 -0.327642 -0.139097 -0.0553528 -0.0597518 378.66 0 4 1.22966 0.141004 0.0453708 1.20261 0.191881 0.272708 -0.005159 0.0812129 0.46496 -0.0992543 -1.41691 -0.153826 -0.751063 0.167372 0.0501436 -0.443587 0.00282051 -0.611987 -0.045575 -0.219633 -0.167716 -0.27071 -0.154104 -0.780055 0.750137 -0.257237 0.0345074 0.00516777 4.99 0 11 1.06937 0.287722 0.828613 2.71252 -0.178398 0.337544 -0.0967169 0.115982 -0.221083 0.46023 -0.773657 0.323387 -0.0110759 -0.178485 -0.655564 -0.199925 0.124005 -0.980496 -0.982916 -0.153197 -0.0368755 0.0744124 -0.0714074 0.104744 0.548265 0.104094 0.0214911 0.0212933 27.5 0 14 -5.40126 -5.45015 1.1863 1.73624 3.04911 -1.76341 -1.55974 0.160842 1.23309 0.345173 0.91723 0.970117 -0.266568 -0.47913 -0.526609 0.472004 -0.725481 0.0750814 -0.406867 -2.19685 -0.5036 0.98446 2.45859 0.0421189 -0.481631 -0.621272 0.392053 0.949594 46.8 0 29 1.11088 0.168717 0.517144 1.32541 -0.191573 0.0195037 -0.0318491 0.11762 0.0176647 0.0448648 1.34507 1.28634 -0.252267 0.274458 -0.810394 -0.587005 0.0874511 -0.550474 -0.154749 -0.19012 -0.0377087 0.0957015 -0.0481976 0.232115 0.606201 -0.342097 0.0367696 0.00747996 6.54 0 33 -0.607877 1.03135 1.74045 1.23211 0.418592 0.119168 0.850893 -0.176267 -0.243501 0.148455 -0.387003 0.398299 0.481917 -0.365439 0.235545 -1.34781 0.504648 -0.798405 0.75971 0.254325 -0.0873292 0.258315 -0.264775 0.118282 0.173508 -0.217041 0.0943119 -0.0330413 14.8 0 35 1.3864 -0.794209 0.778224 -0.864708 -1.06413 0.351296 -1.19145 0.0526856 -0.304404 0.576517 -1.63111 0.0425595 2.0479 -0.739338 1.45622 -0.27205 -0.932007 1.92653 -0.659939 -0.273033 -0.228727 -0.123522 -0.131025 -0.929668 0.181379 1.19493 0.000531332 0.0199106 30.9 0 Click to expand Features Informations: * Time Number of seconds elapsed between this transaction and the first transaction in the dataset * V1may be result of a PCA Dimensionality reduction to protect user identities and sensitive features(v1-v28) * V2 * V3 * V4 * V5 * V6 * V7 * V8 * V9 * V10 * V11 * V12 * V13 * V14 * V15 * V16 * V17 * V18 * V19 * V20 * V21 * V22 * V23 * V24 * V25 * V26 * V27 * V28abc * AmountTransaction amount * Class1 for fraudulent transactions, 0 otherwise MindsDB Code example import mindsdb import pandas as pd from sklearn.metrics import balanced_accuracy_score def run (): mdb = mindsdb . Predictor ( name = 'cc_fraud' ) mdb . learn ( from_data = 'processed_data/train.csv' , to_predict = 'Class' ) predictions = mdb . predict ( when_data = 'processed_data/test.csv' ) pred_val = [ int ( x [ 'Class' ]) for x in predictions ] real_val = [ int ( x ) for x in list ( pd . read_csv ( 'processed_data/test.csv' ))[ 'Class' ])] accuracy = balanced_accuracy_score ( real_val , pred_val ) #show additional info for each transaction row additional_info = [ x . explanation for x in predictions ] return { 'accuracy' : accuracy , 'backend' : backend , 'additional info' : additional_info } # Run as main if __name__ == '__main__' : print ( run ()) Mindsdb accuracy Accuracy Backend Last run MindsDB Version Latest Version 0.921724518459069 Lightwood 16 April 2020","title":"Fraud Detection"},{"location":"tutorials/FraudDetection/#processed-dataset","text":"","title":"Processed Dataset"},{"location":"tutorials/FraudDetection/#_1","text":"The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions. The goal is to identify fraudulent credit card transactions. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise. Time V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 V21 V22 V23 V24 V25 V26 V27 V28 Amount Class 1 -1.35835 -1.34016 1.77321 0.37978 -0.503198 1.8005 0.791461 0.247676 -1.51465 0.207643 0.624501 0.0660837 0.717293 -0.165946 2.34586 -2.89008 1.10997 -0.121359 -2.26186 0.52498 0.247998 0.771679 0.909412 -0.689281 -0.327642 -0.139097 -0.0553528 -0.0597518 378.66 0 4 1.22966 0.141004 0.0453708 1.20261 0.191881 0.272708 -0.005159 0.0812129 0.46496 -0.0992543 -1.41691 -0.153826 -0.751063 0.167372 0.0501436 -0.443587 0.00282051 -0.611987 -0.045575 -0.219633 -0.167716 -0.27071 -0.154104 -0.780055 0.750137 -0.257237 0.0345074 0.00516777 4.99 0 11 1.06937 0.287722 0.828613 2.71252 -0.178398 0.337544 -0.0967169 0.115982 -0.221083 0.46023 -0.773657 0.323387 -0.0110759 -0.178485 -0.655564 -0.199925 0.124005 -0.980496 -0.982916 -0.153197 -0.0368755 0.0744124 -0.0714074 0.104744 0.548265 0.104094 0.0214911 0.0212933 27.5 0 14 -5.40126 -5.45015 1.1863 1.73624 3.04911 -1.76341 -1.55974 0.160842 1.23309 0.345173 0.91723 0.970117 -0.266568 -0.47913 -0.526609 0.472004 -0.725481 0.0750814 -0.406867 -2.19685 -0.5036 0.98446 2.45859 0.0421189 -0.481631 -0.621272 0.392053 0.949594 46.8 0 29 1.11088 0.168717 0.517144 1.32541 -0.191573 0.0195037 -0.0318491 0.11762 0.0176647 0.0448648 1.34507 1.28634 -0.252267 0.274458 -0.810394 -0.587005 0.0874511 -0.550474 -0.154749 -0.19012 -0.0377087 0.0957015 -0.0481976 0.232115 0.606201 -0.342097 0.0367696 0.00747996 6.54 0 33 -0.607877 1.03135 1.74045 1.23211 0.418592 0.119168 0.850893 -0.176267 -0.243501 0.148455 -0.387003 0.398299 0.481917 -0.365439 0.235545 -1.34781 0.504648 -0.798405 0.75971 0.254325 -0.0873292 0.258315 -0.264775 0.118282 0.173508 -0.217041 0.0943119 -0.0330413 14.8 0 35 1.3864 -0.794209 0.778224 -0.864708 -1.06413 0.351296 -1.19145 0.0526856 -0.304404 0.576517 -1.63111 0.0425595 2.0479 -0.739338 1.45622 -0.27205 -0.932007 1.92653 -0.659939 -0.273033 -0.228727 -0.123522 -0.131025 -0.929668 0.181379 1.19493 0.000531332 0.0199106 30.9 0 Click to expand Features Informations: * Time Number of seconds elapsed between this transaction and the first transaction in the dataset * V1may be result of a PCA Dimensionality reduction to protect user identities and sensitive features(v1-v28) * V2 * V3 * V4 * V5 * V6 * V7 * V8 * V9 * V10 * V11 * V12 * V13 * V14 * V15 * V16 * V17 * V18 * V19 * V20 * V21 * V22 * V23 * V24 * V25 * V26 * V27 * V28abc * AmountTransaction amount * Class1 for fraudulent transactions, 0 otherwise","title":""},{"location":"tutorials/FraudDetection/#mindsdb-code-example","text":"import mindsdb import pandas as pd from sklearn.metrics import balanced_accuracy_score def run (): mdb = mindsdb . Predictor ( name = 'cc_fraud' ) mdb . learn ( from_data = 'processed_data/train.csv' , to_predict = 'Class' ) predictions = mdb . predict ( when_data = 'processed_data/test.csv' ) pred_val = [ int ( x [ 'Class' ]) for x in predictions ] real_val = [ int ( x ) for x in list ( pd . read_csv ( 'processed_data/test.csv' ))[ 'Class' ])] accuracy = balanced_accuracy_score ( real_val , pred_val ) #show additional info for each transaction row additional_info = [ x . explanation for x in predictions ] return { 'accuracy' : accuracy , 'backend' : backend , 'additional info' : additional_info } # Run as main if __name__ == '__main__' : print ( run ())","title":"MindsDB Code example"},{"location":"tutorials/FraudDetection/#mindsdb-accuracy","text":"Accuracy Backend Last run MindsDB Version Latest Version 0.921724518459069 Lightwood 16 April 2020","title":"Mindsdb accuracy"},{"location":"tutorials/HotelBooking/","text":"Industry Department Role Travel Operations Business executive Processed Dataset This data set contains booking information for a city hotel and a resort hotel, and includes information such as when the booking was made, length of stay, the number of adults, children, and/or babies, and the number of available parking spaces, among other things. hotel is_canceled lead_time arrival_date_year arrival_date_month arrival_date_week_number arrival_date_day_of_month stays_in_weekend_nights stays_in_week_nights adults children babies meal country market_segment distribution_channel is_repeated_guest previous_cancellations previous_bookings_not_canceled reserved_room_type assigned_room_type booking_changes deposit_type agent company days_in_waiting_list customer_type adr required_car_parking_spaces total_of_special_requests reservation_status reservation_status_date Resort Hotel 0 342 2015 July 27 1 0 0 2 0 0 BB PRT Direct Direct 0 0 0 C C 3 No Deposit nan nan 0 Transient 0 0 0 Check-Out 2015-07-01 Resort Hotel 0 737 2015 July 27 1 0 0 2 0 0 BB PRT Direct Direct 0 0 0 C C 4 No Deposit nan nan 0 Transient 0 0 0 Check-Out 2015-07-01 Resort Hotel 0 7 2015 July 27 1 0 1 1 0 0 BB GBR Direct Direct 0 0 0 A C 0 No Deposit nan nan 0 Transient 75 0 0 Check-Out 2015-07-02 Resort Hotel 0 13 2015 July 27 1 0 1 1 0 0 BB GBR Corporate Corporate 0 0 0 A A 0 No Deposit 304 nan 0 Transient 75 0 0 Check-Out 2015-07-02 Resort Hotel 0 14 2015 July 27 1 0 2 2 0 0 BB GBR Online TA TA/TO 0 0 0 A A 0 No Deposit 240 nan 0 Transient 98 0 1 Check-Out 2015-07-03 Resort Hotel 0 14 2015 July 27 1 0 2 2 0 0 BB GBR Online TA TA/TO 0 0 0 A A 0 No Deposit 240 nan 0 Transient 98 0 1 Check-Out 2015-07-03 Resort Hotel 0 0 2015 July 27 1 0 2 2 0 0 BB PRT Direct Direct 0 0 0 C C 0 No Deposit nan nan 0 Transient 107 0 0 Check-Out 2015-07-03 Click to expand Features Informations: 1. hotel Hotel (H1 = Resort Hotel or H2 = City Hotel) 2. is_canceled Value indicating if the booking was canceled (1) or not (0) lead_timeNumber of days that elapsed between the entering date of the booking into the PMS and the arrival date 3. arrival_date_year Year of arrival date 4. arrival_date_month Month of arrival date 5. arrival_date_week_number Week number of year for arrival date 6. arrival_date_day_of_month Day of arrival date 7. stays_in_weekend_nights Number of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel 8. stays_in_week_nightsNumber of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel 9. adultsNumber of adults 10. childrenNumber of children 11. babiesNumber of babies 12. mealType of meal booked. Categories are presented in standard hospitality meal packages: Undefined/SC \u2013 no meal package; BB \u2013 Bed & Breakfast; HB \u2013 Half board (breakfast and one other meal \u2013 usually dinner); FB \u2013 Full board (breakfast, lunch and dinner) 13. countryCountry of origin. Categories are represented in the ISO 3155\u20133:2013 format 14. market_segment Market segment designation. In categories, the term \u201cTA\u201d means \u201cTravel Agents\u201d and \u201cTO\u201d means \u201cTour Operators\u201d 15. distribution_channel Booking distribution channel. The term \u201cTA\u201d means \u201cTravel Agents\u201d and \u201cTO\u201d means \u201cTour Operators\u201d 16. is_repeated_guestValue indicating if the booking name was from a repeated guest (1) or not (0) 17. previous_cancellationsNumber of previous bookings that were cancelled by the customer prior to the current booking 18. previous_bookings_not_canceledNumber of previous bookings not cancelled by the customer prior to the current booking 19. reserved_room_typeCode of room type reserved. Code is presented instead of designation for anonymity reasons. 20. assigned_room_type Code for the type of room assigned to the booking. Sometimes the assigned room type differs from the reserved room type due to hotel operation reasons (e.g. overbooking) or by customer request. Code is presented instead of designation for anonymity reasons. 21. booking_changes Number of changes/amendments made to the booking from the moment the booking was entered on the PMS until the moment of check-in or cancellation 22. deposit_type Indication on if the customer made a deposit to guarantee the booking. This variable can assume three categories: No Deposit \u2013 no deposit was made; Non Refund \u2013 a deposit was made in the value of the total stay cost; Refundable \u2013 a deposit was made with a value under the total cost of stay. 23. agentID of the travel agency that made the booking 24. companyID of the company/entity that made the booking or responsible for paying the booking. ID is presented instead of designation for anonymity reasons 25. days_in_waiting_list Number of days the booking was in the waiting list before it was confirmed to the customer 26. customer_type Type of booking, assuming one of four categories: Contract - when the booking has an allotment or other type of contract associated to it; Group \u2013 when the booking is associated to a group; Transient \u2013 when the booking is not part of a group or contract, and is not associated to other 27. transient booking; Transient-party \u2013 when the booking is transient, but is associated to at least other transient booking adrAverage Daily Rate as defined by dividing the sum of all lodging transactions by the total number of staying nights 28. required_car_parking_spaces Number of car parking spaces required by the customer 29. total_of_special_requests Number of special requests made by the customer (e.g. twin bed or high floor) 30. reservation_status Reservation last status, assuming one of three categories: Canceled \u2013 booking was canceled by the customer; Check-Out \u2013 customer has checked in but already departed; No-Show \u2013 customer did not check-in and did inform the hotel of the reason why 31. reservation_status_date Date at which the last status was set. This variable can be used in conjunction with the ReservationStatus to understand when was the booking canceled or when did the customer checked-out of the hotel MindsDB Code example import mindsdb import sys import pandas as pd from sklearn.metrics import balanced_accuracy_score def run (): mdb = mindsdb . Predictor ( name = 'hotel_booking' ) mdb . learn ( from_data = 'dataset/train.csv' , to_predict = 'is_canceled' ) test_df = pd . read_csv ( 'dataset/test.csv' ) predictions = mdb . predict ( when_data = 'dataset/test.csv' ) results = [ str ( x [ 'is_canceled' ]) for x in predictions ] real = list ( map ( str , list ( test_df [ 'is_canceled' ]))) accuracy = balanced_accuracy_score ( real , results ) #show additional info for each transaction row additional_info = [ x . explanation for x in predictions ] return { 'accuracy' : accuracy , 'accuracy_function' : 'balanced_accuracy_score' , 'backend' : backend , 'additional_info' : additional_info } if __name__ == '__main__' : result = run () print ( result ) Mindsdb accuracy Accuracy Backend Last run MindsDB Version Latest Version 0.8414694158197122, Lightwood 17 April 2020","title":"Hotel Booking Demand"},{"location":"tutorials/HotelBooking/#processed-dataset","text":"","title":"Processed Dataset"},{"location":"tutorials/HotelBooking/#_1","text":"This data set contains booking information for a city hotel and a resort hotel, and includes information such as when the booking was made, length of stay, the number of adults, children, and/or babies, and the number of available parking spaces, among other things. hotel is_canceled lead_time arrival_date_year arrival_date_month arrival_date_week_number arrival_date_day_of_month stays_in_weekend_nights stays_in_week_nights adults children babies meal country market_segment distribution_channel is_repeated_guest previous_cancellations previous_bookings_not_canceled reserved_room_type assigned_room_type booking_changes deposit_type agent company days_in_waiting_list customer_type adr required_car_parking_spaces total_of_special_requests reservation_status reservation_status_date Resort Hotel 0 342 2015 July 27 1 0 0 2 0 0 BB PRT Direct Direct 0 0 0 C C 3 No Deposit nan nan 0 Transient 0 0 0 Check-Out 2015-07-01 Resort Hotel 0 737 2015 July 27 1 0 0 2 0 0 BB PRT Direct Direct 0 0 0 C C 4 No Deposit nan nan 0 Transient 0 0 0 Check-Out 2015-07-01 Resort Hotel 0 7 2015 July 27 1 0 1 1 0 0 BB GBR Direct Direct 0 0 0 A C 0 No Deposit nan nan 0 Transient 75 0 0 Check-Out 2015-07-02 Resort Hotel 0 13 2015 July 27 1 0 1 1 0 0 BB GBR Corporate Corporate 0 0 0 A A 0 No Deposit 304 nan 0 Transient 75 0 0 Check-Out 2015-07-02 Resort Hotel 0 14 2015 July 27 1 0 2 2 0 0 BB GBR Online TA TA/TO 0 0 0 A A 0 No Deposit 240 nan 0 Transient 98 0 1 Check-Out 2015-07-03 Resort Hotel 0 14 2015 July 27 1 0 2 2 0 0 BB GBR Online TA TA/TO 0 0 0 A A 0 No Deposit 240 nan 0 Transient 98 0 1 Check-Out 2015-07-03 Resort Hotel 0 0 2015 July 27 1 0 2 2 0 0 BB PRT Direct Direct 0 0 0 C C 0 No Deposit nan nan 0 Transient 107 0 0 Check-Out 2015-07-03 Click to expand Features Informations: 1. hotel Hotel (H1 = Resort Hotel or H2 = City Hotel) 2. is_canceled Value indicating if the booking was canceled (1) or not (0) lead_timeNumber of days that elapsed between the entering date of the booking into the PMS and the arrival date 3. arrival_date_year Year of arrival date 4. arrival_date_month Month of arrival date 5. arrival_date_week_number Week number of year for arrival date 6. arrival_date_day_of_month Day of arrival date 7. stays_in_weekend_nights Number of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel 8. stays_in_week_nightsNumber of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel 9. adultsNumber of adults 10. childrenNumber of children 11. babiesNumber of babies 12. mealType of meal booked. Categories are presented in standard hospitality meal packages: Undefined/SC \u2013 no meal package; BB \u2013 Bed & Breakfast; HB \u2013 Half board (breakfast and one other meal \u2013 usually dinner); FB \u2013 Full board (breakfast, lunch and dinner) 13. countryCountry of origin. Categories are represented in the ISO 3155\u20133:2013 format 14. market_segment Market segment designation. In categories, the term \u201cTA\u201d means \u201cTravel Agents\u201d and \u201cTO\u201d means \u201cTour Operators\u201d 15. distribution_channel Booking distribution channel. The term \u201cTA\u201d means \u201cTravel Agents\u201d and \u201cTO\u201d means \u201cTour Operators\u201d 16. is_repeated_guestValue indicating if the booking name was from a repeated guest (1) or not (0) 17. previous_cancellationsNumber of previous bookings that were cancelled by the customer prior to the current booking 18. previous_bookings_not_canceledNumber of previous bookings not cancelled by the customer prior to the current booking 19. reserved_room_typeCode of room type reserved. Code is presented instead of designation for anonymity reasons. 20. assigned_room_type Code for the type of room assigned to the booking. Sometimes the assigned room type differs from the reserved room type due to hotel operation reasons (e.g. overbooking) or by customer request. Code is presented instead of designation for anonymity reasons. 21. booking_changes Number of changes/amendments made to the booking from the moment the booking was entered on the PMS until the moment of check-in or cancellation 22. deposit_type Indication on if the customer made a deposit to guarantee the booking. This variable can assume three categories: No Deposit \u2013 no deposit was made; Non Refund \u2013 a deposit was made in the value of the total stay cost; Refundable \u2013 a deposit was made with a value under the total cost of stay. 23. agentID of the travel agency that made the booking 24. companyID of the company/entity that made the booking or responsible for paying the booking. ID is presented instead of designation for anonymity reasons 25. days_in_waiting_list Number of days the booking was in the waiting list before it was confirmed to the customer 26. customer_type Type of booking, assuming one of four categories: Contract - when the booking has an allotment or other type of contract associated to it; Group \u2013 when the booking is associated to a group; Transient \u2013 when the booking is not part of a group or contract, and is not associated to other 27. transient booking; Transient-party \u2013 when the booking is transient, but is associated to at least other transient booking adrAverage Daily Rate as defined by dividing the sum of all lodging transactions by the total number of staying nights 28. required_car_parking_spaces Number of car parking spaces required by the customer 29. total_of_special_requests Number of special requests made by the customer (e.g. twin bed or high floor) 30. reservation_status Reservation last status, assuming one of three categories: Canceled \u2013 booking was canceled by the customer; Check-Out \u2013 customer has checked in but already departed; No-Show \u2013 customer did not check-in and did inform the hotel of the reason why 31. reservation_status_date Date at which the last status was set. This variable can be used in conjunction with the ReservationStatus to understand when was the booking canceled or when did the customer checked-out of the hotel","title":""},{"location":"tutorials/HotelBooking/#mindsdb-code-example","text":"import mindsdb import sys import pandas as pd from sklearn.metrics import balanced_accuracy_score def run (): mdb = mindsdb . Predictor ( name = 'hotel_booking' ) mdb . learn ( from_data = 'dataset/train.csv' , to_predict = 'is_canceled' ) test_df = pd . read_csv ( 'dataset/test.csv' ) predictions = mdb . predict ( when_data = 'dataset/test.csv' ) results = [ str ( x [ 'is_canceled' ]) for x in predictions ] real = list ( map ( str , list ( test_df [ 'is_canceled' ]))) accuracy = balanced_accuracy_score ( real , results ) #show additional info for each transaction row additional_info = [ x . explanation for x in predictions ] return { 'accuracy' : accuracy , 'accuracy_function' : 'balanced_accuracy_score' , 'backend' : backend , 'additional_info' : additional_info } if __name__ == '__main__' : result = run () print ( result )","title":"MindsDB Code example"},{"location":"tutorials/HotelBooking/#mindsdb-accuracy","text":"Accuracy Backend Last run MindsDB Version Latest Version 0.8414694158197122, Lightwood 17 April 2020","title":"Mindsdb accuracy"},{"location":"tutorials/MedicalDiagnosis/","text":"Industry Department Role Health Care Health Business Executive/Physician Breast Cancer Wisconsin (Diagnostic) Data Set . From the given information of the breast cancer dataset, classify whether it is a malignant cancer or benign cancer. diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave_points_mean symmetry_mean fractal_dimension_mean radius_se texture_se perimeter_se area_se smoothness_se compactness_se concavity_se concave points_se symmetry_se fractal_dimension_se radius_worst texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst 0 17.99 10.38 122.8 1001 0.1184 0.2776 0.3001 0.1471 0.2419 0.07871 1.095 0.9053 8.589 153.4 0.006399 0.04904 0.05373 0.01587 0.03003 0.006193 25.38 17.33 184.6 2019 0.1622 0.6656 0.7119 0.2654 0.4601 0.1189 0 20.57 17.77 132.9 1326 0.08474 0.07864 0.0869 0.07017 0.1812 0.05667 0.5435 0.7339 3.398 74.08 0.005225 0.01308 0.0186 0.0134 0.01389 0.003532 24.99 23.41 158.8 1956 0.1238 0.1866 0.2416 0.186 0.275 0.08902 0 19.69 21.25 130 1203 0.1096 0.1599 0.1974 0.1279 0.2069 0.05999 0.7456 0.7869 4.585 94.03 0.00615 0.04006 0.03832 0.02058 0.0225 0.004571 23.57 25.53 152.5 1709 0.1444 0.4245 0.4504 0.243 0.3613 0.08758 0 11.42 20.38 77.58 386.1 0.1425 0.2839 0.2414 0.1052 0.2597 0.09744 0.4956 1.156 3.445 27.23 0.00911 0.07458 0.05661 0.01867 0.05963 0.009208 14.91 26.5 98.87 567.7 0.2098 0.8663 0.6869 0.2575 0.6638 0.173 0 20.29 14.34 135.1 1297 0.1003 0.1328 0.198 0.1043 0.1809 0.05883 0.7572 0.7813 5.438 94.44 0.01149 0.02461 0.05688 0.01885 0.01756 0.005115 22.54 16.67 152.2 1575 0.1374 0.205 0.4 0.1625 0.2364 0.07678 0 12.45 15.7 82.57 477.1 0.1278 0.17 0.1578 0.08089 0.2087 0.07613 0.3345 0.8902 2.217 27.19 0.00751 0.03345 0.03672 0.01137 0.02165 0.005082 15.47 23.75 103.4 741.6 0.1791 0.5249 0.5355 0.1741 0.3985 0.1244 0 18.25 19.98 119.6 1040 0.09463 0.109 0.1127 0.074 0.1794 0.05742 0.4467 0.7732 3.18 53.91 0.004314 0.01382 0.02254 0.01039 0.01369 0.002179 22.88 27.66 153.2 1606 0.1442 0.2576 0.3784 0.1932 0.3063 0.08368 Click to expand Features Informations: 1. id ID number 2. diagnosis The diagnosis of breast tissues (M = malignant, B = benign) 3. radius_mean mean of distances from center to points on the perimeter 4. texture_means tandard deviation of gray-scale values 5. perimeter_mean mean size of the core tumor 6. area_mean 7. smoothness_mean mean of local variation in radius lengths 8. compactness_mean mean of perimeter^2 / area - 1.0 9. concavity_mean mean of severity of concave portions of the contour 10. concave points_mean mean for number of concave portions of the contour 11. symmetry_mean 12. fractal_dimension_mean mean for \"coastline approximation\" - 1 13. radius_sestandard error for the mean of distances from center to points on the perimeter 14. texture_sestandard error for standard deviation of gray-scale values 15. perimeter_se 16. area_se 17. smoothness_sestandard error for local variation in radius lengths 18. compactness_sestandard error for perimeter^2 / area - 1.0 19. concavity_sestandard error for severity of concave portions of the contour concave points_sestandard error for number of concave portions of the contour 20. symmetry_se 21. fractal_dimension_sestandard error for \"coastline approximation\" - 1 22. radius_worst\"worst\" or largest mean value for mean of distances from center to points on the perimeter 23. texture_worst\"worst\" or largest mean value for standard deviation of gray-scale values 24. perimeter_worst 25. area_worst 26. smoothness_worst \"worst\" or largest mean value for local variation in radius lengths 27. compactness_worst \"worst\" or largest mean value for perimeter^2 / area - 1.0 28. concavity_worst \"worst\" or largest mean value for severity of 29. 29. concave portions of the contour 30. concave points_worst \"worst\" or largest mean value for number of concave portions of the contour 31. symmetry_worst 32. fractal_dimension_worst\"worst\" or largest mean value for \"coastline approximation\" - 1 MindsDB Code example import mindsdb import sys import pandas as pd from sklearn.metrics import balanced_accuracy_score def run ( sample ): mdb = mindsdb . Predictor ( name = 'cancer_model' ) mdb . learn ( from_data = 'processed_data/train.csv' , to_predict = 'diagnosis' ) test_df = pd . read_csv ( 'processed_data/test.csv' ) predictions = mdb . predict ( when_data = 'processed_data/test.csv' ) results = [ str ( x [ 'diagnosis' ]) for x in predictions ] real = list ( map ( str , list ( test_df [ 'diagnosis' ]))) accuracy = balanced_accuracy_score ( real , results ) return { 'accuracy' : accuracy , 'accuracy_function' : 'balanced_accuracy_score' , 'backend' : backend } if __name__ == '__main__' : sample = bool ( sys . argv [ 1 ]) if len ( sys . argv ) > 1 else False result = run ( sample ) print ( result ) Mindsdb accuracy Accuracy Backend Last run MindsDB Version Latest Version 0.9666666666666667 Lightwood 17 April 2020","title":"Medical Diagnosis"},{"location":"tutorials/MedicalDiagnosis/#_1","text":"Breast Cancer Wisconsin (Diagnostic) Data Set . From the given information of the breast cancer dataset, classify whether it is a malignant cancer or benign cancer. diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave_points_mean symmetry_mean fractal_dimension_mean radius_se texture_se perimeter_se area_se smoothness_se compactness_se concavity_se concave points_se symmetry_se fractal_dimension_se radius_worst texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst 0 17.99 10.38 122.8 1001 0.1184 0.2776 0.3001 0.1471 0.2419 0.07871 1.095 0.9053 8.589 153.4 0.006399 0.04904 0.05373 0.01587 0.03003 0.006193 25.38 17.33 184.6 2019 0.1622 0.6656 0.7119 0.2654 0.4601 0.1189 0 20.57 17.77 132.9 1326 0.08474 0.07864 0.0869 0.07017 0.1812 0.05667 0.5435 0.7339 3.398 74.08 0.005225 0.01308 0.0186 0.0134 0.01389 0.003532 24.99 23.41 158.8 1956 0.1238 0.1866 0.2416 0.186 0.275 0.08902 0 19.69 21.25 130 1203 0.1096 0.1599 0.1974 0.1279 0.2069 0.05999 0.7456 0.7869 4.585 94.03 0.00615 0.04006 0.03832 0.02058 0.0225 0.004571 23.57 25.53 152.5 1709 0.1444 0.4245 0.4504 0.243 0.3613 0.08758 0 11.42 20.38 77.58 386.1 0.1425 0.2839 0.2414 0.1052 0.2597 0.09744 0.4956 1.156 3.445 27.23 0.00911 0.07458 0.05661 0.01867 0.05963 0.009208 14.91 26.5 98.87 567.7 0.2098 0.8663 0.6869 0.2575 0.6638 0.173 0 20.29 14.34 135.1 1297 0.1003 0.1328 0.198 0.1043 0.1809 0.05883 0.7572 0.7813 5.438 94.44 0.01149 0.02461 0.05688 0.01885 0.01756 0.005115 22.54 16.67 152.2 1575 0.1374 0.205 0.4 0.1625 0.2364 0.07678 0 12.45 15.7 82.57 477.1 0.1278 0.17 0.1578 0.08089 0.2087 0.07613 0.3345 0.8902 2.217 27.19 0.00751 0.03345 0.03672 0.01137 0.02165 0.005082 15.47 23.75 103.4 741.6 0.1791 0.5249 0.5355 0.1741 0.3985 0.1244 0 18.25 19.98 119.6 1040 0.09463 0.109 0.1127 0.074 0.1794 0.05742 0.4467 0.7732 3.18 53.91 0.004314 0.01382 0.02254 0.01039 0.01369 0.002179 22.88 27.66 153.2 1606 0.1442 0.2576 0.3784 0.1932 0.3063 0.08368 Click to expand Features Informations: 1. id ID number 2. diagnosis The diagnosis of breast tissues (M = malignant, B = benign) 3. radius_mean mean of distances from center to points on the perimeter 4. texture_means tandard deviation of gray-scale values 5. perimeter_mean mean size of the core tumor 6. area_mean 7. smoothness_mean mean of local variation in radius lengths 8. compactness_mean mean of perimeter^2 / area - 1.0 9. concavity_mean mean of severity of concave portions of the contour 10. concave points_mean mean for number of concave portions of the contour 11. symmetry_mean 12. fractal_dimension_mean mean for \"coastline approximation\" - 1 13. radius_sestandard error for the mean of distances from center to points on the perimeter 14. texture_sestandard error for standard deviation of gray-scale values 15. perimeter_se 16. area_se 17. smoothness_sestandard error for local variation in radius lengths 18. compactness_sestandard error for perimeter^2 / area - 1.0 19. concavity_sestandard error for severity of concave portions of the contour concave points_sestandard error for number of concave portions of the contour 20. symmetry_se 21. fractal_dimension_sestandard error for \"coastline approximation\" - 1 22. radius_worst\"worst\" or largest mean value for mean of distances from center to points on the perimeter 23. texture_worst\"worst\" or largest mean value for standard deviation of gray-scale values 24. perimeter_worst 25. area_worst 26. smoothness_worst \"worst\" or largest mean value for local variation in radius lengths 27. compactness_worst \"worst\" or largest mean value for perimeter^2 / area - 1.0 28. concavity_worst \"worst\" or largest mean value for severity of 29. 29. concave portions of the contour 30. concave points_worst \"worst\" or largest mean value for number of concave portions of the contour 31. symmetry_worst 32. fractal_dimension_worst\"worst\" or largest mean value for \"coastline approximation\" - 1","title":""},{"location":"tutorials/MedicalDiagnosis/#mindsdb-code-example","text":"import mindsdb import sys import pandas as pd from sklearn.metrics import balanced_accuracy_score def run ( sample ): mdb = mindsdb . Predictor ( name = 'cancer_model' ) mdb . learn ( from_data = 'processed_data/train.csv' , to_predict = 'diagnosis' ) test_df = pd . read_csv ( 'processed_data/test.csv' ) predictions = mdb . predict ( when_data = 'processed_data/test.csv' ) results = [ str ( x [ 'diagnosis' ]) for x in predictions ] real = list ( map ( str , list ( test_df [ 'diagnosis' ]))) accuracy = balanced_accuracy_score ( real , results ) return { 'accuracy' : accuracy , 'accuracy_function' : 'balanced_accuracy_score' , 'backend' : backend } if __name__ == '__main__' : sample = bool ( sys . argv [ 1 ]) if len ( sys . argv ) > 1 else False result = run ( sample ) print ( result )","title":"MindsDB Code example"},{"location":"tutorials/MedicalDiagnosis/#mindsdb-accuracy","text":"Accuracy Backend Last run MindsDB Version Latest Version 0.9666666666666667 Lightwood 17 April 2020","title":"Mindsdb accuracy"},{"location":"tutorials/PatientHealthOutcomes/","text":"Industry Department Role Health Care Health Business executive / Physician Processed Dataset In the Heart Disease UCI dataset, the data comes from 4 databases: the Hungarian Institute of Cardiology, the University Hospital in Zurich, the University Hospital in Basel Switzerland, and the V.A. Medical Center Long Beach and Cleveland Clinic Foundation. The \"goal\" is to determine the presence of heart disease in the patient. age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target 63 1 3 145 233 1 0 150 0 2.3 0 0 1 1 37 1 2 130 250 0 1 187 0 3.5 0 0 2 1 41 0 1 130 204 0 0 172 0 1.4 2 0 2 1 56 1 1 120 236 0 1 178 0 0.8 2 0 2 1 57 0 0 120 354 0 1 163 1 0.6 2 0 2 1 57 1 0 140 192 0 1 148 0 0.4 1 0 1 1 56 0 1 140 294 0 0 153 0 1.3 1 0 2 1 Click to expand Features Informations: 1. age: age in years 2. sex: sex (1 = male; 0 = female) 3. cp: chest pain type * Value 1: typical angina * Value 2: atypical angina * Value 3: non-anginal pain * Value 4: asymptomatic 4. trestbps: resting blood pressure (in mm Hg on admission to the hospital) 5. chol: serum cholestoral in mg/dl 6. fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) 7. restecg: resting electrocardiographic results * Value 0: normal * Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) * Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria 8. thalach: maximum heart rate achieved 9. exang: exercise induced angina (1 = yes; 0 = no) 10. oldpeak = ST depression induced by exercise relative to rest 11. slope: the slope of the peak exercise ST segment * Value 1: upsloping * Value 2: flat * Value 3: downsloping 12. ca: number of major vessels (0-3) colored by flourosopy 13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect 14. num: diagnosis of heart disease (angiographic disease status) * Value 0: < 50% diameter narrowing * Value 1: > 50% diameter narrowing MindsDB Code example import mindsdb import pandas as pd from sklearn.metrics import balanced_accuracy_score def run (): mdb = mindsdb . Predictor ( name = 'hd' ) mdb . learn ( from_data = 'processed_data/train.csv' , to_predict = 'target' ) predictions = mdb . predict ( when_data = 'processed_data/test.csv' ) pred_val = [ int ( x [ 'target' ]) for x in predictions ] real_val = [ int ( x ) for x in list ( pd . read_csv ( 'processed_data/test.csv' )[ 'target' ])] accuracy = balanced_accuracy_score ( real_val , pred_val ) #show additional info for each transaction row additional_info = [ x . explanation for x in predictions ] return { 'accuracy' : accuracy , 'backend' : backend , 'additional info' : additional_info } # Run as main if __name__ == '__main__' : print ( run ()) Mindsdb accuracy Accuracy Backend Last run MindsDB Version Latest Version 0.8256302521008403 Lightwood 16 April 2020","title":"Patient Health"},{"location":"tutorials/PatientHealthOutcomes/#processed-dataset","text":"","title":"Processed Dataset"},{"location":"tutorials/PatientHealthOutcomes/#_1","text":"In the Heart Disease UCI dataset, the data comes from 4 databases: the Hungarian Institute of Cardiology, the University Hospital in Zurich, the University Hospital in Basel Switzerland, and the V.A. Medical Center Long Beach and Cleveland Clinic Foundation. The \"goal\" is to determine the presence of heart disease in the patient. age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target 63 1 3 145 233 1 0 150 0 2.3 0 0 1 1 37 1 2 130 250 0 1 187 0 3.5 0 0 2 1 41 0 1 130 204 0 0 172 0 1.4 2 0 2 1 56 1 1 120 236 0 1 178 0 0.8 2 0 2 1 57 0 0 120 354 0 1 163 1 0.6 2 0 2 1 57 1 0 140 192 0 1 148 0 0.4 1 0 1 1 56 0 1 140 294 0 0 153 0 1.3 1 0 2 1 Click to expand Features Informations: 1. age: age in years 2. sex: sex (1 = male; 0 = female) 3. cp: chest pain type * Value 1: typical angina * Value 2: atypical angina * Value 3: non-anginal pain * Value 4: asymptomatic 4. trestbps: resting blood pressure (in mm Hg on admission to the hospital) 5. chol: serum cholestoral in mg/dl 6. fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) 7. restecg: resting electrocardiographic results * Value 0: normal * Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) * Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria 8. thalach: maximum heart rate achieved 9. exang: exercise induced angina (1 = yes; 0 = no) 10. oldpeak = ST depression induced by exercise relative to rest 11. slope: the slope of the peak exercise ST segment * Value 1: upsloping * Value 2: flat * Value 3: downsloping 12. ca: number of major vessels (0-3) colored by flourosopy 13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect 14. num: diagnosis of heart disease (angiographic disease status) * Value 0: < 50% diameter narrowing * Value 1: > 50% diameter narrowing","title":""},{"location":"tutorials/PatientHealthOutcomes/#mindsdb-code-example","text":"import mindsdb import pandas as pd from sklearn.metrics import balanced_accuracy_score def run (): mdb = mindsdb . Predictor ( name = 'hd' ) mdb . learn ( from_data = 'processed_data/train.csv' , to_predict = 'target' ) predictions = mdb . predict ( when_data = 'processed_data/test.csv' ) pred_val = [ int ( x [ 'target' ]) for x in predictions ] real_val = [ int ( x ) for x in list ( pd . read_csv ( 'processed_data/test.csv' )[ 'target' ])] accuracy = balanced_accuracy_score ( real_val , pred_val ) #show additional info for each transaction row additional_info = [ x . explanation for x in predictions ] return { 'accuracy' : accuracy , 'backend' : backend , 'additional info' : additional_info } # Run as main if __name__ == '__main__' : print ( run ())","title":"MindsDB Code example"},{"location":"tutorials/PatientHealthOutcomes/#mindsdb-accuracy","text":"Accuracy Backend Last run MindsDB Version Latest Version 0.8256302521008403 Lightwood 16 April 2020","title":"Mindsdb accuracy"},{"location":"tutorials/PredictiveMaintenance/","text":"Industry Department Role High-Tech & Manufacturing Operations Data Scientist Processed Dataset This dataset contains force and torque measurements on a robot after failure detection. Each failure is characterized by 15 force/torque samples collected at regular time intervals. id time F_x F_y F_z T_x T_y T_z target 1 0 -1 -1 63 -3 -1 0 True 1 1 0 0 62 -3 -1 0 True 1 2 -1 -1 61 -3 0 0 True 1 3 -1 -1 63 -2 -1 0 True 1 4 -1 -1 63 -3 -1 0 True 1 5 -1 -1 63 -3 -1 0 True 1 6 -1 -1 63 -3 0 0 True Click to expand Features Informations: id time F_x F_y F_z T_x T_y T_z target Fx1 ... Fx15 is the evolution of force Fx in the observation window import mindsdb import pandas as pd from sklearn.metrics import r2_score def run (): mdb = mindsdb . Predictor ( name = 'robotic_failure' ) mdb . learn ( from_data = 'dataset/train.csv' , to_predict = [ 'target' ]) predictions = mdb . predict ( when = 'test.csv' ) pred_val = [ x [ 'target' ] for x in predictions ] real_val = list ( pd . read_csv ( 'dataset/test.csv' )[ 'target' ]) accuracy = r2_score ( real_val , pred_val ) print ( f 'Got an r2 score of: { accuracy } ' ) #show additional info for each transaction row additional_info = [ x . explanation for x in predictions ] return { 'accuracy' : accuracy , 'accuracy_function' : 'balanced_accuracy_score' , 'backend' : backend , 'additional_info' : additional_info } # Run as main if __name__ == '__main__' : print ( run ()) Mindsdb accuracy Accuracy Backend Last run MindsDB Version Latest Version 0.8399922571492469 Lightwood 15 April 2020","title":"Predictive Maintenance"},{"location":"tutorials/PredictiveMaintenance/#processed-dataset","text":"","title":"Processed Dataset"},{"location":"tutorials/PredictiveMaintenance/#_1","text":"This dataset contains force and torque measurements on a robot after failure detection. Each failure is characterized by 15 force/torque samples collected at regular time intervals. id time F_x F_y F_z T_x T_y T_z target 1 0 -1 -1 63 -3 -1 0 True 1 1 0 0 62 -3 -1 0 True 1 2 -1 -1 61 -3 0 0 True 1 3 -1 -1 63 -2 -1 0 True 1 4 -1 -1 63 -3 -1 0 True 1 5 -1 -1 63 -3 -1 0 True 1 6 -1 -1 63 -3 0 0 True Click to expand Features Informations: id time F_x F_y F_z T_x T_y T_z target Fx1 ... Fx15 is the evolution of force Fx in the observation window import mindsdb import pandas as pd from sklearn.metrics import r2_score def run (): mdb = mindsdb . Predictor ( name = 'robotic_failure' ) mdb . learn ( from_data = 'dataset/train.csv' , to_predict = [ 'target' ]) predictions = mdb . predict ( when = 'test.csv' ) pred_val = [ x [ 'target' ] for x in predictions ] real_val = list ( pd . read_csv ( 'dataset/test.csv' )[ 'target' ]) accuracy = r2_score ( real_val , pred_val ) print ( f 'Got an r2 score of: { accuracy } ' ) #show additional info for each transaction row additional_info = [ x . explanation for x in predictions ] return { 'accuracy' : accuracy , 'accuracy_function' : 'balanced_accuracy_score' , 'backend' : backend , 'additional_info' : additional_info } # Run as main if __name__ == '__main__' : print ( run ())","title":""},{"location":"tutorials/PredictiveMaintenance/#mindsdb-accuracy","text":"Accuracy Backend Last run MindsDB Version Latest Version 0.8399922571492469 Lightwood 15 April 2020","title":"Mindsdb accuracy"},{"location":"tutorials/Timeseries/","text":"Handling Timeseries Data Timeseries interface A timeseries is a problem where rows are related to each other in a sequential way, such that the prediction of the value in the present row should take into account a number of previous rows. To build a timeseries model you need to pass timeseries_settings dictionary to learn timeseries_settings = { order_by: List<String> | Mandatory group_by: List<String> | Optional (default: []) nr_predictions: Int | Optional (default: 1) use_previous_target: Bool | Optional (default: True) window: Int | Mandatory historical_columns: List<String> | Optional (default: []) } Let's go through these settings one by one: order_by - The columns based on which the data should be ordered group_by - The columns based on which to group multiple unrelated entities present in your timeseries data. For example, let's say your data consists of sequential readings from 3x sensors. Treating the problem as a timeseries makes sense for individual sensors, so you would specify: group_by=['sensor_id'] nr_predictions - The number of points in the future that predictions should be made for, defaults to 1 . Once trained, the model will be able to predict up to this many points into the future. use_previous_target - Use the previous values of the target column[s] for making predictions. Defaults to True . [Status: Experimental] window - The number of rows to \"look back\" into when making a prediction, after the rows are ordered by the order_by column and split into groups. Code example import mindsdb mdb = mindsdb . Predictor ( name = 'assembly_machines_model' ) mdb . learn ( from_data = 'assembly_machines_historical_data.tsv' , to_predict = 'failure' , timeseries_settings = { 'order_by' : [ 'timestamp' ], # Order the observations by timestamp 'group_by' : [ 'machine_id' ], # The ordering should be done on a per-machine basis, rather than for every single row 'nr_predictions' : 3 , # Predict failures for the timestamp given and for 2 more timesteps in the future 'use_previous_target' : True , # Us the previous values in the target column (`failure`), since when the last failure happened could be a relevant data-point for our prediction. 'window' : 20 # Consider the previous 20 rows for every single row our model is trying to predict o } ) results = mdb . predict ( when_data = 'new_assembly_machines_data.tsv' ) Historical data When making timeseries predictions, it's important to provide mindsdb with the context for those predictions, i.e. with the previous rows that came before the one you are trying to predict for. Say your columns are: date, nr_customers, store . You order by date , group by store and need to predict nr_customers . You set window=3 . You train your model and then want to make a prediction using a csv with the following content: date, nr_customers, store 2020-10-06, unknown , A1 This prediction will be less than ideal, since mindsdb doesn't know how many customers came to the store on 2020-10-05 or 2020-10-04 , which is probably the main insight the trained model is using to make predictions. So instead you need to pass a file with the following content: date, nr_customers, store 2020-10-04, 55 , A1 2020-10-05, 123 , A1 2020-10-06, None , A1 Note that mindsdb will generate a prediction for every row here (even if the target value nr_customers already exists), but you only care about the prediction for the last row, the previous 2 are there to provide historical context. Also note that, if you window was, say, equal to 5 , you would have had to provide 4 more rows instead of 2 more. Also note that, if you were to give the file: date, nr_customers, store 2020-10-04, 55 , B11 2020-10-05, 123 , A2 2020-10-06, None , A1 This wouldn't count as historical context, since you are grouping by the store column, so only rows where store is A1 will be relevant historical context for predicting a row where the store == A1 Database integration There is an experimental feature, when you train mindsdb from a database, that auto-generates a query to select historical context, based on the query you used to source your training data. This can be enabled by passing advanced_args={'use_database_history': True} to the predict call (or to the SELECT call if operating from within a database). This is still very experimental and has many blindspots, so if you're interested in using this please contact us so we can help and get your feedback on how to improve this. Database example (from SQL) -- Pending, feel free to contribute some or ask us directly about this feature Database example (from code) -- Pending, feel free to contribute some or ask us directly about this feature","title":"Timeseries Data"},{"location":"tutorials/Timeseries/#handling-timeseries-data","text":"","title":"Handling Timeseries Data"},{"location":"tutorials/Timeseries/#timeseries-interface","text":"A timeseries is a problem where rows are related to each other in a sequential way, such that the prediction of the value in the present row should take into account a number of previous rows. To build a timeseries model you need to pass timeseries_settings dictionary to learn timeseries_settings = { order_by: List<String> | Mandatory group_by: List<String> | Optional (default: []) nr_predictions: Int | Optional (default: 1) use_previous_target: Bool | Optional (default: True) window: Int | Mandatory historical_columns: List<String> | Optional (default: []) } Let's go through these settings one by one: order_by - The columns based on which the data should be ordered group_by - The columns based on which to group multiple unrelated entities present in your timeseries data. For example, let's say your data consists of sequential readings from 3x sensors. Treating the problem as a timeseries makes sense for individual sensors, so you would specify: group_by=['sensor_id'] nr_predictions - The number of points in the future that predictions should be made for, defaults to 1 . Once trained, the model will be able to predict up to this many points into the future. use_previous_target - Use the previous values of the target column[s] for making predictions. Defaults to True . [Status: Experimental] window - The number of rows to \"look back\" into when making a prediction, after the rows are ordered by the order_by column and split into groups.","title":"Timeseries interface"},{"location":"tutorials/Timeseries/#code-example","text":"import mindsdb mdb = mindsdb . Predictor ( name = 'assembly_machines_model' ) mdb . learn ( from_data = 'assembly_machines_historical_data.tsv' , to_predict = 'failure' , timeseries_settings = { 'order_by' : [ 'timestamp' ], # Order the observations by timestamp 'group_by' : [ 'machine_id' ], # The ordering should be done on a per-machine basis, rather than for every single row 'nr_predictions' : 3 , # Predict failures for the timestamp given and for 2 more timesteps in the future 'use_previous_target' : True , # Us the previous values in the target column (`failure`), since when the last failure happened could be a relevant data-point for our prediction. 'window' : 20 # Consider the previous 20 rows for every single row our model is trying to predict o } ) results = mdb . predict ( when_data = 'new_assembly_machines_data.tsv' )","title":"Code example"},{"location":"tutorials/Timeseries/#historical-data","text":"When making timeseries predictions, it's important to provide mindsdb with the context for those predictions, i.e. with the previous rows that came before the one you are trying to predict for. Say your columns are: date, nr_customers, store . You order by date , group by store and need to predict nr_customers . You set window=3 . You train your model and then want to make a prediction using a csv with the following content: date, nr_customers, store 2020-10-06, unknown , A1 This prediction will be less than ideal, since mindsdb doesn't know how many customers came to the store on 2020-10-05 or 2020-10-04 , which is probably the main insight the trained model is using to make predictions. So instead you need to pass a file with the following content: date, nr_customers, store 2020-10-04, 55 , A1 2020-10-05, 123 , A1 2020-10-06, None , A1 Note that mindsdb will generate a prediction for every row here (even if the target value nr_customers already exists), but you only care about the prediction for the last row, the previous 2 are there to provide historical context. Also note that, if you window was, say, equal to 5 , you would have had to provide 4 more rows instead of 2 more. Also note that, if you were to give the file: date, nr_customers, store 2020-10-04, 55 , B11 2020-10-05, 123 , A2 2020-10-06, None , A1 This wouldn't count as historical context, since you are grouping by the store column, so only rows where store is A1 will be relevant historical context for predicting a row where the store == A1","title":"Historical data"},{"location":"tutorials/Timeseries/#database-integration","text":"There is an experimental feature, when you train mindsdb from a database, that auto-generates a query to select historical context, based on the query you used to source your training data. This can be enabled by passing advanced_args={'use_database_history': True} to the predict call (or to the SELECT call if operating from within a database). This is still very experimental and has many blindspots, so if you're interested in using this please contact us so we can help and get your feedback on how to improve this.","title":"Database integration"},{"location":"tutorials/Timeseries/#database-example-from-sql","text":"-- Pending, feel free to contribute some or ask us directly about this feature","title":"Database example (from SQL)"},{"location":"tutorials/Timeseries/#database-example-from-code","text":"-- Pending, feel free to contribute some or ask us directly about this feature","title":"Database example (from code)"}]}